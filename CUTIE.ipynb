{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUTIE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSnkM3rZ8g1M6r1kExHBov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywsyws/CUTIE/blob/main/CUTIE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-n4WnjsHLPI"
      },
      "source": [
        "Using CUTIE approach to detect total amount TTC on receipt documents\n",
        "\n",
        "# 0. Preparation\n",
        "## 0.1 Environment Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD7HIRbnlU7D"
      },
      "source": [
        "# # Install tesseract ocr libraries\n",
        "# !sudo apt install tesseract-ocr\n",
        "# !pip3 install pytesseract"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuGYTrg02XVZ",
        "outputId": "8f4c6750-a6c8-4657-a5a8-27130565833e"
      },
      "source": [
        "# Install tensorflow library\n",
        "!pip3 install tensorflow==1.14"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (53.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK2mOPBlavCY"
      },
      "source": [
        "# # Mount Google Drive to Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7L37NagHEsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4610eba4-e56c-45b8-b60b-405ecfe7b704"
      },
      "source": [
        "# Import libraries\n",
        "from IPython.display import Image, display\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "# import pytesseract\n",
        "\n",
        "import json, re, random, argparse, timeit\n",
        "from os import chdir, walk, makedirs\n",
        "from os.path import basename, split, join, exists\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "from pprint import pprint"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PUjJ3bAb5S7"
      },
      "source": [
        "# Set root path for this project\n",
        "root_path = r'/content/gdrive/MyDrive/Colab Notebooks/CUTIE/'\n",
        "# Change working directory\n",
        "chdir(root_path)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V82E7JUdH0bn"
      },
      "source": [
        "# # Display an image\n",
        "# image_name='ExpressExpenseImage/1004-receipt.jpg'\n",
        "# Image(filename=image_name)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDhCT99EVRQ9"
      },
      "source": [
        "## 0.2 Data Preparation\n",
        "### 0.21 Capture texts on images\n",
        "Stores texts and related information in json format for model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bAqQY9ClCO2"
      },
      "source": [
        "# # Display multiple images\n",
        "# for file_num in range(1000, 1003):\n",
        "#   image_name=f'ExpressExpenseImage/{file_num}-receipt.jpg'\n",
        "#   image = Image(filename=image_name)\n",
        "#   display(image)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTsibTTPETRB"
      },
      "source": [
        "# Helper functions\n",
        "def VALUES(texts):\n",
        "  \"\"\" \n",
        "  To map the id of any words related to \"total amount\" and its amount to \n",
        "  related field: value_id and value_text\n",
        "  \"\"\"\n",
        "  value_id, value_text = [], []\n",
        "  for row in range(len(texts)):\n",
        "    word = texts[row][\"text\"]\n",
        "    if re.match(r'tota.|.total.|^total|totl|ttl|due$|due|$total|drv\\sthru', word, re.I):\n",
        "      value_id.append(texts[row][\"word_id\"])\n",
        "      value_text.append(word)\n",
        "      top = texts[row]['bbox'][1]\n",
        "      for i in range(1,3):\n",
        "        top_next = texts[row+1]['bbox'][1]\n",
        "        top_next_2 = texts[row+2]['bbox'][1]\n",
        "        if abs((top-top_next)/top_next*100) < 4:\n",
        "          value_id.append(texts[row+i][\"word_id\"])\n",
        "          value_text.append(texts[row+i][\"text\"])\n",
        "          print(value_id, value_text) # TBD\n",
        "          return value_id, value_text\n",
        "        else:\n",
        "          print(value_id, value_text) # TBD\n",
        "          return value_id, value_text\n",
        "  if value_id == []:\n",
        "    print(f\"Cannot find 'Total' in {image_name}\")\n",
        "\n",
        "  return value_id, value_text"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "momM21k4IBQg"
      },
      "source": [
        "# Turn results from the Tesseract OCR Engine to json format.\n",
        "# The json format will then be used as model input \n",
        "\n",
        "min_file_num = 1000\n",
        "max_file_num = 1020\n",
        "\n",
        "for file_num in range(min_file_num, max_file_num):\n",
        "  image_name=f'ExpressExpenseImage/{file_num}-receipt.jpg'\n",
        "\n",
        "  # Use Tesseract to localize each area of text in the input image\n",
        "  receipt = pytesseract.image_to_data(image_name, lang='eng', config='--psm 11')#custom_config\n",
        "\n",
        "  # Split texts into rows\n",
        "  rows = receipt.split('\\n')\n",
        "  # Count number of rows in the text\n",
        "  num_of_rows = len(rows)\n",
        "  # Declare texts List to store text information\n",
        "  texts = []\n",
        "\n",
        "  # Get all the texts of the receipt\n",
        "  for r in range(1, num_of_rows-1):\n",
        "    # Declare words dictionary\n",
        "    words = dict()\n",
        "    # Split words in each row\n",
        "    splitted_row = rows[r].split('\\t')\n",
        "    # Set id for each word\n",
        "    words['word_id'] = r\n",
        "    # Get bounding box coordinates\n",
        "    words['bbox'] = tuple([int(splitted_row[6]), int(splitted_row[7]),    # left and top\n",
        "                          int(splitted_row[6]) + int(splitted_row[8]),   # right = left + width\n",
        "                          int(splitted_row[7]) + int(splitted_row[9])])  # bottom = top + height\n",
        "    \n",
        "    # Get word from each word_id\n",
        "    if len(splitted_row) < 12:\n",
        "      words['text'] = ''\n",
        "    else:\n",
        "      words['text'] = splitted_row[11]\n",
        "    \n",
        "    # Write to texts List only if there is a word in that word_id\n",
        "    if words['text']:\n",
        "      texts.append(words)\n",
        "\n",
        "  ### Combine all the necessary information into a dictionary\n",
        "\n",
        "  # Declare document Dictionary\n",
        "  doc = {}\n",
        "  # Add extracted texts in the document Dictionary\n",
        "  doc.update({'text_boxes': texts})\n",
        "\n",
        "  # Declare classes and fields Lists\n",
        "  classes = ['O', 'TTL']\n",
        "  fields = []\n",
        "  for cl in classes:\n",
        "    if cl == 'TTL':\n",
        "      value_id, value_text = VALUES(texts)\n",
        "      # Define fields\n",
        "      new_field = {\"field_name\": cl, \"value_id\": value_id, \"value_text\": value_text, \"key_id\": [], \"key_text\": []}\n",
        "      fields.append(new_field)\n",
        "\n",
        "  # Add fields and global_attributes in the document Dictionary\n",
        "  doc.update({'fileds': fields})\n",
        "  doc.update({\"global_attributes\":{\"file_id\": basename(image_name)}})\n",
        "\n",
        "  # Serialize the document Dictionary into json format\n",
        "  json_file_path = f\"ExpressExpenseJson/{file_num}-receipt.json\"\n",
        "  with open(json_file_path, 'w') as write_file:\n",
        "    json.dump(doc, write_file, indent = 4, ensure_ascii=False)\n",
        "\n",
        "\n",
        "  # json_obj = json.dumps(doc, indent = 4, ensure_ascii=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCKv4EGlMuwZ"
      },
      "source": [
        "### 0.22 Manuelly label wrong labels\n",
        "All Manuelly labeled json files are stored in the \"ExpressExpenseJson\" folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vBVvuW5LfYZ"
      },
      "source": [
        "## 0.3 Dictionary Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHPdT5x4U5y7"
      },
      "source": [
        "DEBUG = False # True to show grid as image \n",
        "\n",
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        pass\n",
        " \n",
        "    try:\n",
        "        unicodedata.numeric(s)\n",
        "        return True\n",
        "    except (TypeError, ValueError):\n",
        "        pass \n",
        "    return False\n",
        "\n",
        "class DataLoader():\n",
        "    \"\"\"\n",
        "    Produce grid tables\n",
        "    \"\"\"\n",
        "    def __init__(self, params, update_dict=True, load_dictionary=False, data_split=0.75):\n",
        "        self.random = False\n",
        "        self.data_laundry = False\n",
        "        self.encoding_factor = 1 # ensures the size (rows/cols) of grid table compat with the network\n",
        "        self.classes = ['O', 'TTL']\n",
        "        #self.classes = ['DontCare', 'Table'] # for table\n",
        "        #self.classes = ['DontCare', 'Column0', 'Column1', 'Column2', 'Column3', 'Column4', 'Column5'] # for column\n",
        "        #self.classes = ['DontCare', 'Column']\n",
        "        #self.classes = ['DontCare', 'VendorName', 'VendorTaxID', 'InvoiceDate', 'InvoiceNumber', 'ExpenseAmount', 'BaseAmount', 'TaxAmount', 'TaxRate'] # for Spanish project\n",
        "        \n",
        "        self.doc_path = params.doc_path\n",
        "        self.doc_test_path = params.test_path\n",
        "        self.use_cutie2 = params.use_cutie2 \n",
        "        # self.text_case = params.text_case \n",
        "        # self.tokenize = params.tokenize\n",
        "        # if self.tokenize:\n",
        "        #     self.tokenizer = tokenization.FullTokenizer('dict/vocab.txt', do_lower_case=not self.text_case)\n",
        "        \n",
        "        self.rows = self.encoding_factor # to be updated \n",
        "        self.cols = self.encoding_factor # to be updated \n",
        "        self.segment_grid = params.segment_grid if hasattr(params, 'segment_grid') else False # segment grid into two parts if grid is larger than cols_target\n",
        "        self.augment_strategy = params.augment_strategy if hasattr(params, 'augment_strategy') else 1 \n",
        "        # self.pm_strategy = params.positional_mapping_strategy if hasattr(params, 'positional_mapping_strategy') else 2 \n",
        "        self.rows_segment = params.rows_segment if hasattr(params, 'rows_segment') else 72 \n",
        "        self.cols_segment = params.cols_segment if hasattr(params, 'cols_segment') else 72\n",
        "        self.rows_target = params.rows_target if hasattr(params, 'rows_target') else 64 \n",
        "        self.cols_target = params.cols_target if hasattr(params, 'cols_target') else 64 \n",
        "        self.rows_ulimit = params.rows_ulimit if hasattr(params, 'rows_ulimit') else 80 # handle OOM, must be multiple of self.encoding_factor\n",
        "        self.cols_ulimit = params.cols_ulimit if hasattr(params, 'cols_ulimit') else 80 # handle OOM, must be multiple of self.encoding_factor\n",
        "                \n",
        "        # self.fill_bbox = params.fill_bbox if hasattr(params, 'fill_bbox') else False # fill bbox with labels or use one single lable for the entire bbox\n",
        "        \n",
        "        self.data_augmentation_dropout = params.data_augmentation_dropout if hasattr(params, 'data_augmentation_dropout') else False # TBD: randomly dropout rows/cols\n",
        "        self.data_augmentation_extra = params.data_augmentation_extra if hasattr(params, 'data_augmentation_extra') else False # randomly expand rows/cols\n",
        "        self.da_extra_rows = params.data_augmentation_extra_rows if hasattr(params, 'data_augmentation_extra_rows') else 0 # randomly expand rows/cols\n",
        "        self.da_extra_cols = params.data_augmentation_extra_cols if hasattr(params, 'data_augmentation_extra_cols') else 0 # randomly expand rows/cols\n",
        "        \n",
        "        # Set up parameters to be tuned\n",
        "        self.load_dictionary = load_dictionary # load dictionary from file rather than start from empty \n",
        "        self.dict_path = params.load_dict_from_path if load_dictionary else params.dict_path\n",
        "        # if self.load_dictionary:\n",
        "        #     self.dictionary = np.load(self.dict_path + 'dictionary.npy', allow_pickle=True).item()\n",
        "        #     self.word_to_index = np.load(self.dict_path + 'word_to_index.npy', allow_pickle=True).item()\n",
        "        #     self.index_to_word = np.load(self.dict_path + 'index_to_word.npy', allow_pickle=True).item()\n",
        "        # else:\n",
        "        #     self.dictionary = {'[PAD]':0, '[UNK]':0} # word/counts. to be updated in self.load_data() and self.update_docs_dictionary()\n",
        "        #     self.word_to_index = {}\n",
        "        #     self.index_to_word = {}\n",
        "        self.dictionary = np.load(self.dict_path + 'dictionary.npy', allow_pickle=True).item()\n",
        "        self.word_to_index = np.load(self.dict_path + 'word_to_index.npy', allow_pickle=True).item()\n",
        "        self.index_to_word = np.load(self.dict_path + 'index_to_word.npy', allow_pickle=True).item()\n",
        "\n",
        "        self.data_split = data_split # split data to training/validation, 0 for all for validation\n",
        "        self.data_mode = 2 # 0 to consider key and value as two different class, 1 the same class, 2 only value considered\n",
        "        self.remove_lowfreq_words = False # remove low frequency words when set as True\n",
        "        \n",
        "        self.num_classes = len(self.classes)\n",
        "        self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 1        \n",
        "        \n",
        "        # TBD: build a special cared dictionary\n",
        "        self.special_dict = {'*', '='} # map texts to specific tokens        \n",
        "        \n",
        "        # Load words and their class as training/validation docs and labels \n",
        "        self.training_doc_files = self.get_filenames(self.doc_path)\n",
        "        self.training_docs, self.training_labels = self.load_data(self.training_doc_files, update_dict=update_dict) # TBD: optimize the update dict flag\n",
        "        \n",
        "        # Polish and load dictionary/word_to_index/index_to_word as file\n",
        "        self.num_words = len(self.dictionary)              \n",
        "        self.update_word_to_index()\n",
        "        self.update_docs_dictionary(self.training_docs, 3, self.remove_lowfreq_words) # remove low frequency words and add it under the <unknown> key\n",
        "        \n",
        "        # Save dictionary/word_to_index/index_to_word as file\n",
        "        np.save(self.dict_path + 'dictionary.npy', self.dictionary)\n",
        "        np.save(self.dict_path + 'word_to_index.npy', self.word_to_index)\n",
        "        np.save(self.dict_path + 'index_to_word.npy', self.index_to_word)\n",
        "        np.save(self.dict_path + 'classes.npy', self.classes)\n",
        "        \n",
        "        # Split training / validation docs and show statistics\n",
        "        num_training = int(len(self.training_docs)*self.data_split)\n",
        "        data_to_be_fetched = [i for i in range(len(self.training_docs))]\n",
        "        selected_training_index = data_to_be_fetched[:num_training] \n",
        "        if self.random:\n",
        "            selected_training_index = random.sample(data_to_be_fetched, num_training)\n",
        "        selected_validation_index = list(set(data_to_be_fetched).difference(set(selected_training_index)))\n",
        "        self.validation_docs = [self.training_docs[x] for x in selected_validation_index]\n",
        "        self.training_docs = [self.training_docs[x] for x in selected_training_index]\n",
        "        self.validation_labels = self.training_labels\n",
        "        print('\\n\\nDATASET: %d vocabularies, %d target classes'%(len(self.dictionary), len(self.classes)))\n",
        "        print('DATASET: %d for training, %d for validation'%(len(self.training_docs), len(self.validation_docs)))\n",
        "        \n",
        "        # Load test files if any\n",
        "        self.test_doc_files = self.get_filenames(params.test_path) if hasattr(params, 'test_path') else []\n",
        "        self.test_docs, self.test_labels = self.load_data(self.test_doc_files, update_dict=update_dict) # TBD: optimize the update dict flag\n",
        "        print('DATASET: %d for test from %s \\n'%(len(self.test_docs), params.test_path if hasattr(params, 'test_path') else '_'))\n",
        "        \n",
        "        self.data_shape_statistic() # show data shape static\n",
        "        if len(self.training_docs) > 0:# adapt grid table size to all training dataset docs \n",
        "            self.rows, self.cols, _, _ = self.cal_rows_cols(self.training_docs)  \n",
        "            print('\\nDATASHAPE: data set with maximum grid table of ({},{}), updated.\\n'.format(self.rows, self.cols))    \n",
        "        else:\n",
        "            self.rows, self.cols = self.rows_ulimit, self.cols_ulimit\n",
        "                \n",
        "        # Call self.next_batch() outside to generate a batch of grid tables data and labels\n",
        "        self.training_data_tobe_fetched = [i for i in range(len(self.training_docs))]\n",
        "        self.validation_data_tobe_fetched = [i for i in range(len(self.validation_docs))]        \n",
        "        self.test_data_tobe_fetched = [i for i in range(len(self.test_docs))]\n",
        "        \n",
        "    \n",
        "    def update_word_to_index(self):\n",
        "        # Create word_to_index document\n",
        "        if self.load_dictionary:\n",
        "            max_index = len(self.word_to_index.keys())\n",
        "            for word in self.dictionary:\n",
        "                if word not in self.word_to_index:\n",
        "                    max_index += 1\n",
        "                    self.word_to_index[word] = max_index\n",
        "                    self.index_to_word[max_index] = word            \n",
        "        else:   \n",
        "            self.word_to_index = dict(list(zip(self.dictionary.keys(), list(range(self.num_words))))) \n",
        "            self.index_to_word = dict(list(zip(list(range(self.num_words)), self.dictionary.keys())))\n",
        "    \n",
        "    def update_docs_dictionary(self, docs, lower_limit, remove_lowfreq_words):\n",
        "        # assign docs words that appear less than @lower_limit times to word [UNK]\n",
        "        if remove_lowfreq_words: \n",
        "            for doc in docs:\n",
        "                for line in doc:\n",
        "                    [file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                        [image_w, image_h], max_row_words, max_col_words] = line \n",
        "                    if self.dictionary[dressed_text] < lower_limit:\n",
        "                        line = [file_name, '[UNK]', self.word_to_index['[UNK]'], [x_left, y_top, x_right, y_bottom], \\\n",
        "                                [image_w, image_h], max_row_words, max_col_words]\n",
        "                        self.dictionary[dressed_text] -= 1\n",
        "                        self.dictionary['[UNK]'] += 1\n",
        "    \n",
        "    def next_batch(self):\n",
        "        batch_size = self.batch_size\n",
        "        \n",
        "        while True:\n",
        "            if len(self.training_data_tobe_fetched) < batch_size:\n",
        "                self.training_data_tobe_fetched = [i for i in range(len(self.training_docs))]            \n",
        "            selected_index = random.sample(self.training_data_tobe_fetched, batch_size)\n",
        "            self.training_data_tobe_fetched = list(set(self.training_data_tobe_fetched).difference(set(selected_index)))\n",
        "    \n",
        "            training_docs = [self.training_docs[x] for x in selected_index]\n",
        "            \n",
        "            ## data augmentation in each batch if self.data_augmentation==True\n",
        "            rows, cols, pre_rows, pre_cols = self.cal_rows_cols(training_docs, extra_augmentation=self.data_augmentation_extra, dropout=self.data_augmentation_dropout)\n",
        "            if self.data_augmentation_extra:\n",
        "                print('Training grid AUGMENT size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, cols, pre_rows, pre_cols))\n",
        "            \n",
        "            grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "                self.positional_mapping(training_docs, self.training_labels, rows, cols)   \n",
        "            if updated_cols > cols:\n",
        "                print('Training grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, updated_cols, rows, cols))\n",
        "                grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "                    self.positional_mapping(training_docs, self.training_labels, rows, updated_cols, update_col=False)  \n",
        "            \n",
        "            ## load image and generate corresponding @ps_1dindices\n",
        "            images, ps_1d_indices = [], []\n",
        "\n",
        "            if self.use_cutie2:\n",
        "                images, ps_1d_indices = self.positional_sampling(self.doc_path, file_names, ps_indices_x, ps_indices_y, updated_cols)   \n",
        "                #print(\"image fetched {}\".format(len(images)))          \n",
        "                if len(images) == batch_size:\n",
        "                    break\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "                 'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "                 'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "                 'file_name': file_names, 'shape': [rows,cols]}\n",
        "        return batch\n",
        "    \n",
        "    def fetch_validation_data(self):\n",
        "        batch_size = 1\n",
        "        \n",
        "        while True:\n",
        "            if len(self.validation_data_tobe_fetched) == 0:\n",
        "                self.validation_data_tobe_fetched = [i for i in range(len(self.validation_docs))]            \n",
        "            selected_index = random.sample(self.validation_data_tobe_fetched, 1)\n",
        "            self.validation_data_tobe_fetched = list(set(self.validation_data_tobe_fetched).difference(set(selected_index)))\n",
        "    \n",
        "            validation_docs = [self.validation_docs[x] for x in selected_index]\n",
        "            \n",
        "            ## fixed validation shape leads to better result (to be verified)\n",
        "            real_rows, real_cols, _, _ = self.cal_rows_cols(validation_docs, extra_augmentation=False)\n",
        "            rows = max(self.rows_target, real_rows)\n",
        "            cols = max(self.rows_target, real_cols)\n",
        "            \n",
        "            grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "                self.positional_mapping(validation_docs, self.validation_labels, rows, cols)   \n",
        "            if updated_cols > cols:\n",
        "                print('Validation grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, updated_cols, rows, cols))\n",
        "                grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "                    self.positional_mapping(validation_docs, self.validation_labels, rows, updated_cols, update_col=False)     \n",
        "            \n",
        "            ## load image and generate corresponding @ps_1dindices\n",
        "            images, ps_1d_indices = [], []\n",
        "            if self.use_cutie2:\n",
        "                images, ps_1d_indices = self.positional_sampling(self.doc_path, file_names, ps_indices_x, ps_indices_y, updated_cols)  \n",
        "                if len(images) == batch_size:\n",
        "                    break        \n",
        "            else:\n",
        "                break\n",
        "            \n",
        "        def build_gt_pyramid(self, gt_classes):\n",
        "            gt_classes = np.array(gt_classes)\n",
        "            \n",
        "            rate = 4 # self.pooling_factor\n",
        "            b, h, w = np.shape(gt_classes)\n",
        "            same_padding_left = (rate-w%rate)//2 if w%rate else 0\n",
        "            same_padding_right = rate-(rate-w%rate)//2 if w%rate else 0\n",
        "            same_padding_top = (rate-h%rate)//2 if h%rate else 0\n",
        "            same_padding_bottom = rate-(rate-h%rate)//2 if h%rate else 0\n",
        "            for gt_class in gt_classes:\n",
        "                pad_v =  np.pad(gt_class, ((same_padding_top, same_padding_bottom), (0,0)), 'constant', constant_values=((0,0),(0,0)))\n",
        "                pad_h =  np.pad(gt_class, ((0,0), (same_padding_left, same_padding_right)), 'constant', constant_values=((0,0),(0,0)))\n",
        "                \n",
        "                ## find mask range for each single entity\n",
        "                num_entities = np.max(gt_classes) / self.num_classes\n",
        "                entity_ranges = [[] for _ in range(0,num_entities)]\n",
        "                for i in range(1, num_entities):\n",
        "                    if i % self.num_classes: # only consider non <DontCare> classes\n",
        "                        range_y, range_x = np.where(gt_classes==i)\n",
        "                        # entity_ranges[i] = [top, left, bottom, right, height, width]\n",
        "                        entity_ranges[i] = [min(range_y), min(range_x), max(range_y), max(range_x), \n",
        "                                            max(range_y) - min(range_y), max(range_x) - min(range_x)] \n",
        "                           \n",
        "                \n",
        "        \n",
        "        batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "                 'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "                 'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "                 'file_name': file_names, 'shape': [rows,cols]}\n",
        "        return batch\n",
        "    \n",
        "    def fetch_test_data(self): \n",
        "        batch_size = 1\n",
        "        \n",
        "        while True:\n",
        "            if len(self.test_data_tobe_fetched) == 0:\n",
        "                self.test_data_tobe_fetched = [i for i in range(len(self.test_docs))]\n",
        "                return None\n",
        "                        \n",
        "            selected_index = self.test_data_tobe_fetched[0]\n",
        "            self.test_data_tobe_fetched = list(set(self.test_data_tobe_fetched).difference(set([selected_index])))\n",
        "    \n",
        "            test_docs = [self.test_docs[selected_index]]\n",
        "            \n",
        "            real_rows, real_cols, _, _ = self.cal_rows_cols(test_docs, extra_augmentation=False)\n",
        "            rows = max(self.rows_target, real_rows) # small shaped documents have better performance with shape 64\n",
        "            cols = max(self.cols_target, real_cols) # large shaped docuemnts have better performance with shape 80\n",
        "                \n",
        "            grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "                self.positional_mapping(test_docs, self.test_labels, rows, cols)   \n",
        "            if updated_cols > cols:\n",
        "                print('Test grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, updated_cols, rows, cols))\n",
        "                grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "                    self.positional_mapping(test_docs, self.test_labels, rows, updated_cols, update_col=False)    \n",
        "                    \n",
        "            ## load image and generate corresponding @ps_1dindices\n",
        "            images, ps_1d_indices = [], []\n",
        "            if self.use_cutie2:\n",
        "                images, ps_1d_indices = self.positional_sampling(self.doc_test_path, file_names, ps_indices_x, ps_indices_y, updated_cols)          \n",
        "                if len(images) == batch_size:\n",
        "                    break          \n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "                 'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "                 'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "                 'file_name': file_names, 'shape': [rows,cols]}\n",
        "        return batch\n",
        "    \n",
        "    def form_label_matrix(self, gt_classes, target_h, target_w):\n",
        "        \"\"\"\n",
        "        build gt_classes and gt_masks with given target featuremap shape (height, width)\n",
        "        by inspecting bboxes regions (x,y,w,h)\n",
        "        for table / row / column identity segmentation\n",
        "        \"\"\"\n",
        "        def has_entity_with_augmentation(entity_ranges, roi, use_jittering=False):                    \n",
        "            ## find mask with maximum overlap\n",
        "            max_iou = 0\n",
        "            max_idx = None\n",
        "            roi_t, roi_l, roi_b, roi_r = roi\n",
        "            roi_h = roi_b - roi_t\n",
        "            roi_w = roi_r - roi_l\n",
        "            roi_cy = roi_t + roi_h/2\n",
        "            roi_cx = roi_l + roi_w/2\n",
        "            for idx, entity in enumerate(entity_ranges):\n",
        "                if len(entity):\n",
        "                    t, l, b, r, h, w = entity\n",
        "                    if l>roi_l and r<roi_r and t>roi_t and b<roi_b: # overlap 1\n",
        "                        iou = h*w / (roi_h*roi_w)\n",
        "                    elif l<roi_l and r>roi_r and t<roi_t and b>roi_b: # overlap 2\n",
        "                        iou = roi_h*roi_w / (h*w)\n",
        "                    elif l>roi_r or t>roi_b or b<roi_t or r<roi_l: # no intersection\n",
        "                        continue\n",
        "                    else:\n",
        "                        iou = min(h*w, roi_h*roi_w) / max(h*w, roi_h*roi_w)\n",
        "                        \n",
        "                    # TBD: add jittering augmentation method  \n",
        "                    if use_jittering:\n",
        "                        pass                          \n",
        "                    if iou > max_iou:\n",
        "                        max_idx = idx\n",
        "                        max_iou = iou\n",
        "                        \n",
        "            ## check centrality / containment / uniqueness\n",
        "            t, l, b, r, h, w = entity[idx]\n",
        "            cy = t + h/2\n",
        "            cx = l + w/2\n",
        "            if roi_t+h/3 < cy and cy < toi_b-h/3 and roi_l+w/3 < cx and cx < roi_r-w/3: # centrality\n",
        "                if (w > h and roi_w > w*0.9) or (w < h and roi_h > h*0.9): # containment\n",
        "                    if True: # uniqueness is already checked with maixmum IOU\n",
        "                        return True\n",
        "            return False                 \n",
        "    \n",
        "        shape = gt_classes.shape\n",
        "        rate_v = shape[0] / target_h\n",
        "        rate_h = shape[1] / target_w\n",
        "        dst_classes = [[[] for i in range(target_h)] for j in range(target_w)]\n",
        "        dst_masks = [[[] for i in range(target_h)] for j in range(target_w)]\n",
        "        for i in range(target_h):\n",
        "            for j in range(target_w):\n",
        "                roi = [rate_h*j, rate_v*i, rate_h*(j+1), rate_v*(i+1)] # [top, left, bottom, right]\n",
        "                \n",
        "                dst_classes[i][j] = has_entity_with_augmentation(entity_ranges, roi, False)\n",
        "                \n",
        "                mask = gt_classes[roi[1]:roi[3], roi[0]:roi[2]]\n",
        "                dst_masks[i][j] = mask if dst_classes[i][j] else np.zeros(np.shape(mask))\n",
        "        \n",
        "        return np.array(dst_classes), np.array(dst_masks)\n",
        "        \n",
        "    def data_shape_statistic(self):        \n",
        "        def shape_statistic(docs):\n",
        "            res_all = defaultdict(int)\n",
        "            res_row = defaultdict(int)\n",
        "            res_col = defaultdict(int)\n",
        "            for doc in docs:\n",
        "                rows, cols, _, _ = self.cal_rows_cols([doc])\n",
        "                res_all[rows] += 1\n",
        "                res_all[cols] += 1\n",
        "                res_row[rows] += 1\n",
        "                res_col[cols] += 1\n",
        "            res_all = sorted(res_all.items(), key=lambda x:x[0], reverse=True)\n",
        "            res_row = sorted(res_row.items(), key=lambda x:x[0], reverse=True)\n",
        "            res_col = sorted(res_col.items(), key=lambda x:x[0], reverse=True)\n",
        "            return res_all, res_row, res_col\n",
        "    \n",
        "        tss, tss_r, tss_c = shape_statistic(self.training_docs) # training shape static\n",
        "        vss, vss_r, vss_c = shape_statistic(self.validation_docs)\n",
        "        tess, tess_r, tess_c = shape_statistic(self.test_docs)\n",
        "        print(\"Training statistic: \", tss)\n",
        "        print(\"\\t num: \", len(self.training_docs))\n",
        "        print(\"\\t rows statistic: \", tss_r)\n",
        "        print(\"\\t cols statistic: \", tss_c)\n",
        "        print(\"\\nValidation statistic: \", vss)\n",
        "        print(\"\\t num: \", len(self.validation_docs))\n",
        "        print(\"\\t rows statistic: \", vss_r)\n",
        "        print(\"\\t cols statistic: \", vss_c)\n",
        "        print(\"\\nTest statistic: \", tess)\n",
        "        print(\"\\t num: \", len(self.test_docs))\n",
        "        print(\"\\t rows statistic: \", tess_r)\n",
        "        print(\"\\t cols statistic: \", tess_c)\n",
        "        \n",
        "        ## remove data samples not matching the training principle\n",
        "        def data_laundry(docs):\n",
        "            idx = 0\n",
        "            while idx < len(docs):\n",
        "                rows, cols, _, _ = self.cal_rows_cols([docs[idx]])\n",
        "                if rows > self.rows_ulimit or cols > self.cols_ulimit:\n",
        "                    del docs[idx]\n",
        "                else:\n",
        "                    idx += 1\n",
        "        if self.data_laundry:\n",
        "            print(\"\\nRemoving grids with shape larger than ({},{}).\".format(self.rows_ulimit, self.cols_ulimit))\n",
        "            data_laundry(self.training_docs)\n",
        "            data_laundry(self.validation_docs)\n",
        "            data_laundry(self.training_docs)\n",
        "        \n",
        "            tss, tss_r, tss_c = shape_statistic(self.training_docs) # training shape static\n",
        "            vss, vss_r, vss_c = shape_statistic(self.validation_docs)\n",
        "            tess, tess_r, tess_c = shape_statistic(self.test_docs)\n",
        "            print(\"Training statistic after laundary: \", tss)\n",
        "            print(\"\\t num: \", len(self.training_docs))\n",
        "            print(\"\\t rows statistic: \", tss_r)\n",
        "            print(\"\\t cols statistic: \", tss_c)\n",
        "            print(\"Validation statistic after laundary: \", vss)\n",
        "            print(\"\\t num: \", len(self.validation_docs))\n",
        "            print(\"\\t rows statistic: \", vss_r)\n",
        "            print(\"\\t cols statistic: \", vss_c)\n",
        "            print(\"Test statistic after laundary: \", tess)\n",
        "            print(\"\\t num: \", len(self.test_docs))\n",
        "            print(\"\\t rows statistic: \", tess_r)\n",
        "            print(\"\\t cols statistic: \", tess_c)\n",
        "    \n",
        "    def positional_mapping(self, docs, labels, rows, cols):\n",
        "        \"\"\"\n",
        "        docs in format:\n",
        "        [[file_name, text, word_id, [x_left, y_top, x_right, y_bottom], [left, top, right, bottom], max_row_words, max_col_words] ]\n",
        "        return grid_tables, gird_labels, dict bboxes {file_name:[]}, file_names\n",
        "        \"\"\"\n",
        "        grid_tables = []\n",
        "        gird_labels = []\n",
        "        ps_indices_x = [] # positional sampling indices\n",
        "        ps_indices_y = [] # positional sampling indices\n",
        "        bboxes = {}\n",
        "        label_mapids = []\n",
        "        bbox_mapids = [] # [{}, ] bbox identifier, each id with one or multiple bbox/bboxes\n",
        "        file_names = []\n",
        "        for doc in docs:\n",
        "            items = []\n",
        "            cols_e = 2 * cols # use @cols_e larger than required @cols as buffer\n",
        "            grid_table = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            grid_label = np.zeros([rows, cols_e], dtype=np.int8)\n",
        "            ps_x = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            ps_y = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            bbox = [[] for c in range(cols_e) for r in range(rows)]\n",
        "            bbox_id, bbox_mapid = 0, {} # one word in one or many positions in a bbox is mapped in bbox_mapid\n",
        "            label_mapid = [[] for _ in range(self.num_classes)] # each class is connected to several bboxes (words)\n",
        "            drawing_board = np.zeros([rows, cols_e], dtype=str)\n",
        "            for item in doc:\n",
        "                file_name = item[0]\n",
        "                text = item[1]\n",
        "                word_id = item[2]\n",
        "                x_left, y_top, x_right, y_bottom = item[3][:]\n",
        "                left, top, right, bottom = item[4][:]\n",
        "                \n",
        "                dict_id = self.word_to_index[text]                \n",
        "                entity_id, class_id = self.dress_class(file_name, word_id, labels)\n",
        "                \n",
        "                bbox_id += 1\n",
        "#                 if self.fill_bbox: # TBD: overlap avoidance\n",
        "#                     top = int(rows * y_top / image_h)\n",
        "#                     bottom = int(rows * y_bottom / image_h)\n",
        "#                     left = int(cols * x_left / image_w)\n",
        "#                     right = int(cols * x_right / image_w)\n",
        "#                     grid_table[top:bottom, left:right] = dict_id  \n",
        "#                     grid_label[top:bottom, left:right] = class_id  \n",
        "#                      \n",
        "#                     label_mapid[class_id].append(bbox_id)\n",
        "#                     for row in range(top, bottom):\n",
        "#                         for col in range(left, right):\n",
        "#                             bbox_mapid[row*cols+col] = bbox_id\n",
        "#                      \n",
        "#                     for y in range(top, bottom):\n",
        "#                         for x in range(left, right):\n",
        "#                             bbox[y][x] = [x_left, y_top, x_right-x_left, y_bottom-y_top]\n",
        "                label_mapid[class_id].append(bbox_id)    \n",
        "                \n",
        "                #v_c = (y_top - top + (y_bottom-y_top)/2) / (bottom-top)\n",
        "                #h_c = (x_left - left + (x_right-x_left)/2) / (right-left)\n",
        "                #v_c = (y_top + (y_bottom-y_top)/2) / bottom\n",
        "                #h_c = (x_left + (x_right-x_left)/2) / right \n",
        "                #v_c = (y_top-top) / (bottom-top)\n",
        "                #h_c = (x_left-left) / (right-left)\n",
        "                #v_c = (y_top) / (bottom)\n",
        "                #h_c = (x_left) / (right)\n",
        "                box_y = y_top + (y_bottom-y_top)/2\n",
        "                box_x = x_left # h_l is used for image feature map positional sampling\n",
        "                v_c = (y_top - top + (y_bottom-y_top)/2) / (bottom-top)\n",
        "                h_c = (x_left - left + (x_right-x_left)/2) / (right-left) # h_c is used for sorting items\n",
        "                row = int(rows * v_c) \n",
        "                col = int(cols * h_c) \n",
        "                items.append([row, col, [box_y, box_x], [v_c, h_c], file_name, dict_id, class_id, entity_id, bbox_id, [x_left, y_top, x_right-x_left, y_bottom-y_top]])                       \n",
        "            \n",
        "            items.sort(key=lambda x: (x[0], x[3], x[5])) # sort according to row > h_c > bbox_id\n",
        "            for item in items:\n",
        "                row, col, [box_y, box_x], [v_c, h_c], file_name, dict_id, class_id, entity_id, bbox_id, box = item\n",
        "                entity_class_id = entity_id*self.num_classes + class_id\n",
        "                \n",
        "                while col < cols and grid_table[row, col] != 0:\n",
        "                    col += 1            \n",
        "                \n",
        "                ptr = 0\n",
        "                if col == cols: # shift to find slot to drop the current item\n",
        "                    col -= 1\n",
        "                    while ptr<cols and grid_table[row, ptr] != 0:\n",
        "                        ptr += 1\n",
        "                    if ptr == cols:\n",
        "                        grid_table[row, :-1] = grid_table[row, 1:]\n",
        "                    else:\n",
        "                        grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                \n",
        "                grid_table[row, col] = dict_id\n",
        "                grid_label[row, col] = entity_class_id\n",
        "                ps_x[row, col] = box_x\n",
        "                ps_y[row, col] = box_y\n",
        "                bbox_mapid[row*cols+col] = bbox_id     \n",
        "                bbox[row*cols+col] = box\n",
        "\n",
        "                # # self.pm_strategy 0: skip if overlap\n",
        "                # # self.pm_strategy 1: shift to find slot if overlap\n",
        "                # # self.pm_strategy 2: expand grid table if overlap\n",
        "                # if self.pm_strategy == 0:\n",
        "                #     if col == cols:                     \n",
        "                #         print('overlap in {} row {} r{}c{}!'.\n",
        "                #               format(file_name, row, rows, cols))\n",
        "                #         #print(grid_table[row,:])\n",
        "                #         #print('overlap in {} <{}> row {} r{}c{}!'.\n",
        "                #         #      format(file_name, self.index_to_word[dict_id], row, rows, cols))\n",
        "                #     else:\n",
        "                #         grid_table[row, col] = dict_id\n",
        "                #         grid_label[row, col] = entity_class_id                       \n",
        "                #         bbox_mapid[row*cols+col] = bbox_id                       \n",
        "                #         bbox[row*cols+col] = box   \n",
        "                # elif self.pm_strategy==1 or self.pm_strategy==2:\n",
        "                #     ptr = 0\n",
        "                #     if col == cols: # shift to find slot to drop the current item\n",
        "                #         col -= 1\n",
        "                #         while ptr<cols and grid_table[row, ptr] != 0:\n",
        "                #             ptr += 1\n",
        "                #         if ptr == cols:\n",
        "                #             grid_table[row, :-1] = grid_table[row, 1:]\n",
        "                #         else:\n",
        "                #             grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                        \n",
        "                #     if self.pm_strategy == 2:\n",
        "                #         while col < cols_e and grid_table[row, col] != 0:\n",
        "                #             col += 1\n",
        "                #         if col > cols: # update maximum cols in current grid\n",
        "                #             print(grid_table[row,:col])\n",
        "                #             print('overlap in {} <{}> row {} r{}c{}!'.\n",
        "                #                   format(file_name, self.index_to_word[dict_id], row, rows, cols))\n",
        "                #             cols = col\n",
        "                #         if col == cols_e:      \n",
        "                #             print('overlap!')\n",
        "                    \n",
        "                #     grid_table[row, col] = dict_id\n",
        "                #     grid_label[row, col] = entity_class_id\n",
        "                #     ps_x[row, col] = box_x\n",
        "                #     ps_y[row, col] = box_y\n",
        "                #     bbox_mapid[row*cols+col] = bbox_id     \n",
        "                #     bbox[row*cols+col] = box\n",
        "                \n",
        "            cols = self.fit_shape(cols)\n",
        "            grid_table = grid_table[..., :cols]\n",
        "            grid_label = grid_label[..., :cols]\n",
        "            ps_x = np.array(ps_x[..., :cols])\n",
        "            ps_y = np.array(ps_y[..., :cols])\n",
        "            \n",
        "            # if DEBUG:\n",
        "            #     self.grid_visualization(file_name, grid_table, grid_label)\n",
        "            \n",
        "            grid_tables.append(np.expand_dims(grid_table, -1)) \n",
        "            gird_labels.append(grid_label) \n",
        "            ps_indices_x.append(ps_x)\n",
        "            ps_indices_y.append(ps_y)\n",
        "            bboxes[file_name] = bbox\n",
        "            label_mapids.append(label_mapid)\n",
        "            bbox_mapids.append(bbox_mapid)\n",
        "            file_names.append(file_name)\n",
        "            \n",
        "        return grid_tables, gird_labels, bboxes, label_mapids, bbox_mapids, file_names, cols, ps_indices_x, ps_indices_y\n",
        "    \n",
        "    def positional_sampling(self, path, file_names, ps_indices_x, ps_indices_y, updated_cols):\n",
        "        images, ps_1d_indices = [], []\n",
        "        \n",
        "        ## load image and generate corresponding @ps_1dindices\n",
        "        max_h, max_w = 0, updated_cols\n",
        "        for i in range(len(file_names)):\n",
        "            file_name = file_names[i]\n",
        "            file_path = join(path, file_name) # TBD: ensure image is upright\n",
        "            ps_1d_x = np.array(ps_indices_x[i], dtype=np.float32).reshape([-1])\n",
        "            ps_1d_y = np.array(ps_indices_y[i], dtype=np.float32).reshape([-1])\n",
        "            \n",
        "            image = cv2.imread(file_path)\n",
        "            if image is not None:\n",
        "                h, w, _ = image.shape # [h,w,c]\n",
        "                factor = max_w / w\n",
        "                \n",
        "                h = int(h*factor)\n",
        "                ps_1d_x *= factor # TBD: implement more accurate mapping method rather than nearest neighbor, since the .4 or .6 leads to two different sampling results\n",
        "                ps_1d_y *= factor                \n",
        "                \n",
        "                ps_1d = np.int32(np.floor(ps_1d_x) + np.floor(ps_1d_y) * max_w)\n",
        "                max_items = max_w * h - 1\n",
        "                for i in range(len(ps_1d)):\n",
        "                    if ps_1d[i] > max_items - 1:\n",
        "                        ps_1d[i] = max_items - 1\n",
        "                    \n",
        "                \n",
        "                image = cv2.resize(image, (max_w, h))\n",
        "                image = (image-127.5) / 255\n",
        "            else:\n",
        "                print('{} ignored due to image file not found.'.format(file_path))\n",
        "                image, ps_1d = None, None\n",
        "                break\n",
        "                \n",
        "            if image is not None and ps_1d is not None: # ignore data with no images                 \n",
        "                ps_1d_indices.append(ps_1d)\n",
        "                images.append(image)\n",
        "                h,w,c = image.shape\n",
        "                if h > max_h:\n",
        "                    max_h = h\n",
        "            else:\n",
        "                pass\n",
        "                #print('{} ignored due to image file not found.'.format(file_path))\n",
        "                \n",
        "        ## pad image to the same shape\n",
        "        for i,image in enumerate(images): \n",
        "            pad_img = np.zeros([max_h, max_w, 3], dtype=image.dtype)\n",
        "            pad_img[:image.shape[0], :, :] = image\n",
        "            images[i] = pad_img\n",
        "        \n",
        "        return images, ps_1d_indices\n",
        "    \n",
        "    def load_data(self, data_files, update_dict=False):\n",
        "        \"\"\"\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        load doc words with location and class returned in format: \n",
        "        [[file_name, text, word_id, [x_left, y_top, x_right, y_bottom], [left, top, right, bottom], max_row_words, max_col_words] ]\n",
        "        \"\"\"\n",
        "        label_dressed = {}\n",
        "        doc_dressed = []\n",
        "        if not data_files:\n",
        "            print(\"no data file found.\")        \n",
        "        for file in data_files:\n",
        "            with open(file, encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                file_id = data['global_attributes']['file_id']\n",
        "                \n",
        "                label = self.collect_label(file_id, data['fileds'])\n",
        "                # ignore corrupted data\n",
        "                if not label:\n",
        "                    continue                \n",
        "                label_dressed.update(label) \n",
        "                \n",
        "                data = self.collect_data(file_id, data['text_boxes'], update_dict)\n",
        "                for i in data:\n",
        "                    doc_dressed.append(i)\n",
        "                    \n",
        "        return doc_dressed, label_dressed       \n",
        "    \n",
        "    def cal_rows_cols(self, docs, extra_augmentation=False, dropout=False):                  \n",
        "        max_row = self.encoding_factor\n",
        "        max_col = self.encoding_factor\n",
        "        for doc in docs:\n",
        "            for line in doc: \n",
        "                _, _, _, _, _, max_row_words, max_col_words = line\n",
        "                if max_row_words > max_row:\n",
        "                    max_row = max_row_words\n",
        "                if max_col_words > max_col:\n",
        "                    max_col = max_col_words\n",
        "        \n",
        "        pre_rows = self.fit_shape(max_row) #(max_row//self.encoding_factor+1) * self.encoding_factor\n",
        "        pre_cols = self.fit_shape(max_col) #(max_col//self.encoding_factor+1) * self.encoding_factor\n",
        "        \n",
        "        rows, cols = 0, 0\n",
        "        if extra_augmentation:\n",
        "            pad_row = int(random.gauss(0, self.da_extra_rows*self.encoding_factor)) #abs(random.gauss(0, u))\n",
        "            pad_col = int(random.gauss(0, self.da_extra_cols*self.encoding_factor)) #random.randint(0, u)\n",
        "            \n",
        "            if self.augment_strategy == 1: # strategy 1: augment data by increasing grid shape sizes\n",
        "                pad_row = abs(pad_row)\n",
        "                pad_col = abs(pad_col)\n",
        "                rows = self.fit_shape(max_row+pad_row) # apply upper boundary to avoid OOM\n",
        "                cols = self.fit_shape(max_col+pad_col) # apply upper boundary to avoid OOM\n",
        "            elif self.augment_strategy == 2 or self.augment_strategy == 3: # strategy 2: augment by increasing or decreasing the target gird shape size\n",
        "                rows = self.fit_shape(max(self.rows_target+pad_row, max_row)) # protect grid shape\n",
        "                cols = self.fit_shape(max(self.cols_target+pad_col, max_col)) # protect grid shape\n",
        "            else:\n",
        "                raise Exception('unknown augment strategy')\n",
        "            rows = min(rows, self.rows_ulimit) # apply upper boundary to avoid OOM\n",
        "            cols = min(cols, self.cols_ulimit) # apply upper boundary to avoid OOM                                \n",
        "        else:\n",
        "            rows = pre_rows\n",
        "            cols = pre_cols\n",
        "        return rows, cols, pre_rows, pre_cols \n",
        "    \n",
        "    def fit_shape(self, shape): # modify shape size to fit the encoding factor\n",
        "        while shape % self.encoding_factor:\n",
        "            shape += 1\n",
        "        return shape\n",
        "    \n",
        "    def expand_shape(self, shape): # expand shape size with step 2\n",
        "        return self.fit_shape(shape+1)\n",
        "        \n",
        "    def collect_data(self, file_name, content, update_dict):\n",
        "        \"\"\"\n",
        "        dress and preserve only interested data.\n",
        "        \"\"\"          \n",
        "        content_dressed = []\n",
        "        left, top, right, bottom, buffer = 9999, 9999, 0, 0, 2\n",
        "        for line in content:\n",
        "            bbox = line['bbox'] # handle data corrupt\n",
        "            if len(bbox) == 0:\n",
        "                continue\n",
        "            if line['text'] in self.special_dict: # ignore potential overlap causing characters\n",
        "                continue\n",
        "            \n",
        "            x_left, y_top, x_right, y_bottom = self.dress_bbox(bbox)        \n",
        "            # TBD: the real image size is better for calculating the relative x/y/w/h\n",
        "            if x_left < left: left = x_left - buffer\n",
        "            if y_top < top: top = y_top - buffer\n",
        "            if x_right > right: right = x_right + buffer\n",
        "            if y_bottom > bottom: bottom = y_bottom + buffer\n",
        "            \n",
        "            word_id = line['word_id']\n",
        "            dressed_texts = self.dress_text(line['text'], update_dict)\n",
        "            \n",
        "            num_block = len(dressed_texts)\n",
        "            for i, dressed_text in enumerate(dressed_texts): # handling tokenized text, separate bbox\n",
        "                new_left = int(x_left + (x_right-x_left) / num_block * (i))\n",
        "                new_right = int(x_left + (x_right-x_left) / num_block * (i+1))\n",
        "                content_dressed.append([file_name, dressed_text, word_id, [new_left, y_top, new_right, y_bottom]])\n",
        "            \n",
        "        # initial calculation of maximum number of words in rows/cols in terms of image size\n",
        "        num_words_row = [0 for _ in range(bottom)] # number of words in each row\n",
        "        num_words_col = [0 for _ in range(right)] # number of words in each column\n",
        "        for line in content_dressed:\n",
        "            _, _, _, [x_left, y_top, x_right, y_bottom] = line\n",
        "            for y in range(y_top, y_bottom):\n",
        "                num_words_row[y] += 1\n",
        "            for x in range(x_left, x_right):\n",
        "                num_words_col[x] += 1\n",
        "        max_row_words = self.fit_shape(max(num_words_row))\n",
        "        max_col_words = 0#self.fit_shape(max(num_words_col))\n",
        "        \n",
        "        # further expansion of maximum number of words in rows/cols in terms of grid shape\n",
        "        max_rows = max(self.encoding_factor, max_row_words)\n",
        "        max_cols = max(self.encoding_factor, max_col_words)\n",
        "        DONE = False\n",
        "        while not DONE:\n",
        "            DONE = True\n",
        "            grid_table = np.zeros([max_rows, max_cols], dtype=np.int32)\n",
        "            for line in content_dressed:\n",
        "                _, _, _, [x_left, y_top, x_right, y_bottom] = line\n",
        "                row = int(max_rows * (y_top - top + (y_bottom-y_top)/2) / (bottom-top))\n",
        "                col = int(max_cols * (x_left - left + (x_right-x_left)/2) / (right-left))\n",
        "                #row = int(max_rows * (y_top + (y_bottom-y_top)/2) / (bottom))\n",
        "                #col = int(max_cols * (x_left + (x_right-x_left)/2) / (right))\n",
        "                #row = int(max_rows * (y_top-top) / (bottom-top))\n",
        "                #col = int(max_cols * (x_left-left) / (right-left))\n",
        "                #row = int(max_rows * (y_top) / (bottom))\n",
        "                #col = int(max_cols * (x_left) / (right))\n",
        "                #row = int(max_rows * (y_top + (y_bottom-y_top)/2) / bottom)  \n",
        "                #col = int(max_cols * (x_left + (x_right-x_left)/2) / right) \n",
        "                \n",
        "                while col < max_cols and grid_table[row, col] != 0: # shift to find slot to drop the current item\n",
        "                    col += 1\n",
        "                if col == max_cols: # shift to find slot to drop the current item\n",
        "                    col -= 1\n",
        "                    ptr = 0\n",
        "                    while ptr<max_cols and grid_table[row, ptr] != 0:\n",
        "                        ptr += 1\n",
        "                    if ptr == max_cols: # overlap cannot be solved in current row, then expand the grid\n",
        "                        max_cols = self.expand_shape(max_cols)\n",
        "                        DONE = False\n",
        "                        break\n",
        "                    \n",
        "                    grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                \n",
        "                if DONE:\n",
        "                    if row > max_rows or col>max_cols:\n",
        "                        print('wrong')\n",
        "                    grid_table[row, col] = 1\n",
        "        \n",
        "        max_rows = self.fit_shape(max_rows)\n",
        "        max_cols = self.fit_shape(max_cols)\n",
        "        \n",
        "        #print('{} collected in shape: {},{}'.format(file_name, max_rows, max_cols))\n",
        "        \n",
        "        # segment grid into two parts if number of cols is larger than self.cols_target\n",
        "        data = []\n",
        "        if self.segment_grid and max_cols > self.cols_segment:\n",
        "            content_dressed_left = []\n",
        "            content_dressed_right = []\n",
        "            cnt = defaultdict(int) # counter for number of words in a specific row\n",
        "            cnt_l, cnt_r = defaultdict(int), defaultdict(int) # update max_cols if larger than self.cols_segment\n",
        "            left_boundary = max_cols - self.cols_segment\n",
        "            right_boundary = self.cols_segment\n",
        "            for i, line in enumerate(content_dressed):\n",
        "                file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom] = line\n",
        "                \n",
        "                row = int(max_rows * (y_top + (y_bottom-y_top)/2) / bottom)\n",
        "                cnt[row] += 1                \n",
        "                if cnt[row] <= left_boundary:\n",
        "                    cnt_l[row] += 1\n",
        "                    content_dressed_left.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, self.cols_segment])\n",
        "                elif left_boundary < cnt[row] <= right_boundary:\n",
        "                    cnt_l[row] += 1\n",
        "                    cnt_r[row] += 1\n",
        "                    content_dressed_left.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, self.cols_segment])\n",
        "                    content_dressed_right.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max(max(cnt_r.values()), self.cols_segment)])\n",
        "                else:\n",
        "                    cnt_r[row] += 1\n",
        "                    content_dressed_right.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max(max(cnt_r.values()), self.cols_segment)])\n",
        "            #print(sorted(cnt.items(), key=lambda x:x[1], reverse=True))\n",
        "            #print(sorted(cnt_l.items(), key=lambda x:x[1], reverse=True))\n",
        "            #print(sorted(cnt_r.items(), key=lambda x:x[1], reverse=True))\n",
        "            if max(cnt_l.values()) < 2*self.cols_segment:\n",
        "                data.append(content_dressed_left)\n",
        "            if max(cnt_r.values()) < 2*self.cols_segment: # avoid OOM, which tends to happen in the right side\n",
        "                data.append(content_dressed_right)\n",
        "        else:\n",
        "            for i, line in enumerate(content_dressed): # append height/width/numofwords to the list\n",
        "                file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom] = line\n",
        "                content_dressed[i] = [file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max_cols ]\n",
        "            data.append(content_dressed)\n",
        "        return data\n",
        "    \n",
        "    def collect_label(self, file_id, content):\n",
        "        \"\"\"\n",
        "        dress and preserve only interested data.\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        \"\"\"\n",
        "        label_dressed = dict()\n",
        "        label_dressed[file_id] = {cls:[] for cls in self.classes[1:]}\n",
        "        for line in content:\n",
        "            cls = line['field_name']\n",
        "            if cls in self.classes:\n",
        "                #identity = line.get('identity', 0) \n",
        "                label_dressed[file_id][cls].append( {'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''} )\n",
        "                label_dressed[file_id][cls][-1]['key_id'] = line.get('key_id', [])\n",
        "                label_dressed[file_id][cls][-1]['value_id'] = line['value_id'] # value_id\n",
        "                label_dressed[file_id][cls][-1]['key_text'] = line.get('key_text', []) \n",
        "                label_dressed[file_id][cls][-1]['value_text'] = line['value_text'] # value_text\n",
        "                \n",
        "        # handle corrupted data\n",
        "        for cls in label_dressed[file_id]: \n",
        "            for idx, label in enumerate(label_dressed[file_id][cls]):\n",
        "                if len(label) == 0: # no relevant class in sample @file_id\n",
        "                    continue\n",
        "                if (len(label['key_text'])>0 and len(label['key_id'])==0) or \\\n",
        "                   (len(label['value_text'])>0 and len(label['value_id'])==0):\n",
        "                    return None\n",
        "            \n",
        "        return label_dressed\n",
        "\n",
        "    def dress_class(self, file_name, word_id, labels):\n",
        "        \"\"\"\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        \"\"\"\n",
        "        if file_name in labels:\n",
        "            for cls, cls_labels in labels[file_name].items():\n",
        "                for idx, cls_label in enumerate(cls_labels):\n",
        "                    for key, values in cls_label.items():\n",
        "                        if (key=='key_id' or key=='value_id') and word_id in values:\n",
        "                            if key == 'key_id':\n",
        "                                if self.data_mode == 0:\n",
        "                                    return idx, self.classes.index(cls) * 2 - 1 # odd\n",
        "                                elif self.data_mode == 1:\n",
        "                                    return idx, self.classes.index(cls)\n",
        "                                else: # ignore key_id when self.data_mode is not 0 or 1\n",
        "                                    return 0, 0\n",
        "                            elif key == 'value_id':\n",
        "                                if self.data_mode == 0:\n",
        "                                    return idx, self.classes.index(cls) * 2 # even \n",
        "                                else: # when self.data_mode is 1 or 2\n",
        "                                    return idx, self.classes.index(cls) \n",
        "            return 0, 0 # 0 is of class type 'DontCare'\n",
        "        print(\"No matched labels found for {}\".format(file_name))\n",
        "    \n",
        "    def dress_text(self, text, update_dict):\n",
        "        \"\"\"\n",
        "        three cases covered: \n",
        "        alphabetic string, numeric string, special character\n",
        "        \"\"\"\n",
        "        string = text.lower()\n",
        "        for i, c in enumerate(string):\n",
        "            if is_number(c):\n",
        "                string = string[:i] + '0' + string[i+1:]\n",
        "                \n",
        "        strings = [string]\n",
        "        # if self.tokenize:\n",
        "        #     strings = self.tokenizer.tokenize(strings[0])\n",
        "        #     #print(string, '-->', strings)\n",
        "            \n",
        "        for idx, string in enumerate(strings):            \n",
        "            if string.isalpha():\n",
        "                if string in self.special_dict:\n",
        "                    string = self.special_dict[string]\n",
        "                # TBD: convert a word to its most similar word in a known vocabulary\n",
        "            elif is_number(string):\n",
        "                pass\n",
        "            elif len(string)==1: # special character\n",
        "                pass\n",
        "            else:\n",
        "                # TBD: seperate string as parts for alpha and number combinated strings\n",
        "                #string = re.findall('[a-z]+', string)\n",
        "                pass            \n",
        "            \n",
        "            if string not in self.dictionary.keys():\n",
        "                if update_dict:\n",
        "                    self.dictionary[string] = 0\n",
        "                else:\n",
        "                    #print('unknown text: ' + string)\n",
        "                    string = '[UNK]' # TBD: take special care to unmet words\\\n",
        "            self.dictionary[string] += 1\n",
        "            \n",
        "            strings[idx] = string\n",
        "        return strings\n",
        "            \n",
        "    def dress_bbox(self, bbox):\n",
        "        positions = np.array(bbox).reshape([-1])\n",
        "        x_left = max(0, min(positions[0::2]))\n",
        "        x_right = max(positions[0::2])\n",
        "        y_top = max(0, min(positions[1::2]))\n",
        "        y_bottom = max(positions[1::2])\n",
        "        w = x_right - x_left\n",
        "        h = y_bottom - y_top\n",
        "        return int(x_left), int(y_top), int(x_right), int(y_bottom)       \n",
        "    \n",
        "    def get_filenames(self, data_path):\n",
        "        files = []\n",
        "        for dirpath,dirnames,filenames in walk(data_path):\n",
        "            for filename in filenames:\n",
        "                file = join(dirpath,filename)\n",
        "                if file.endswith('csv') or file.endswith('json'):\n",
        "                    files.append(file)\n",
        "        return files       \n",
        "            \n",
        "    # def grid_visualization(self, file_name, grid, label):\n",
        "    #     import cv2\n",
        "    #     height, width = np.shape(grid)\n",
        "    #     grid_box_h, grid_box_w = 20, 40\n",
        "    #     palette = np.zeros([height*grid_box_h, width*grid_box_w, 3], np.uint8)\n",
        "    #     font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    #     gt_color = [[255, 250, 240], [152, 245, 255], [127, 255, 212], [100, 149, 237], \n",
        "    #                 [192, 255, 62], [175, 238, 238], [255, 130, 171], [240, 128, 128], [255, 105, 180]]\n",
        "    #     cv2.putText(palette, file_name+\"({},{})\".format(height,width), (grid_box_h,grid_box_w), font, 0.6, [255,0,0])  \n",
        "    #     for h in range(height):\n",
        "    #         cv2.line(palette, (0,h*grid_box_h), (width*grid_box_w, h*grid_box_h), (100,100,100))\n",
        "    #         for w in range(width):\n",
        "    #             if grid[h,w]:\n",
        "    #                 org = (int((w+1)*grid_box_w*0.7),int((h+1)*grid_box_h*0.9))\n",
        "    #                 color = gt_color[label[h,w]]\n",
        "    #                 cv2.putText(palette, self.index_to_word[grid[h,w]], org, font, 0.4, color)        \n",
        "        \n",
        "    #     img = cv2.imread(self.doc_path+'/'+file_name)\n",
        "    #     if img is not None:\n",
        "    #         shape = list(img.shape)\n",
        "    #         max_len = 768\n",
        "    #         factor = max_len / max(shape)\n",
        "    #         shape[0], shape[1] = [int(s*factor) for s in shape[:2]]\n",
        "    #         img = cv2.resize(img, (shape[1], shape[0]))  \n",
        "    #         cv2.imshow(\"img\", img)\n",
        "    #     cv2.imshow(\"grid\", palette)\n",
        "    #     cv2.waitKey(0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oggp9nxa6NuG"
      },
      "source": [
        "# 2. Set up Model and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-39pT9WRQV"
      },
      "source": [
        "# Set up layers for the model framework\n",
        "\n",
        "def layer(op):\n",
        "    def layer_decorated(self, *args, **kwargs):\n",
        "        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))        \n",
        "        if len(self.layer_inputs) == 0:\n",
        "            raise RuntimeError('No input variables found for layers %s' % name)\n",
        "        elif len(self.layer_inputs) == 1:\n",
        "            layer_input = self.layer_inputs[0]\n",
        "        else:\n",
        "            layer_input = list(self.layer_inputs)            \n",
        "            \n",
        "        layer_output = op(self, layer_input, *args, **kwargs)\n",
        "        \n",
        "        self.layers[name] = layer_output\n",
        "        self.feed(layer_output)\n",
        "        \n",
        "        return self\n",
        "    return layer_decorated\n",
        "    \n",
        "    \n",
        "class Model(object):\n",
        "    def __init__(self, trainable=True):\n",
        "        self.layers = dict()      \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "    \n",
        "    \n",
        "    # def build_loss(self):\n",
        "    #     raise NotImplementedError('Must be subclassed.')\n",
        "    \n",
        "    \n",
        "    def setup(self):        \n",
        "        raise NotImplementedError('Must be subclassed.')\n",
        "     \n",
        "    \n",
        "    @layer\n",
        "    def embed(self, layer_input, vocabulary_size, embedding_size, name, dropout=1, trainable=True):\n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_embedding = tf.random_uniform_initializer(-1.0, 1.0)\n",
        "            embeddings = self.make_var('weights', [vocabulary_size, embedding_size], init_embedding, None, trainable)\n",
        "            shape = tf.shape(layer_input)\n",
        "            \n",
        "            reshaped_input = tf.reshape(layer_input, [-1])\n",
        "            e = tf.nn.embedding_lookup(embeddings, reshaped_input)\n",
        "            e = tf.nn.dropout(e, dropout)\n",
        "            reshaped_e = tf.reshape(e, [shape[0], shape[1], shape[2], embedding_size])\n",
        "            return reshaped_e\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def bert_embed(self, layer_input, vocab_size, embedding_size=768, use_one_hot_embeddings=False, \n",
        "                   initializer_range=0.02, name=\"embeddings\", trainable=False):\n",
        "        with tf.compat.v1.variable_scope(\"bert\"):\n",
        "          with tf.compat.v1.variable_scope(\"embeddings\"):\n",
        "            # Perform embedding lookup on the word ids.\n",
        "            (embedding_output, embedding_table) = self.embedding_lookup(\n",
        "                input_ids=layer_input, vocab_size=vocab_size, embedding_size=embedding_size,\n",
        "                initializer_range=initializer_range,\n",
        "                word_embedding_name=\"word_embeddings\",\n",
        "                use_one_hot_embeddings=use_one_hot_embeddings,\n",
        "                trainable=trainable)\n",
        "            self.embedding_table = embedding_table # the inherited class need a self.embedding_table variable\n",
        "            return embedding_output        \n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def positional_sampling(self, layer_input, feature_dimension, name='positional_sampling'):\n",
        "        featuremap = layer_input[0]\n",
        "        batch_indices = layer_input[1]\n",
        "        grid = layer_input[2]        \n",
        "        \n",
        "        shape_grid = tf.shape(grid)\n",
        "        \n",
        "        featuremap_flat = tf.reshape(featuremap, [shape_grid[0], -1, feature_dimension])        \n",
        "        batch_indices_flat = tf.reshape(batch_indices, [shape_grid[0], -1])        \n",
        "        batch_ps_flat = tf.batch_gather(featuremap_flat, batch_indices_flat)\n",
        "        \n",
        "        b, h, w, c = shape_grid[0], shape_grid[1], shape_grid[2], feature_dimension\n",
        "        return tf.reshape(batch_ps_flat, [b,h,w,c])\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def sepconv(self, layer_input, k_h, k_w, cardinality, compression, name, activation='relu', trainable=True):\n",
        "        \"\"\" customized seperable convolution\n",
        "        \"\"\"\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')\n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape().as_list()[-1]\n",
        "            \n",
        "            layer_output = []\n",
        "            c = c_i / cardinality / compression\n",
        "            for _ in range(cardinality):\n",
        "                a = self.convolution(convolve, activate, layer_input, 1, 1, c_i, c,\n",
        "                                     init_weights, init_biases, regularizer, trainable, '0_{}'.format(_))                \n",
        "                a = self.convolution(convolve, activate, a, k_h, k_w, c, c, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '1_{}'.format(_))\n",
        "                a = self.convolution(convolve, activate, a, 1, 1, c, c_i, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '2_{}'.format(_))\n",
        "                layer_output.append(a)\n",
        "            layer_output = tf.add_n(layer_output)\n",
        "            return tf.add(layer_output, layer_input)\n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def up_sepconv(self, layer_input, k_h, k_w, cardinality, compression, name, activation='relu', trainable=True):\n",
        "        \"\"\" customized upscale seperable convolution\n",
        "        \"\"\"\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')        \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            shape = tf.shape(layer_input)\n",
        "            h = shape[1]\n",
        "            w = shape[2]\n",
        "            layer_input = tf.image.resize_nearest_neighbor(layer_input, [2*h, 2*w])\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape().as_list()[-1]\n",
        "            \n",
        "            layer_output = []\n",
        "            c = c_i / cardinality / compression\n",
        "            for _ in range(cardinality):\n",
        "                a = self.convolution(convolve, activate, layer_input, 1, 1, c_i, c,\n",
        "                                     init_weights, init_biases, regularizer, trainable, '0_{}'.format(_))                \n",
        "                a = self.convolution(convolve, activate, a, k_h, k_w, c, c, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '1_{}'.format(_))\n",
        "                a = self.convolution(convolve, activate, a, 1, 1, c, c_i, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '2_{}'.format(_))\n",
        "                layer_output.append(a)\n",
        "            layer_output = tf.add_n(layer_output)\n",
        "            return tf.add(layer_output, layer_input)\n",
        "        \n",
        "        \n",
        "    @layer\n",
        "    def dense_block(self, layer_input, k_h, k_w, c_o, depth, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')\n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)  \n",
        "            \n",
        "            layer_tmp = layer_input\n",
        "            for d in range(depth):          \n",
        "                c_i = layer_tmp.get_shape()[-1]\n",
        "                a = self.convolution(convolve, activate, layer_tmp, 1, 1, c_i, c_i//2,\n",
        "                                     init_weights, init_biases, regularizer, trainable)\n",
        "                \n",
        "                a = self.convolution(convolve, activate, a, k_h, k_w, c_i, c_o, \n",
        "                                     init_weights, init_biases, regularizer, trainable)\n",
        "                \n",
        "                layer_tmp = tf.concat([a, layer_input], 3)\n",
        "                \n",
        "            return layer_tmp\n",
        "            \n",
        "        \n",
        "    @layer\n",
        "    def conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,s_h,s_w,1], 'SAME')\n",
        "        #convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, 2, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "     \n",
        "     \n",
        "    @layer\n",
        "    def dilate_conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, rate, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, rate, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def dilate_module(self, layer_input, k_h, k_w, c_o, s_h, s_w, rate, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, rate, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def up_conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, name, factor=2, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,s_h,s_w,1], 'SAME')\n",
        "        #convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, 2, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')        \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            shape = tf.shape(layer_input)\n",
        "            h = shape[1]\n",
        "            w = shape[2]\n",
        "            layer_input = tf.image.resize_nearest_neighbor(layer_input, [factor*h, factor*w])\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def attention(self, layer_input, num_heads, name, att_dropout=0.0, hidden_dropout=0.1, trainable=True):\n",
        "    #     \"\"\"\n",
        "    #     implement self attention with residual addition,\n",
        "    #     layer_input[0] and layer_input[1] should have the same shape for residual addition \n",
        "    #     \"\"\"\n",
        "    #     f = layer_input[0]\n",
        "    #     x = layer_input[1]\n",
        "        \n",
        "    #     convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "    #     with tf.variable_scope(name) as scope:\n",
        "    #         init_weights = tf.truncated_normal_initializer(0.0, 0.02)\n",
        "    #         regularizer = self.l2_regularizer(self.weight_decay)\n",
        "    #         shape = tf.shape(f)\n",
        "    #         c_i = f.get_shape()[-1]\n",
        "    #         c_o = f.get_shape()[-1]\n",
        "    #         c_a = c_o // num_heads # attention kernel depth, size per head\n",
        "            \n",
        "    #         query = self.make_var('weights_query', [1, 1, c_i, c_a], init_weights, regularizer, trainable)\n",
        "    #         query_layer = convolve(f, query) # [B, H, W, c_a]\n",
        "    #         query_layer = tf.reshape(query_layer, [shape[0], -1, c_a]) # [B, H*W, c_a]\n",
        "            \n",
        "    #         key = self.make_var('weights_key', [1, 1, c_i, c_a], init_weights, regularizer, trainable)\n",
        "    #         key_layer = convolve(f, key) # [B, H, W, c_a]\n",
        "    #         key_layer = tf.reshape(key_layer, [shape[0], -1, c_a]) # [B, H*W, c_a]\n",
        "            \n",
        "    #         value = self.make_var('weights_value', [1, 1, c_i, c_o], init_weights, regularizer, trainable)\n",
        "    #         value_layer = convolve(f, value) \n",
        "    #         value_layer = tf.reshape(value_layer, [shape[0], -1, c_o])# [B, H*W, c_o]\n",
        "            \n",
        "    #         attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) # [B, H*W, H*W]\n",
        "    #         attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(c_a.value)))\n",
        "            \n",
        "    #         attention_probs = tf.nn.softmax(attention_scores)\n",
        "    #         #attention_probs = dropout(attention_probs, att_dropout)\n",
        "            \n",
        "    #         context_layer = tf.matmul(attention_probs, value_layer) # [B, H*W, c_o]\n",
        "    #         context_layer = tf.reshape(context_layer, shape) # [B, H, W, c_o]\n",
        "            \n",
        "    #         kernel = self.make_var('output', [1, 1, c_o, c_o], init_weights, regularizer, trainable)\n",
        "    #         attention_output = convolve(context_layer, kernel) \n",
        "    #         #attention_output = dropout(attention_output, hidden_dropout)\n",
        "    #         attention_output = attention_output + x\n",
        "            \n",
        "    #         return tf.contrib.layers.instance_norm(attention_output, center=False, scale=False)\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def concat(self, layer_input, axis, name):\n",
        "        return tf.concat(layer_input, axis)\n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def add(self, layer_input, name):\n",
        "    #     return tf.math.add_n(layer_input)\n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def max_pool(self, layer_input, k_h, k_w, s_h, s_w, name, padding='SAME'):\n",
        "        return tf.nn.max_pool(layer_input, [1,k_h,k_w,1], [1,s_h,s_w,1], name=name, padding=padding)\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def global_pool(self, layer_input, name):\n",
        "        shape = tf.shape(layer_input)\n",
        "        h = shape[1]\n",
        "        w = shape[2]\n",
        "        output = tf.reduce_mean(layer_input, [1,2], keepdims=True, name=name)\n",
        "        return tf.image.resize_nearest_neighbor(output, [h, w])\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def softmax(self, layer_input, name):\n",
        "        return tf.nn.softmax(layer_input, name=name)      \n",
        "    \n",
        "    \n",
        "    # def embedding_lookup(self, input_ids, vocab_size, embedding_size=768,\n",
        "    #                      initializer_range=0.02, word_embedding_name=\"word_embeddings\",\n",
        "    #                      use_one_hot_embeddings=False, trainable=False):\n",
        "    #     \"\"\"Looks up words embeddings for id tensor.\n",
        "        \n",
        "    #     Args:\n",
        "    #       input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
        "    #         ids.\n",
        "    #       vocab_size: int. Size of the embedding vocabulary.\n",
        "    #       embedding_size: int. Width of the word embeddings.\n",
        "    #       initializer_range: float. Embedding initialization range.\n",
        "    #       word_embedding_name: string. Name of the embedding table.\n",
        "    #       use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
        "    #         embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n",
        "    #         for TPUs.\n",
        "        \n",
        "    #     Returns:\n",
        "    #       float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "    #     \"\"\"\n",
        "    #     bert_vocab_size = 119547\n",
        "    #     # This function assumes that the input is of shape [batch_size, seq_length,\n",
        "    #     # num_inputs].\n",
        "    #     #\n",
        "    #     # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
        "    #     # reshape to [batch_size, seq_length, 1].\n",
        "    #     if input_ids.shape.ndims == 3: # originally 2\n",
        "    #         input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "        \n",
        "    #     bert_embedding_table = embedding_table = tf.get_variable(\n",
        "    #         name=word_embedding_name,\n",
        "    #         shape=[bert_vocab_size, embedding_size],\n",
        "    #         initializer=tf.truncated_normal_initializer(stddev=initializer_range),\n",
        "    #         trainable=trainable)\n",
        "    #     if vocab_size > bert_vocab_size: # handle dict augmentation\n",
        "    #         embedding_table_plus = tf.get_variable(\n",
        "    #             name=word_embedding_name + '_plus',\n",
        "    #             shape=[vocab_size-bert_vocab_size, embedding_size],\n",
        "    #             initializer=tf.truncated_normal_initializer(stddev=initializer_range),\n",
        "    #             trainable=True)\n",
        "    #         embedding_table = tf.concat([embedding_table, embedding_table_plus], 0)        \n",
        "        \n",
        "    #     if use_one_hot_embeddings:\n",
        "    #         flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "    #         one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    #         output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "    #     else:\n",
        "    #         output = tf.nn.embedding_lookup(embedding_table, input_ids)\n",
        "        \n",
        "    #     input_shape = self.get_shape_list(input_ids)\n",
        "        \n",
        "    #     output = tf.reshape(output,\n",
        "    #                         input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "    #     return (output, bert_embedding_table)\n",
        "    \n",
        "    # def get_shape_list(self, tensor, expected_rank=None, name=None):\n",
        "    #     \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "        \n",
        "    #     Args:\n",
        "    #       tensor: A tf.Tensor object to find the shape of.\n",
        "    #       expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "    #         specified and the `tensor` has a different rank, and exception will be\n",
        "    #         thrown.\n",
        "    #       name: Optional name of the tensor for the error message.\n",
        "        \n",
        "    #     Returns:\n",
        "    #       A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    #       be returned as python integers, and dynamic dimensions will be returned\n",
        "    #       as tf.Tensor scalars.\n",
        "    #     \"\"\"\n",
        "    #     if name is None:\n",
        "    #       name = tensor.name\n",
        "        \n",
        "    #     if expected_rank is not None:\n",
        "    #       assert_rank(tensor, expected_rank, name)\n",
        "        \n",
        "    #     shape = tensor.shape.as_list()\n",
        "        \n",
        "    #     non_static_indexes = []\n",
        "    #     for (index, dim) in enumerate(shape):\n",
        "    #       if dim is None:\n",
        "    #         non_static_indexes.append(index)\n",
        "        \n",
        "    #     if not non_static_indexes:\n",
        "    #       return shape\n",
        "        \n",
        "    #     dyn_shape = tf.shape(tensor)\n",
        "    #     for index in non_static_indexes:\n",
        "    #       shape[index] = dyn_shape[index]\n",
        "    #     return shape\n",
        "    \n",
        "    \n",
        "    def convolution(self, convolve, activate, input, k_h, k_w, c_i, c_o, init_weights, init_biases, \n",
        "                    regularizer, trainable, name=''):   \n",
        "        kernel = self.make_var('weights'+name, [k_h, k_w, c_i, c_o], init_weights, regularizer, trainable) \n",
        "        biases = self.make_var('biases'+name, [c_o], init_biases, None, trainable)\n",
        "        tf.summary.histogram('w', kernel)\n",
        "        tf.summary.histogram('b', biases)\n",
        "        # test with different orders: convolve/activate/normalize; normalize/convolve/activate; convolve/normalize/activate\n",
        "        wx = convolve(input, kernel)\n",
        "        a = activate(tf.nn.bias_add(wx, biases))\n",
        "        a = tf.contrib.layers.instance_norm(a, center=False, scale=False)\n",
        "        return a\n",
        "    \n",
        "    \n",
        "    def l2_regularizer(self, weight_decay=0.0005, scope=None):\n",
        "        def regularizer(tensor):\n",
        "            with tf.name_scope(scope, default_name='l2_regularizer', values=[tensor]):\n",
        "                factor = tf.convert_to_tensor(weight_decay, name='weight_decay')\n",
        "                return tf.multiply(factor, tf.nn.l2_loss(tensor), name='decayed_value')\n",
        "        return regularizer\n",
        "    \n",
        "    \n",
        "    def make_var(self, name, shape, initializer=None, regularizer=None, trainable=True):\n",
        "        return tf.compat.v1.get_variable(name, shape, initializer=initializer, regularizer=regularizer, trainable=trainable)      \n",
        "    \n",
        "    \n",
        "    def feed(self, *args):\n",
        "        assert len(args) != 0\n",
        "        \n",
        "        self.layer_inputs = []\n",
        "        for layer in args:\n",
        "            if isinstance(layer, str):\n",
        "                try:\n",
        "                    layer = self.layers[layer]\n",
        "                    print(layer)\n",
        "                except KeyError:\n",
        "                    print(list(self.layers.keys()))\n",
        "                    raise KeyError('Unknown layer name fed: %s' % layer)\n",
        "            self.layer_inputs.append(layer)\n",
        "        return self\n",
        "        \n",
        "        \n",
        "    def get_output(self, layer):\n",
        "        try:\n",
        "            layer = self.layers[layer]\n",
        "        except KeyError:\n",
        "            print(list(self.layers.keys()))\n",
        "            raise KeyError('Unknown layer name fed: %s' % layer)\n",
        "        return layer\n",
        "        \n",
        "        \n",
        "    def get_unique_name(self, prefix):\n",
        "        id = sum(t.startswith(prefix) for t,_ in list(self.layers.items())) + 1\n",
        "        return '%s_%d' % (prefix, id)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_T3mimzmHL6"
      },
      "source": [
        "class CUTIE(Model):\n",
        "    \"\"\" Set up mode framework\n",
        "    \"\"\"\n",
        "    def __init__(self, num_vocabs, num_classes, params, trainable=True):\n",
        "        self.name = \"CUTIE_benchmark\"\n",
        "        \n",
        "        self.data = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None, 1], name='grid_table')\n",
        "        self.gt_classes = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None], name='gt_classes')\n",
        "        self.use_ghm = tf.equal(1, params.use_ghm) if hasattr(params, 'use_ghm') else tf.equal(1, 0) #params.use_ghm \n",
        "        self.activation = 'sigmoid' if (hasattr(params, 'use_ghm') and params.use_ghm) else 'relu'\n",
        "        self.ghm_weights = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, num_classes], name='ghm_weights')        \n",
        "        self.layers = dict({'data': self.data, 'gt_classes': self.gt_classes, 'ghm_weights': self.ghm_weights}) \n",
        "         \n",
        "        self.num_vocabs = num_vocabs\n",
        "        self.num_classes = num_classes     \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.embedding_size = params.embedding_size\n",
        "        self.weight_decay = params.weight_decay if hasattr(params, 'weight_decay') else 0.0\n",
        "        self.hard_negative_ratio = params.hard_negative_ratio if hasattr(params, 'hard_negative_ratio') else 0.0\n",
        "        self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 0\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "        \n",
        "    \n",
        "    def setup(self):        \n",
        "        # Input\n",
        "        (self.feed('data')\n",
        "             .embed(self.num_vocabs, self.embedding_size, name='embedding'))  \n",
        "        \n",
        "        # Encoder\n",
        "        (self.feed('embedding')\n",
        "             .conv(3, 5, 64, 1, 1, name='encoder1_1')\n",
        "             .conv(3, 5, 128, 1, 1, name='encoder1_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool1')\n",
        "             .conv(3, 5, 128, 1, 1, name='encoder2_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder2_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool2')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder3_1')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder3_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool3')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder4_1')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder4_2'))\n",
        "        \n",
        "        # Decoder\n",
        "        (self.feed('encoder4_2')\n",
        "             .up_conv(3, 5, 512, 1, 1, name='up1')\n",
        "             .conv(3, 5, 256, 1, 1, name='decoder1_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='decoder1_2')\n",
        "             .up_conv(3, 5, 256, 1, 1, name='up2')\n",
        "             .conv(3, 5, 128, 1, 1, name='decoder2_1')\n",
        "             .conv(3, 5, 128, 1, 1, name='decoder2_2')\n",
        "             .up_conv(3, 5, 128, 1, 1, name='up3')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder3_1')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder3_2'))\n",
        "        \n",
        "        # Classification\n",
        "        (self.feed('decoder3_2')\n",
        "             .conv(1, 1, self.num_classes, 1, 1, activation=self.activation, name='cls_logits')\n",
        "             .softmax(name='softmax'))  \n",
        "        \n",
        "    # def disp_results(self, data_input, data_label, model_output, threshold):\n",
        "    #     data_input_flat = data_input.reshape([-1]) # [b * h * w]\n",
        "    #     labels = [] # [b * h * w, classes]\n",
        "    #     for item in data_label.reshape([-1]):\n",
        "    #         labels.append([i==item for i in range(self.num_classes)])\n",
        "    #     logits = model_output.reshape([-1, self.num_classes]) # [b * h * w, classes] \n",
        "        \n",
        "    #     # ignore none word input\n",
        "    #     labels_flat = []\n",
        "    #     results_flat = []\n",
        "    #     for idx, item in enumerate(data_input_flat):\n",
        "    #         if item != 0: \n",
        "    #             labels_flat.extend(labels[idx])\n",
        "    #             results_flat.extend(logits[idx] > threshold)\n",
        "        \n",
        "    #     num_p = sum(labels_flat)\n",
        "    #     num_n = sum([1-label for label in labels_flat])   \n",
        "    #     num_all = len(results_flat)     \n",
        "    #     num_correct = sum([True for i in range(num_all) if labels_flat[i] == results_flat[i]])        \n",
        "        \n",
        "    #     labels_flat_p = [label!=0 for label in labels_flat]\n",
        "    #     labels_flat_n = [label==0 for label in labels_flat]\n",
        "    #     num_tp = sum([labels_flat_p[i] * results_flat[i] for i in range(num_all)])\n",
        "    #     num_tn = sum([labels_flat_n[i] * (not results_flat[i]) for i in range(num_all)])\n",
        "    #     num_fp = num_n - num_tp\n",
        "    #     num_fn = num_p - num_tp\n",
        "        \n",
        "    #     # accuracy, precision, recall\n",
        "    #     accuracy = num_correct / num_all\n",
        "    #     precision = num_tp / (num_tp + num_fp)\n",
        "    #     recall = num_tp / (num_tp + num_fn)\n",
        "        \n",
        "    #     return accuracy, precision, recall\n",
        "        \n",
        "        \n",
        "    # def inference(self):\n",
        "    #     return self.get_output('softmax') #cls_logits\n",
        "        \n",
        "    \n",
        "    def build_loss(self):\n",
        "        labels = self.get_output('gt_classes')\n",
        "        cls_logits = self.get_output('cls_logits')         \n",
        "        cls_logits = tf.cond(self.use_ghm, lambda: cls_logits*self.get_output('ghm_weights'), \n",
        "                             lambda: cls_logits, name=\"GradientHarmonizingMechanism\")      \n",
        "        \n",
        "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=cls_logits)\n",
        "            \n",
        "        with tf.compat.v1.variable_scope('HardNegativeMining'):\n",
        "            labels = tf.reshape(labels, [-1])  \n",
        "            cross_entropy = tf.reshape(cross_entropy, [-1])\n",
        "            \n",
        "            fg_idx = tf.where(tf.not_equal(labels, 0))\n",
        "            fgs = tf.gather(cross_entropy, fg_idx)\n",
        "            bg_idx = tf.where(tf.equal(labels, 0))\n",
        "            bgs = tf.gather(cross_entropy, bg_idx)\n",
        "             \n",
        "            num = self.hard_negative_ratio * tf.shape(fgs)[0]\n",
        "            num_bg = tf.cond(tf.shape(bgs)[0]<num, lambda:tf.shape(bgs)[0], lambda:num)\n",
        "            sorted_bgs, _ = tf.nn.top_k(tf.transpose(bgs), num_bg, sorted=True)\n",
        "            cross_entropy = fgs + sorted_bgs\n",
        "        \n",
        "        # Total loss\n",
        "        model_loss = tf.reduce_mean(cross_entropy)\n",
        "        regularization_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), name='regularization')\n",
        "        total_loss = model_loss + regularization_loss\n",
        "        \n",
        "        tf.summary.scalar('model_loss', model_loss)\n",
        "        tf.summary.scalar('regularization_loss', regularization_loss)\n",
        "        tf.summary.scalar('total_loss', total_loss)\n",
        "        \n",
        "        logits = self.get_output('cls_logits')\n",
        "        softmax_logits = self.get_output('softmax') #cls_logits\n",
        "        return model_loss, regularization_loss, total_loss, logits, softmax_logits \n",
        "    \n",
        "    # def build_multi_loss(self):\n",
        "    #     labels = self.get_output('gt_classes')\n",
        "    #     cls_logits = self.get_output('cls_logits')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrtT9uFkpt0S"
      },
      "source": [
        "class CUTIERes(CUTIE):\n",
        "    \"\"\" Set up CUTIE-B model\n",
        "    \"\"\"\n",
        "    def __init__(self, num_vocabs, num_classes, params, trainable=True):\n",
        "        self.name = \"CUTIE_atrousSPP\" # \n",
        "        \n",
        "        self.data_grid = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None, 1], name='data_grid')\n",
        "        self.gt_classes = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None], name='gt_classes') \n",
        "        self.data_image = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, 3], name='data_image') # not used in CUTIEv1\n",
        "        self.ps_1d_indices = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name='ps_1d_indices') # not used in CUTIEv1\n",
        "        \n",
        "        self.use_ghm = tf.equal(1, params.use_ghm) if hasattr(params, 'use_ghm') else tf.equal(1, 0) #params.use_ghm \n",
        "        self.activation = 'sigmoid' if (hasattr(params, 'use_ghm') and params.use_ghm) else 'relu'\n",
        "        self.dropout = params.data_augmentation_dropout if hasattr(params, 'data_augmentation_dropout') else 1\n",
        "        self.ghm_weights = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, num_classes], name='ghm_weights')        \n",
        "        self.layers = dict({'data_grid': self.data_grid, 'gt_classes': self.gt_classes, 'ghm_weights':self.ghm_weights})\n",
        "\n",
        "        self.num_vocabs = num_vocabs\n",
        "        self.num_classes = num_classes     \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.embedding_size = params.embedding_size\n",
        "        self.weight_decay = params.weight_decay if hasattr(params, 'weight_decay') else 0.0\n",
        "        self.hard_negative_ratio = params.hard_negative_ratio if hasattr(params, 'hard_negative_ratio') else 0.0\n",
        "        self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 0\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "        \n",
        "    \n",
        "    def setup(self):        \n",
        "        # Input\n",
        "        (self.feed('data_grid')\n",
        "             .embed(self.num_vocabs, self.embedding_size, name='embedding', dropout=self.dropout))  \n",
        "        \n",
        "        # Encoder\n",
        "        (self.feed('embedding')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_2')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_3')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_4')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 2, name='encoder1_5')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 4, name='encoder1_6')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 8, name='encoder1_7')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 16, name='encoder1_8'))\n",
        "        \n",
        "        # Atrous Spatial Pyramid Pooling module\n",
        "        #(self.feed('encoder1_8')\n",
        "        #     .conv(1, 1, 256, 1, 1, name='aspp_0'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 4, name='aspp_1'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 8, name='aspp_2'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 16, name='aspp_3'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .global_pool(name='aspp_4'))\n",
        "        (self.feed('aspp_1', 'aspp_2', 'aspp_3', 'aspp_4')\n",
        "             .concat(3, name='aspp_concat')\n",
        "             .conv(1, 1, 256, 1, 1, name='aspp_1x1'))\n",
        "        \n",
        "        # Combine low level features\n",
        "        (self.feed('encoder1_1', 'aspp_1x1')\n",
        "             .concat(3, name='concat1')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder1_1'))\n",
        "        \n",
        "        # Classification\n",
        "        (self.feed('decoder1_1') \n",
        "             .conv(1, 1, self.num_classes, 1, 1, activation=self.activation, name='cls_logits') # sigmoid for ghm\n",
        "             .softmax(name='softmax'))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5-GaLCEGmTv"
      },
      "source": [
        "### Parse arguments needed for the model\n",
        "parser = argparse.ArgumentParser(description='CUTIE parameters')\n",
        "\n",
        "# Dummy parser arguments for notebook\n",
        "parser.add_argument('-f')\n",
        "\n",
        "# Data\n",
        "parser.add_argument('--use_cutie2', type=bool, default=False) # True to read image from doc_path \n",
        "parser.add_argument('--doc_path', type=str, default='ExpressExpenseJson')\n",
        "parser.add_argument('--save_prefix', type=str, default='ExpressExpense', help='prefix for ckpt') # TBD: save log/models with prefix\n",
        "parser.add_argument('--test_path', type=str, default='') # leave empty if no test data provided\n",
        "\n",
        "# Checkpoint\n",
        "# parser.add_argument('--restore_ckpt', type=bool, default=False) \n",
        "# parser.add_argument('--restore_bertembedding_only', type=bool, default=False) # effective when restore_ckpt is True\n",
        "# parser.add_argument('--embedding_file', type=str, default='bert/multi_cased_L-12_H-768_A-12/bert_model.ckpt') \n",
        "parser.add_argument('--ckpt_path', type=str, default='checkpoint/')\n",
        "# parser.add_argument('--ckpt_file', type=str, default='CUTIE_atrousSPP_d20000c5(r80c80)_iter_29201.ckpt')  \n",
        "\n",
        "# Dict\n",
        "parser.add_argument('--load_dict', type=bool, default=True, help='True to work based on an existing dict') \n",
        "parser.add_argument('--load_dict_from_path', type=str, default='dict/') # 40000 or 20000TC or table\n",
        "# parser.add_argument('--tokenize', type=bool, default=False) # tokenize input text ### default = True\n",
        "# parser.add_argument('--text_case', type=bool, default=False) # case sensitive ### default = True == case sensitive\n",
        "parser.add_argument('--update_dict', type=bool, default=False)\n",
        "parser.add_argument('--dict_path', type=str, default='dict/ExpressExpense') # not used if load_dict is True\n",
        "\n",
        "# Data manipulation\n",
        "parser.add_argument('--segment_grid', type=bool, default=False) # segment grid into two parts if grid is larger than cols_target\n",
        "parser.add_argument('--rows_segment', type=int, default=72) \n",
        "parser.add_argument('--cols_segment', type=int, default=72) \n",
        "parser.add_argument('--augment_strategy', type=int, default=1) # 1 for increasing grid shape size, 2 for gaussian around target shape\n",
        "# parser.add_argument('--positional_mapping_strategy', type=int, default=1)\n",
        "parser.add_argument('--rows_target', type=int, default=64) \n",
        "parser.add_argument('--cols_target', type=int, default=64) \n",
        "parser.add_argument('--rows_ulimit', type=int, default=80) # used when data augmentation is true\n",
        "parser.add_argument('--cols_ulimit', type=int, default=80) \n",
        "# parser.add_argument('--fill_bbox', type=bool, default=False) # fill bbox with dict_id / label_id\n",
        "\n",
        "parser.add_argument('--data_augmentation_extra', type=bool, default=True) # randomly expand rows/cols\n",
        "parser.add_argument('--data_augmentation_dropout', type=float, default=0.9) ### default=1\n",
        "parser.add_argument('--data_augmentation_extra_rows', type=int, default=16) \n",
        "parser.add_argument('--data_augmentation_extra_cols', type=int, default=16) \n",
        "\n",
        "# Training\n",
        "parser.add_argument('--batch_size', type=int, default=5) # default=32\n",
        "parser.add_argument('--iterations', type=int, default=2) # default=40000\n",
        "parser.add_argument('--lr_decay_step', type=int, default=13000) \n",
        "parser.add_argument('--learning_rate', type=float, default=0.0001)\n",
        "parser.add_argument('--lr_decay_factor', type=float, default=0.1) \n",
        "\n",
        "# Loss optimization\n",
        "parser.add_argument('--hard_negative_ratio', type=int, help='the ratio between negative and positive losses', default=3) \n",
        "parser.add_argument('--use_ghm', type=int, default=0) # 1 to use GHM, 0 to not use\n",
        "parser.add_argument('--ghm_bins', type=int, default=30) # to be tuned\n",
        "parser.add_argument('--ghm_momentum', type=int, default=0) # 0 / 0.75\n",
        "\n",
        "# Log\n",
        "parser.add_argument('--log_path', type=str, default='log/') \n",
        "parser.add_argument('--log_disp_step', type=int, default=200) \n",
        "parser.add_argument('--log_save_step', type=int, default=1) # default=200\n",
        "parser.add_argument('--validation_step', type=int, default=200) \n",
        "parser.add_argument('--test_step', type=int, default=400) \n",
        "parser.add_argument('--ckpt_save_step', type=int, default=1) #default=1000\n",
        "\n",
        "# Model\n",
        "parser.add_argument('--embedding_size', type=int, default=128) # not used for bert embedding which has 768 as default\n",
        "parser.add_argument('--weight_decay', type=float, default=0.0005) \n",
        "parser.add_argument('--eps', type=float, default=1e-6) \n",
        "\n",
        "# Inference\n",
        "parser.add_argument('--c_threshold', type=float, default=0.5) \n",
        "params = parser.parse_args()\n",
        "\n",
        "# Set up variables\n",
        "edges = [float(x)/params.ghm_bins for x in range(params.ghm_bins+1)]\n",
        "edges[-1] += params.eps\n",
        "acc_sum = [0.0 for _ in range(params.ghm_bins)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0lBjrjCZDMF",
        "outputId": "413dc288-e327-4bd4-8842-d482a8559058"
      },
      "source": [
        "# Call DataLoader to generate dictionary for training\n",
        "data_loader = DataLoader(params, update_dict=params.update_dict, load_dictionary=params.load_dict, data_split=0.75)\n",
        "num_words = max(20000, data_loader.num_words)\n",
        "num_classes = data_loader.num_classes\n",
        "# for _ in range(2000):\n",
        "#     a = data_loader.next_batch()\n",
        "#     b = data_loader.fetch_validation_data()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "DATASET: 2 vocabularies, 2 target classes\n",
            "DATASET: 15 for training, 6 for validation\n",
            "no data file found.\n",
            "DATASET: 0 for test from  \n",
            "\n",
            "Training statistic:  [(32, 1), (29, 1), (28, 1), (22, 1), (20, 1), (17, 2), (15, 2), (14, 4), (12, 1), (9, 1), (8, 1), (7, 3), (6, 3), (5, 4), (3, 3), (2, 1)]\n",
            "\t num:  15\n",
            "\t rows statistic:  [(9, 1), (8, 1), (7, 3), (6, 3), (5, 4), (3, 2), (2, 1)]\n",
            "\t cols statistic:  [(32, 1), (29, 1), (28, 1), (22, 1), (20, 1), (17, 2), (15, 2), (14, 4), (12, 1), (3, 1)]\n",
            "\n",
            "Validation statistic:  [(23, 1), (18, 1), (16, 1), (14, 1), (13, 1), (9, 1), (8, 1), (7, 2), (6, 1), (4, 2)]\n",
            "\t num:  6\n",
            "\t rows statistic:  [(8, 1), (7, 2), (6, 1), (4, 2)]\n",
            "\t cols statistic:  [(23, 1), (18, 1), (16, 1), (14, 1), (13, 1), (9, 1)]\n",
            "\n",
            "Test statistic:  []\n",
            "\t num:  0\n",
            "\t rows statistic:  []\n",
            "\t cols statistic:  []\n",
            "\n",
            "DATASHAPE: data set with maximum grid table of (9,32), updated.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkxQOhsPJbKx",
        "outputId": "8db7ab24-1d81-467c-d9f2-2f0dc41ab7e9"
      },
      "source": [
        "# Initilize CUTIE-B model\n",
        "network = CUTIERes(num_words, num_classes, params)\n",
        "model_loss, regularization_loss, total_loss, model_logits, model_output = network.build_loss()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"data_grid:0\", shape=(?, ?, ?, 1), dtype=int32)\n",
            "WARNING:tensorflow:From <ipython-input-12-64396451d12c>:48: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"embedding/Reshape_1:0\", shape=(?, ?, ?, 128), dtype=float32)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_2/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_3/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"ResizeNearestNeighbor:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_1x1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"decoder1_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 64), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-13-9e72360fdb89>:114: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJJ9IADvM4a3"
      },
      "source": [
        "# Operators\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.Variable(params.learning_rate, trainable=False)\n",
        "optimizer = tf.train.AdamOptimizer(lr)\n",
        "tvars = tf.trainable_variables()\n",
        "grads = tf.gradients(total_loss, tvars)\n",
        "clipped_grads, norm = tf.clip_by_global_norm(grads, 10.0)\n",
        "train_op = optimizer.apply_gradients(list(zip(clipped_grads, tvars)), global_step=global_step) \n",
        "with tf.control_dependencies([train_op]):\n",
        "    train_dummy = tf.constant(0)\n",
        "\n",
        "tf.contrib.training.add_gradients_summaries(zip(clipped_grads, tvars))\n",
        "summary_op = tf.summary.merge_all()    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otG3z4soPLWv",
        "outputId": "13c0c30c-3602-461b-8d7b-de65fe283de1"
      },
      "source": [
        " # Calculate the number of parameters\n",
        "total_parameters = 0\n",
        "for variable in tf.trainable_variables():\n",
        "    shape = variable.get_shape()\n",
        "    variable_parameters = 1\n",
        "    for dim in shape:\n",
        "        variable_parameters *= dim.value\n",
        "    total_parameters += variable_parameters\n",
        "print(network.name, ': ', total_parameters/1000/1000, 'M parameters \\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUTIE_atrousSPP :  13.63885 M parameters \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayp49OUsTf4W"
      },
      "source": [
        "# Helper functions\n",
        "\n",
        "def cal_accuracy(c_threshold, data_loader, grid_table, gt_classes, model_output_val, label_mapids, bbox_mapids):\n",
        "    \"\"\" Calculate accuracy and related indicators\n",
        "    \"\"\"\n",
        "    #num_tp = 0\n",
        "    #num_fn = 0\n",
        "    res = ''\n",
        "    num_correct = 0\n",
        "    num_correct_strict = 0\n",
        "    num_correct_soft = 0\n",
        "    num_all = grid_table.shape[0] * (model_output_val.shape[-1]-1)\n",
        "    for b in range(grid_table.shape[0]):\n",
        "        data_input_flat = grid_table[b,:,:,0].reshape([-1])\n",
        "        labels = gt_classes[b,:,:].reshape([-1])\n",
        "        logits = model_output_val[b,:,:,:].reshape(([-1, data_loader.num_classes]))\n",
        "        label_mapid = label_mapids[b]\n",
        "        bbox_mapid = bbox_mapids[b]\n",
        "        rows, cols = grid_table.shape[1:3]\n",
        "        bbox_id = np.array([row*cols+col for row in range(rows) for col in range(cols)])\n",
        "        \n",
        "        # ignore inputs that are not word\n",
        "        indexes = np.where(data_input_flat != 0)[0]\n",
        "        print(f'len(data_input_flat): {len(data_input_flat)}')\n",
        "        print(f'indexes: {indexes}')\n",
        "        data_selected = data_input_flat[indexes]\n",
        "        labels_selected = labels[indexes]\n",
        "        logits_array_selected = logits[indexes]\n",
        "        bbox_id_selected = bbox_id[indexes]\n",
        "        \n",
        "        # calculate accuracy\n",
        "        #test_classes = [1,2,3,4,5]\n",
        "        #for c in test_classes:\n",
        "        for c in range(1, data_loader.num_classes):\n",
        "            labels_indexes = np.where(labels_selected == c)[0]\n",
        "            logits_indexes = np.where(logits_array_selected[:,c] > c_threshold)[0]\n",
        "            \n",
        "            labels_words = list(data_loader.index_to_word[i] for i in data_selected[labels_indexes])\n",
        "            logits_words = list(data_loader.index_to_word[i] for i in data_selected[logits_indexes])\n",
        "            \n",
        "            label_bbox_ids = label_mapid[c] # GT bbox_ids related to the type of class\n",
        "            logit_bbox_ids = [bbox_mapid[bbox] for bbox in bbox_id_selected[logits_indexes] if bbox in bbox_mapid]            \n",
        "            \n",
        "            #if np.array_equal(labels_indexes, logits_indexes):\n",
        "            if set(label_bbox_ids) == set(logit_bbox_ids): # decide as correct when all ids match\n",
        "                num_correct_strict += 1  \n",
        "                num_correct_soft += 1\n",
        "            elif set(label_bbox_ids).issubset(set(logit_bbox_ids)): # correct when gt is subset of gt\n",
        "                num_correct_soft += 1\n",
        "            try: # calculate prevalence with decimal precision\n",
        "                num_correct += np.shape(np.intersect1d(labels_indexes, logits_indexes))[0] / np.shape(labels_indexes)[0]\n",
        "            except ZeroDivisionError:\n",
        "                if np.shape(labels_indexes)[0] == 0:\n",
        "                    num_correct += 1\n",
        "                else:\n",
        "                    num_correct += 0        \n",
        "            \n",
        "            # show results without the <DontCare> class                    \n",
        "            if b==0:\n",
        "                res += '\\n{}(GT/Inf):\\t\"'.format(data_loader.classes[c])\n",
        "                \n",
        "                # ground truth label\n",
        "                res += ' '.join(data_loader.index_to_word[i] for i in data_selected[labels_indexes])\n",
        "                res += '\" | \"'\n",
        "                res += ' '.join(data_loader.index_to_word[i] for i in data_selected[logits_indexes])\n",
        "                res += '\"'\n",
        "                \n",
        "                # wrong inferences results\n",
        "                if not np.array_equal(labels_indexes, logits_indexes): \n",
        "                    res += '\\n \\t FALSES =>>'\n",
        "                    logits_flat = logits_array_selected[:,c]\n",
        "                    fault_logits_indexes = np.setdiff1d(logits_indexes, labels_indexes)\n",
        "                    for i in range(len(data_selected)):\n",
        "                        if i not in fault_logits_indexes: # only show fault_logits_indexes\n",
        "                            continue\n",
        "                        w = data_loader.index_to_word[data_selected[i]]\n",
        "                        l = data_loader.classes[labels_selected[i]]\n",
        "                        res += ' \"%s\"/%s, '%(w, l)\n",
        "                        #res += ' \"%s\"/%.2f%s, '%(w, logits_flat[i], l)\n",
        "                        \n",
        "                #print(res)\n",
        "    prevalence = num_correct / num_all\n",
        "    accuracy_strict = num_correct_strict / num_all\n",
        "    accuracy_soft = num_correct_soft / num_all\n",
        "    return prevalence, accuracy_strict, accuracy_soft, res.encode(\"utf-8\")\n",
        "\n",
        "\n",
        "def save_ckpt(sess, path, save_prefix, data_loader, network, num_words, num_classes, iter):\n",
        "    \"\"\" Save checkpoint\n",
        "    \"\"\"\n",
        "    # print(f'function save_ckpt is called in iter {iter} for session {sess}')\n",
        "    ckpt_path = join(path, save_prefix)\n",
        "    # print(f'ckpt_path: {ckpt_path}')\n",
        "    if not exists(ckpt_path):\n",
        "        makedirs(ckpt_path)\n",
        "        print(f'ckpt_path: {ckpt_path} is created!')\n",
        "    filename = join(ckpt_path, network.name + '_d{:d}c{:d}(r{:d}c{:d})_iter_{:d}'.\n",
        "                            format(num_words, num_classes, data_loader.rows_ulimit, data_loader.cols_ulimit, iter) + '.ckpt')\n",
        "    # print(f'filename is {filename}')\n",
        "    ckpt_saver.save(sess, filename)\n",
        "    print('\\nCheckpoint saved to: {:s}\\n'.format(filename))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiDkia-nPPz5",
        "outputId": "b68498ab-3078-451d-b2d3-a6dd0d0ed321"
      },
      "source": [
        "# Train the model\n",
        "loss_curve = []\n",
        "training_recall, validation_recall, test_recall = [], [], []\n",
        "training_acc_strict, validation_acc_strict, test_acc_strict = [], [], []\n",
        "training_acc_soft, validation_acc_soft, test_acc_soft = [], [], []\n",
        "\n",
        "ckpt_saver = tf.train.Saver(max_to_keep=200)\n",
        "summary_path = join(params.log_path, params.save_prefix, network.name)\n",
        "summary_writer = tf.summary.FileWriter(summary_path, tf.get_default_graph(), flush_secs=10)\n",
        "\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "config.gpu_options.allow_growth = True\n",
        "with tf.Session(config=config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    iter_start = 0\n",
        "    \n",
        "    # # Restore parameters\n",
        "    # if params.restore_ckpt:\n",
        "    #     if params.restore_bertembedding_only:\n",
        "    #         if 'bert' not in network.name:\n",
        "    #             raise Exception('no bert embedding was designed in the built model, \\\n",
        "    #                 switch restore_bertembedding_only off or built a related model')\n",
        "    #         try:\n",
        "    #             load_variable = {\"bert/embeddings/word_embeddings\": network.embedding_table}\n",
        "    #             ckpt_saver = tf.train.Saver(load_variable, max_to_keep=50)\n",
        "    #             ckpt_path = params.embedding_file\n",
        "    #             ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
        "    #             print('Restoring from {}...'.format(ckpt_path))\n",
        "    #             ckpt_saver.restore(sess, ckpt_path)\n",
        "    #             print('Restored from {}'.format(ckpt_path))\n",
        "    #         except:\n",
        "    #             raise Exception('Check your path {:s}'.format(ckpt_path))\n",
        "    #     else:\n",
        "    #         try:\n",
        "    #             ckpt_path = os.path.join(params.ckpt_path, params.ckpt_file)\n",
        "    #             ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
        "    #             print('Restoring from {}...'.format(ckpt_path))\n",
        "    #             ckpt_saver.restore(sess, ckpt_path)\n",
        "    #             print('Restored from {}'.format(ckpt_path))\n",
        "    #             stem = os.path.splitext(os.path.basename(ckpt_path))[0]\n",
        "    #             #iter_start = int(stem.split('_')[-1]) - 1\n",
        "    #             sess.run(global_step.assign(iter_start))\n",
        "    #         except:\n",
        "    #             raise Exception('Check your pretrained {:s}'.format(ckpt_path))\n",
        "        \n",
        "    # Iterations\n",
        "    print(\"Start Training\")\n",
        "    for iter in range(iter_start, params.iterations+1):\n",
        "        print(f'ITER: {iter}\\n')\n",
        "        timer_start = timeit.default_timer()\n",
        "        \n",
        "        # Learning rate decay\n",
        "        # print(f'iter%params.lr_decay_step = {iter%params.lr_decay_step}')\n",
        "        if iter!=0 and iter%params.lr_decay_step==0:\n",
        "            sess.run(tf.assign(lr, lr.eval()*params.lr_decay_factor))\n",
        "        \n",
        "        # Get data for one batch\n",
        "        # data =  next_batch(training_data_tobe_fetched, training_docs)\n",
        "        data = data_loader.next_batch()\n",
        "        # print(f'data for iter: {iter} = {data}')\n",
        "        feeds = [network.data_grid, network.gt_classes, network.data_image, network.ps_1d_indices, network.ghm_weights]\n",
        "        fetches = [model_loss, regularization_loss, total_loss, summary_op, train_dummy, model_logits, model_output]\n",
        "        h = sess.partial_run_setup(fetches, feeds)\n",
        "        \n",
        "        # One step inference \n",
        "        feed_dict = {\n",
        "            network.data_grid: data['grid_table'],\n",
        "            network.gt_classes: data['gt_classes']\n",
        "        }\n",
        "\n",
        "        # if params.use_cutie2:\n",
        "        #     feed_dict = {\n",
        "        #         network.data_grid: data['grid_table'],\n",
        "        #         network.gt_classes: data['gt_classes'],\n",
        "        #         network.data_image: data['data_image'],\n",
        "        #         network.ps_1d_indices: data['ps_1d_indices']\n",
        "        #     }\n",
        "        fetches = [model_logits, model_output]\n",
        "        (model_logit_val, model_output_val) = sess.partial_run(h, fetches, feed_dict)\n",
        "        \n",
        "        # One step training\n",
        "        ghm_weights = np.ones(np.shape(model_logit_val))\n",
        "        if params.use_ghm:\n",
        "            ghm_weights = calc_ghm_weights(np.array(model_logit_val), np.array(data['gt_classes']))\n",
        "        feed_dict = {\n",
        "            network.ghm_weights: ghm_weights,\n",
        "        }\n",
        "        fetches = [model_loss, regularization_loss, total_loss, summary_op, train_dummy]\n",
        "        (model_loss_val, regularization_loss_val, total_loss_val, summary_str, _) =\\\n",
        "            sess.partial_run(h, fetches=fetches, feed_dict=feed_dict)\n",
        "            \n",
        "        # Calculate training accuracy and display results\n",
        "        if iter%params.log_disp_step == 0: \n",
        "            timer_stop = timeit.default_timer()\n",
        "            print('\\t >>time per step: %.2fs <<'%(timer_stop - timer_start))\n",
        "            \n",
        "            recall, acc_strict, acc_soft, res = cal_accuracy(params.c_threshold, data_loader, np.array(data['grid_table']), \n",
        "                                                    np.array(data['gt_classes']), model_output_val, \n",
        "                                                    np.array(data['label_mapids']), np.array(data['bbox_mapids']))\n",
        "            loss_curve += [total_loss_val]\n",
        "            training_recall += [recall]        \n",
        "            training_acc_strict += [acc_strict]   \n",
        "            training_acc_soft += [acc_soft]       \n",
        "            \n",
        "            #print(res.decode())\n",
        "            print('\\nIter: %d/%d, total loss: %.4f, model loss: %.4f, regularization loss: %.4f'%\\\n",
        "                  (iter, params.iterations, total_loss_val, model_loss_val, regularization_loss_val))\n",
        "            print('LOSS CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.log_disp_step,w) for i,w in enumerate(loss_curve)]))\n",
        "            print('TRAINING ACC CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.log_disp_step,w) for i,w in enumerate(training_acc_strict)]))\n",
        "            print('TRAINING ACC (Recall/Acc): %.3f / %.3f (%.3f) | highest %.3f / %.3f (%.3f)'\\\n",
        "                  %(recall, acc_strict, acc_soft, max(training_recall), max(training_acc_strict), max(training_acc_soft)))\n",
        "            \n",
        "        # Calculate validation accuracy and display results\n",
        "        # print(f'iter%params.validation_step = {iter%params.validation_step}')\n",
        "        # print(f'len(data_loader.validation_docs) = {len(data_loader.validation_docs)}')\n",
        "        if iter%params.validation_step == 0 and len(data_loader.validation_docs):                \n",
        "            recalls, accs_strict, accs_soft = [], [], []\n",
        "            for _ in range(len(data_loader.validation_docs)):\n",
        "                data = data_loader.fetch_validation_data()\n",
        "                grid_tables = data['grid_table']\n",
        "                gt_classes = data['gt_classes']\n",
        "                \n",
        "                feed_dict = {\n",
        "                    network.data_grid: grid_tables,\n",
        "                }\n",
        "                if params.use_cutie2:\n",
        "                    feed_dict = {\n",
        "                        network.data_grid: grid_tables,\n",
        "                        network.data_image: data['data_image'],\n",
        "                        network.ps_1d_indices: data['ps_1d_indices']\n",
        "                    }\n",
        "                fetches = [model_output]                    \n",
        "                [model_output_val] = sess.run(fetches=fetches, feed_dict=feed_dict)  \n",
        "                recall, acc_strict, acc_soft, res = cal_accuracy(params.c_threshold, data_loader, np.array(grid_tables), \n",
        "                                                        np.array(gt_classes), model_output_val,\n",
        "                                                        np.array(data['label_mapids']), np.array(data['bbox_mapids']))\n",
        "                recalls += [recall]\n",
        "                accs_strict += [acc_strict] \n",
        "                accs_soft += [acc_soft]\n",
        "\n",
        "            recall = sum(recalls) / len(recalls)\n",
        "            acc_strict = sum(accs_strict) / len(accs_strict)\n",
        "            acc_soft = sum(accs_soft) / len(accs_soft)\n",
        "            validation_recall += [recall]\n",
        "            validation_acc_strict += [acc_strict]  \n",
        "            validation_acc_soft += [acc_soft]\n",
        "            #print(res.decode()) # show res from the last execution of the while loop \n",
        "\n",
        "            print('VALIDATION ACC (STRICT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.validation_step,w) for i,w in enumerate(validation_acc_strict)]))\n",
        "            print('VALIDATION ACC (SOFT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.validation_step,w) for i,w in enumerate(validation_acc_soft)]))\n",
        "            print('TRAINING RECALL CURVE: ' + ' >'.join(['{:d}:{:.2f}'.\n",
        "                              format(i*params.log_disp_step,w) for i,w in enumerate(training_recall)]))\n",
        "            print('VALIDATION RECALL CURVE: ' + ' >'.join(['{:d}:{:.2f}'.\n",
        "                              format(i*params.validation_step,w) for i,w in enumerate(validation_recall)]))   \n",
        "            \n",
        "            idx = np.argmax(validation_acc_strict)\n",
        "            print('VALIDATION Statistic %d(%d) (Recall/Acc): %.3f / %.3f (%.3f) | highest %.3f / %.3f (%.3f) \\n'\n",
        "                  %(iter, idx*params.validation_step, recall, acc_strict, acc_soft, \n",
        "                    validation_recall[idx], validation_acc_strict[idx], validation_acc_soft[idx]))       \n",
        "            \n",
        "            # Save best performance checkpoint\n",
        "            # print(f'iter>=params.ckpt_save_step: {iter>=params.ckpt_save_step}')\n",
        "            # print(f'validation_acc_strict[-1] > max(validation_acc_strict[:-1]+[0]): { validation_acc_strict[-1] > max(validation_acc_strict[:-1]+[0])}')\n",
        "            if iter>=params.ckpt_save_step and validation_acc_strict[-1] > max(validation_acc_strict[:-1]+[0]):\n",
        "                # Save as iter+1 to indicate best validation\n",
        "                save_ckpt(sess, params.ckpt_path, params.save_prefix, data_loader, network, num_words, num_classes, iter+1)\n",
        "                print('\\nBest up-to-date performance validation checkpoint saved.\\n')\n",
        "                \n",
        "            #     print(\"saved best performace checkpoint\")\n",
        "            # print(\"DIDNT SAVE BEST PERFORANCE CHECKPOINT\")\n",
        "            \n",
        "        # # Calculate test accuracy and display results\n",
        "        # if params.test_path!='' and iter%params.test_step == 0 and len(data_loader.test_docs):\n",
        "            \n",
        "        #     recalls, accs_strict, accs_soft = [], [], []\n",
        "        #     while True:\n",
        "        #         data = data_loader.fetch_test_data()\n",
        "        #         if data == None:\n",
        "        #             break\n",
        "        #         grid_tables = data['grid_table']\n",
        "        #         gt_classes = data['gt_classes']\n",
        "                \n",
        "                \n",
        "        #         feed_dict = {\n",
        "        #             network.data_grid: grid_tables,\n",
        "        #         }\n",
        "        #         if params.use_cutie2:\n",
        "        #             feed_dict = {\n",
        "        #                 network.data_grid: grid_tables,\n",
        "        #                 network.data_image: data['data_image'],\n",
        "        #                 network.ps_1d_indices: data['ps_1d_indices']\n",
        "        #             }\n",
        "        #         fetches = [model_output]                    \n",
        "        #         [model_output_val] = sess.run(fetches=fetches, feed_dict=feed_dict)                    \n",
        "        #         recall, acc_strict, acc_soft, res = cal_accuracy(data_loader, np.array(grid_tables), \n",
        "        #                                                 np.array(gt_classes), model_output_val,\n",
        "        #                                                 np.array(data['label_mapids']), np.array(data['bbox_mapids']))\n",
        "        #         recalls += [recall]\n",
        "        #         accs_strict += [acc_strict] \n",
        "        #         accs_soft += [acc_soft]\n",
        "\n",
        "        #     recall = sum(recalls) / len(recalls)\n",
        "        #     acc_strict = sum(accs_strict) / len(accs_strict)\n",
        "        #     acc_soft = sum(accs_soft) / len(accs_soft)\n",
        "        #     test_recall += [recall]\n",
        "        #     test_acc_strict += [acc_strict]   \n",
        "        #     test_acc_soft += [acc_soft]\n",
        "        #     idx = np.argmax(test_acc_strict)\n",
        "        #     print('\\n TEST ACC (Recall/Acc): %.3f / %.3f (%.3f) | highest %.3f / %.3f (%.3f) \\n'\n",
        "        #           %(recall, acc_strict, acc_soft, test_recall[idx], test_acc_strict[idx], test_acc_soft[idx]))\n",
        "        #     print('TEST ACC (STRICT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "        #                     format(i*params.test_step,w) for i,w in enumerate(test_acc_strict)]))\n",
        "        #     print('TEST ACC (SOFT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "        #                     format(i*params.test_step,w) for i,w in enumerate(test_acc_soft)]))\n",
        "        #     print('TEST RECALL CURVE: ' + ' >'.join(['{:d}:{:.2f}'.\n",
        "        #                     format(i*params.test_step,w) for i,w in enumerate(test_recall)]))            \n",
        "            \n",
        "        #     # save best performance checkpoint\n",
        "        #     if iter>=params.ckpt_save_step and test_acc_strict[-1] > max(test_acc_strict[:-1]+[0]):\n",
        "        #         # save as iter+1 to indicate best test\n",
        "        #         save_ckpt(sess, params.ckpt_path, params.save_prefix, data_loader, network, num_words, num_classes, iter+2)\n",
        "        #         print('\\nBest up-to-date performance test checkpoint saved.\\n')\n",
        "                \n",
        "        # Save checkpoints\n",
        "        print(f'iter = {iter}')\n",
        "        print(f'params.log_save_step = {params.log_save_step}')\n",
        "        print(f'params.ckpt_save_step = {params.ckpt_save_step}')\n",
        "        print(f'iter>=params.log_save_step = {iter>=params.log_save_step}')\n",
        "        print(f'iter%params.ckpt_save_step = {iter%params.ckpt_save_step} ')\n",
        "        if iter>=params.log_save_step and iter%params.ckpt_save_step == 0:\n",
        "            save_ckpt(sess, params.ckpt_path, params.save_prefix, data_loader, network, num_words, num_classes, iter)\n",
        "            \n",
        "        #     print(\"save checkpoint 1\")\n",
        "        # print(\"save checkpoint 2\")\n",
        "\n",
        "        # Save logs\n",
        "        print(f'params.og_save_step = {params.log_save_step}')\n",
        "        print(f'iter%params.log_save_step = { iter%params.log_save_step} ')\n",
        "        if iter>=params.log_save_step and iter%params.log_save_step == 0:\n",
        "            summary_writer.add_summary(summary_str, iter+1)    \n",
        "        #     print(\"save logs 1\")\n",
        "        # print(\"save logs 2\")\n",
        "        # print(iter)\n",
        "\n",
        "pprint(params)\n",
        "pprint('Data rows/cols:{},{}'.format(data_loader.rows, data_loader.cols))\n",
        "summary_writer.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "ITER: 0\n",
            "\n",
            "Training grid AUGMENT size: (29,22) from (8,17)\n",
            "\t >>time per step: 18.72s <<\n",
            "len(data_input_flat): 638\n",
            "indexes: [  3  12  94  96 198 263 275 276 297 298 333 361 363 376 383 385 513 529\n",
            " 538 562 577 608]\n",
            "len(data_input_flat): 638\n",
            "indexes: [ 20  34 616 617 621]\n",
            "len(data_input_flat): 638\n",
            "indexes: [ 21  28  31  50  51  53  56  57  58  72  75  77  79  98 120 132 179 181\n",
            " 192 201 203 206 214 223 225 228 245 246 248 258 267 269 280 296 302 363\n",
            " 367 387 390 403 405 408 411 438 469 494 514 517 536 539 558 561 624 627]\n",
            "len(data_input_flat): 638\n",
            "indexes: [ 55  73  74  77  80 119 121 122 123 132 137 147 149 159 161 168 170 171\n",
            " 180 182 184 186 208 209 212 213 215 224 226 255 269 278 282 291 313 319\n",
            " 322 326 334 336 347 357 369 370 392 411 413 422 436 455 457 458 502 503\n",
            " 546 547]\n",
            "len(data_input_flat): 638\n",
            "indexes: [  1  30  32  34  50  52  53  55  56  57  58  73  78  93 111 113 115 118\n",
            " 123 126 128 133 137 155 158 167 171 176 179 181 183 196 198 201 203 206\n",
            " 218 220 223 225 240 255 270 284 291 306 314 327 328 335 350 360 364 366\n",
            " 376 391 394 442 446 449 451 471 474 493 496 551 554 557 559 561 565 574\n",
            " 577 580 622 625 627 630]\n",
            "\n",
            "Iter: 0/2, total loss: 6.2378, model loss: 6.0234, regularization loss: 0.2143\n",
            "LOSS CURVE: 0:6.238\n",
            "TRAINING ACC CURVE: 0:0.000\n",
            "TRAINING ACC (Recall/Acc): 0.700 / 0.000 (0.600) | highest 0.700 / 0.000 (0.600)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len(data_input_flat): 4096\n",
            "indexes: [  92  301  344  544  556  596  797  988 1000 1042 1196 1233 1245 1608\n",
            " 1621 1634 2042 2053 2060 2069 2250 2259 2299 2638 2732 2747 2890 2899\n",
            " 2924 2939 3084 3195 3530 3644 3975 3994]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  95  105  222  229  346  354  363  481  488  513  675  908  917  923\n",
            "  952 1079 1085 1099 1228 1235 1275 1356 1363 1367 1473 1658 1661 1677\n",
            " 1689 1698 1703 1707 1806 1815 1949 1980 1997 2006 2108 2124 2135 2250\n",
            " 2257 2266 2298 2302 2370 2394 2399 2429 2433 2442 2448 2455 2701 2748\n",
            " 2755 2877 2890 2947 3147 3196 3473 3489 3513 3808 3813 3817 3938 3946\n",
            " 4066]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  35  164  604  612  663  737  784  811  859  935  983 1036 1056 1062\n",
            " 1136 1144 1174 1183 1191 1262 1303 1321 1329 1441 1609 1621 1691 1697\n",
            " 1832 1852 1860 1872 1910 1931 1973 1980 1981 2183 2193 2205 2228 2232\n",
            " 2237 2313 2330 2438 2448 2557 2698 2813 2939 3013 3063 3129 3865 3949\n",
            " 3980 4011 4066]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  100  156  172  212  281  292  299  516  527  686  824  831  834\n",
            "  985 1003 1090 1106 1136 1241 1295 1410 1417 1428 1463 1547 1559 1730\n",
            " 1784 1930 1942 2113 2120 2127 2169 2241 2252 2265 2361 2443 2457 2489\n",
            " 2641 2681 2760 2895 2938 3257 3450 3606 3700 3991 3998 4009]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87   96  209  217  226  332  340  349  356  362  405  480  644  649\n",
            "  653  694  750  803  835  841  846  850  880  885  971 1014 1028 1071\n",
            " 1115 1153 1157 1162 1203 1281 1287 1297 1306 1331 1409 1413 1416 1460\n",
            " 1477 1537 1546 1553 1564 1587 1601 1605 1798 1806 1843 2051 2058 2099\n",
            " 2164 2179 2183 2290 2417 2442 2484 2499 2673 2757 2963 2977 2984 3155\n",
            " 3161 3167 3180 3205 3213 3260 3266 3274 3281 3293 3308 3317 3365 3372\n",
            " 3379 3382 3388 3394 3402 3414 3423 3568 3590 3603 3614 3621 3907 3910\n",
            " 3914 3919 3924]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 117  124  206  268  300  337  344  347  356  814  818  847  859  869\n",
            "  950  957 1031 1174 1178 1183 1190 1496 1542 1554 1668 1678 1992 2044\n",
            " 2146 2183 2188 2192 2197 2203 2217 2514 2540 2745 2820 2836 2939 3017\n",
            " 3034 3127 3334 3353 3442 3451 3529 3641 3671 3752 3870 3912 4043]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.667\n",
            "TRAINING RECALL CURVE: 0:0.70\n",
            "VALIDATION RECALL CURVE: 0:0.75\n",
            "VALIDATION Statistic 0(0) (Recall/Acc): 0.750 / 0.000 (0.667) | highest 0.750 / 0.000 (0.667) \n",
            "\n",
            "iter = 0\n",
            "params.log_save_step = 1\n",
            "params.ckpt_save_step = 1\n",
            "iter>=params.log_save_step = False\n",
            "iter%params.ckpt_save_step = 0 \n",
            "params.og_save_step = 1\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 1\n",
            "\n",
            "Training grid AUGMENT size: (19,47) from (9,29)\n",
            "iter = 1\n",
            "params.log_save_step = 1\n",
            "params.ckpt_save_step = 1\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 0 \n",
            "ckpt_path: checkpoint/ExpressExpense is created!\n",
            "\n",
            "Checkpoint saved to: checkpoint/ExpressExpense/CUTIE_atrousSPP_d20000c2(r80c80)_iter_1.ckpt\n",
            "\n",
            "params.og_save_step = 1\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 2\n",
            "\n",
            "Training grid AUGMENT size: (19,38) from (7,32)\n",
            "iter = 2\n",
            "params.log_save_step = 1\n",
            "params.ckpt_save_step = 1\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 0 \n",
            "\n",
            "Checkpoint saved to: checkpoint/ExpressExpense/CUTIE_atrousSPP_d20000c2(r80c80)_iter_2.ckpt\n",
            "\n",
            "params.og_save_step = 1\n",
            "iter%params.log_save_step = 0 \n",
            "Namespace(augment_strategy=1, batch_size=5, c_threshold=0.5, ckpt_path='checkpoint/', ckpt_save_step=1, cols_segment=72, cols_target=64, cols_ulimit=80, data_augmentation_dropout=0.9, data_augmentation_extra=True, data_augmentation_extra_cols=16, data_augmentation_extra_rows=16, dict_path='dict/ExpressExpense', doc_path='ExpressExpenseJson', embedding_size=128, eps=1e-06, f='/root/.local/share/jupyter/runtime/kernel-3467c956-c41b-4219-bd33-c1abc4df24f9.json', ghm_bins=30, ghm_momentum=0, hard_negative_ratio=3, iterations=2, learning_rate=0.0001, load_dict=True, load_dict_from_path='dict/', log_disp_step=200, log_path='log/', log_save_step=1, lr_decay_factor=0.1, lr_decay_step=13000, rows_segment=72, rows_target=64, rows_ulimit=80, save_prefix='ExpressExpense', segment_grid=False, test_path='', test_step=400, update_dict=False, use_cutie2=False, use_ghm=0, validation_step=200, weight_decay=0.0005)\n",
            "'Data rows/cols:9,32'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CWL7T9R5Pjy"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}