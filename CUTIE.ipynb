{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUTIE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNegbmy8P3IK3x/JBEyQ13a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywsyws/CUTIE/blob/main/CUTIE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-n4WnjsHLPI"
      },
      "source": [
        "Using CUTIE approach to detect total amount TTC on receipt documents\n",
        "\n",
        "# 0. Preparation\n",
        "## 0.1 Environment Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD7HIRbnlU7D"
      },
      "source": [
        "# # Install tesseract ocr libraries\n",
        "# !sudo apt install tesseract-ocr\n",
        "# !pip3 install pytesseract"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuGYTrg02XVZ",
        "outputId": "4c185ee9-2a0b-42a2-bac1-a3b9f867a4ba"
      },
      "source": [
        "# Install tensorflow library\n",
        "!pip3 install tensorflow==1.14"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 87kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Installing collected packages: tensorboard, keras-applications, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK2mOPBlavCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4a3b9d-3293-4681-d60a-57347c217292"
      },
      "source": [
        "# Mount Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7L37NagHEsm"
      },
      "source": [
        "# Import libraries\n",
        "from IPython.display import Image, display\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "# import pytesseract\n",
        "\n",
        "import json, re, random, argparse, timeit\n",
        "from os import chdir, walk, makedirs\n",
        "from os.path import basename, split, join, exists\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "from pprint import pprint"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PUjJ3bAb5S7"
      },
      "source": [
        "# Set root path for this project\n",
        "root_path = r'/content/gdrive/MyDrive/Colab Notebooks/CUTIE/'\n",
        "# Change working directory\n",
        "chdir(root_path)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V82E7JUdH0bn"
      },
      "source": [
        "# # Display an image\n",
        "# image_name='ExpressExpenseImage/1004-receipt.jpg'\n",
        "# Image(filename=image_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDhCT99EVRQ9"
      },
      "source": [
        "## 0.2 Data Preparation\n",
        "### 0.21 Capture texts on images\n",
        "Stores texts and related information in json format for model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bAqQY9ClCO2"
      },
      "source": [
        "# # Display multiple images\n",
        "# for file_num in range(1000, 1003):\n",
        "#   image_name=f'ExpressExpenseImage/{file_num}-receipt.jpg'\n",
        "#   image = Image(filename=image_name)\n",
        "#   display(image)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTsibTTPETRB"
      },
      "source": [
        "# # Helper functions\n",
        "# def VALUES(texts):\n",
        "#   \"\"\" \n",
        "#   To map the id of any words related to \"total amount\" and its amount to \n",
        "#   related field: value_id and value_text\n",
        "#   \"\"\"\n",
        "#   value_id, value_text = [], []\n",
        "#   for row in range(len(texts)):\n",
        "#     word = texts[row][\"text\"]\n",
        "#     if re.match(r'tota.|.total.|^total|totl|ttl|due$|due|$total|drv\\sthru', word, re.I):\n",
        "#       value_id.append(texts[row][\"word_id\"])\n",
        "#       value_text.append(word)\n",
        "#       top = texts[row]['bbox'][1]\n",
        "#       if len(texts) - row == 1:\n",
        "#         break\n",
        "#       for i in range(1,3):\n",
        "#         top_next = texts[row+1]['bbox'][1]\n",
        "#         # top_next_2 = texts[row+2]['bbox'][1]\n",
        "#         if abs((top-top_next)/top_next*100) < 4:\n",
        "#           value_id.append(texts[row+i][\"word_id\"])\n",
        "#           value_text.append(texts[row+i][\"text\"])\n",
        "#           print(value_id, value_text) # TBD\n",
        "#           return value_id, value_text\n",
        "#         else:\n",
        "#           print(value_id, value_text) # TBD\n",
        "#           return value_id, value_text\n",
        "#   if value_id == []:\n",
        "#     print(f\"Cannot find 'Total' in {image_name}\")\n",
        "\n",
        "#   return value_id, value_text"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "momM21k4IBQg"
      },
      "source": [
        "# # Turn results from the Tesseract OCR Engine to json format.\n",
        "# # The json format will then be used as model input \n",
        "\n",
        "# # # For train/validation set\n",
        "# # min_file_num = 1000\n",
        "# # max_file_num = 1150\n",
        "\n",
        "# # For test set\n",
        "# min_file_num = 1150\n",
        "# max_file_num = 1200\n",
        "\n",
        "# for file_num in range(min_file_num, max_file_num):\n",
        "#   # image_name=f'ExpressExpenseImage/{file_num}-receipt.jpg' # For train/validation set\n",
        "#   image_name=f'ExpressExpenseImageTest/{file_num}-receipt.jpg' # For test set\n",
        "\n",
        "#   # Use Tesseract to localize each area of text in the input image\n",
        "#   receipt = pytesseract.image_to_data(image_name, lang='eng', config='--psm 11')#custom_config\n",
        "\n",
        "#   # Split texts into rows\n",
        "#   rows = receipt.split('\\n')\n",
        "#   # Count number of rows in the text\n",
        "#   num_of_rows = len(rows)\n",
        "#   # Declare texts List to store text information\n",
        "#   texts = []\n",
        "\n",
        "#   # Get all the texts of the receipt\n",
        "#   for r in range(1, num_of_rows-1):\n",
        "#     # Declare words dictionary\n",
        "#     words = dict()\n",
        "#     # Split words in each row\n",
        "#     splitted_row = rows[r].split('\\t')\n",
        "#     # Set id for each word\n",
        "#     words['word_id'] = r\n",
        "#     # Get bounding box coordinates\n",
        "#     words['bbox'] = tuple([int(splitted_row[6]), int(splitted_row[7]),    # left and top\n",
        "#                           int(splitted_row[6]) + int(splitted_row[8]),   # right = left + width\n",
        "#                           int(splitted_row[7]) + int(splitted_row[9])])  # bottom = top + height\n",
        "    \n",
        "#     # Get word from each word_id\n",
        "#     if len(splitted_row) < 12:\n",
        "#       words['text'] = ''\n",
        "#     else:\n",
        "#       words['text'] = splitted_row[11]\n",
        "    \n",
        "#     # Write to texts List only if there is a word in that word_id\n",
        "#     if words['text']:\n",
        "#       texts.append(words)\n",
        "\n",
        "#   ### Combine all the necessary information into a dictionary\n",
        "\n",
        "#   # Declare document Dictionary\n",
        "#   doc = {}\n",
        "#   # Add extracted texts in the document Dictionary\n",
        "#   doc.update({'text_boxes': texts})\n",
        "\n",
        "#   # Declare classes and fields Lists\n",
        "#   classes = ['O', 'TTL']\n",
        "#   fields = []\n",
        "#   for cl in classes:\n",
        "#     if cl == 'TTL':\n",
        "#       value_id, value_text = VALUES(texts)\n",
        "#       # Define fields\n",
        "#       new_field = {\"field_name\": cl, \"value_id\": value_id, \"value_text\": value_text, \"key_id\": [], \"key_text\": []}\n",
        "#       fields.append(new_field)\n",
        "\n",
        "#   # Add fields and global_attributes in the document Dictionary\n",
        "#   doc.update({'fileds': fields})\n",
        "#   doc.update({\"global_attributes\":{\"file_id\": basename(image_name)}})\n",
        "\n",
        "#   # Serialize the document Dictionary into json format\n",
        "#   # json_file_path = f\"ExpressExpenseJson/{file_num}-receipt.json\" # For train/validation set\n",
        "#   json_file_path = f\"ExpressExpenseJsonTest/{file_num}-receipt.json\" # For test set\n",
        "#   with open(json_file_path, 'w') as write_file:\n",
        "#     json.dump(doc, write_file, indent = 4, ensure_ascii=False)\n",
        "\n",
        "\n",
        "#   # json_obj = json.dumps(doc, indent = 4, ensure_ascii=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCKv4EGlMuwZ"
      },
      "source": [
        "### 0.22 Manuelly label wrong labels\n",
        "All Manuelly labeled json files are stored in the \"ExpressExpenseJson\" folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vBVvuW5LfYZ"
      },
      "source": [
        "## 0.3 Dictionary Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHPdT5x4U5y7"
      },
      "source": [
        "# DEBUG = False # True to show grid as image \n",
        "\n",
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        pass\n",
        " \n",
        "    try:\n",
        "        unicodedata.numeric(s)\n",
        "\n",
        "        \n",
        "        return True\n",
        "    except (TypeError, ValueError):\n",
        "        pass \n",
        "    return False\n",
        "\n",
        "class DataLoader():\n",
        "    \"\"\"\n",
        "    Produce grid tables\n",
        "    \"\"\"\n",
        "    def __init__(self, params, update_dict=True, load_dictionary=False, data_split=0.75):\n",
        "        self.random = False\n",
        "        self.data_laundry = False\n",
        "        self.encoding_factor = 1 # ensures the size (rows/cols) of grid table compat with the network\n",
        "        self.classes = ['O', 'TTL']\n",
        "        #self.classes = ['DontCare', 'Table'] # for table\n",
        "        #self.classes = ['DontCare', 'Column0', 'Column1', 'Column2', 'Column3', 'Column4', 'Column5'] # for column\n",
        "        #self.classes = ['DontCare', 'Column']\n",
        "        #self.classes = ['DontCare', 'VendorName', 'VendorTaxID', 'InvoiceDate', 'InvoiceNumber', 'ExpenseAmount', 'BaseAmount', 'TaxAmount', 'TaxRate'] # for Spanish project\n",
        "        \n",
        "        self.doc_path = params.doc_path\n",
        "        self.doc_test_path = params.test_path\n",
        "        self.use_cutie2 = params.use_cutie2 \n",
        "        # self.text_case = params.text_case \n",
        "        # self.tokenize = params.tokenize\n",
        "        # if self.tokenize:\n",
        "        #     self.tokenizer = tokenization.FullTokenizer('dict/vocab.txt', do_lower_case=not self.text_case)\n",
        "        \n",
        "        self.rows = self.encoding_factor # to be updated \n",
        "        self.cols = self.encoding_factor # to be updated \n",
        "        self.segment_grid = params.segment_grid if hasattr(params, 'segment_grid') else False # segment grid into two parts if grid is larger than cols_target\n",
        "        self.augment_strategy = params.augment_strategy if hasattr(params, 'augment_strategy') else 1 \n",
        "        # self.pm_strategy = params.positional_mapping_strategy if hasattr(params, 'positional_mapping_strategy') else 2 \n",
        "        self.rows_segment = params.rows_segment if hasattr(params, 'rows_segment') else 72 \n",
        "        self.cols_segment = params.cols_segment if hasattr(params, 'cols_segment') else 72\n",
        "        self.rows_target = params.rows_target if hasattr(params, 'rows_target') else 64 \n",
        "        self.cols_target = params.cols_target if hasattr(params, 'cols_target') else 64 \n",
        "        self.rows_ulimit = params.rows_ulimit if hasattr(params, 'rows_ulimit') else 80 # handle OOM, must be multiple of self.encoding_factor\n",
        "        self.cols_ulimit = params.cols_ulimit if hasattr(params, 'cols_ulimit') else 80 # handle OOM, must be multiple of self.encoding_factor\n",
        "                \n",
        "        # self.fill_bbox = params.fill_bbox if hasattr(params, 'fill_bbox') else False # fill bbox with labels or use one single lable for the entire bbox\n",
        "        \n",
        "        self.data_augmentation_dropout = params.data_augmentation_dropout if hasattr(params, 'data_augmentation_dropout') else False # TBD: randomly dropout rows/cols\n",
        "        self.data_augmentation_extra = params.data_augmentation_extra if hasattr(params, 'data_augmentation_extra') else False # randomly expand rows/cols\n",
        "        self.da_extra_rows = params.data_augmentation_extra_rows if hasattr(params, 'data_augmentation_extra_rows') else 0 # randomly expand rows/cols\n",
        "        self.da_extra_cols = params.data_augmentation_extra_cols if hasattr(params, 'data_augmentation_extra_cols') else 0 # randomly expand rows/cols\n",
        "        \n",
        "        # Set up parameters to be tuned\n",
        "        self.load_dictionary = load_dictionary # load dictionary from file rather than start from empty \n",
        "        self.dict_path = params.load_dict_from_path if load_dictionary else params.dict_path\n",
        "        # if self.load_dictionary:\n",
        "        #     self.dictionary = np.load(self.dict_path + 'dictionary.npy', allow_pickle=True).item()\n",
        "        #     self.word_to_index = np.load(self.dict_path + 'word_to_index.npy', allow_pickle=True).item()\n",
        "        #     self.index_to_word = np.load(self.dict_path + 'index_to_word.npy', allow_pickle=True).item()\n",
        "        # else:\n",
        "        #     self.dictionary = {'[PAD]':0, '[UNK]':0} # word/counts. to be updated in self.load_data() and self.update_docs_dictionary()\n",
        "        #     self.word_to_index = {}\n",
        "        #     self.index_to_word = {}\n",
        "        self.dictionary = np.load(self.dict_path + 'dictionary.npy', allow_pickle=True).item()\n",
        "        self.word_to_index = np.load(self.dict_path + 'word_to_index.npy', allow_pickle=True).item()\n",
        "        self.index_to_word = np.load(self.dict_path + 'index_to_word.npy', allow_pickle=True).item()\n",
        "\n",
        "        self.data_split = data_split # split data to training/validation, 0 for all for validation\n",
        "        self.data_mode = 2 # 0 to consider key and value as two different class, 1 the same class, 2 only value considered\n",
        "        self.remove_lowfreq_words = False # remove low frequency words when set as True\n",
        "        \n",
        "        self.num_classes = len(self.classes)\n",
        "        self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 1        \n",
        "        \n",
        "        # TBD: build a special cared dictionary\n",
        "        self.special_dict = {'*', '='} # map texts to specific tokens        \n",
        "        \n",
        "        # Load words and their class as training/validation docs and labels \n",
        "        self.training_doc_files = self.get_filenames(self.doc_path)\n",
        "        self.training_docs, self.training_labels = self.load_data(self.training_doc_files, update_dict=update_dict) # TBD: optimize the update dict flag\n",
        "        \n",
        "        # Polish and load dictionary/word_to_index/index_to_word as file\n",
        "        self.num_words = len(self.dictionary)              \n",
        "        self.update_word_to_index()\n",
        "        self.update_docs_dictionary(self.training_docs, 3, self.remove_lowfreq_words) # remove low frequency words and add it under the <unknown> key\n",
        "        \n",
        "        # Save dictionary/word_to_index/index_to_word as file\n",
        "        np.save(self.dict_path + 'dictionary.npy', self.dictionary)\n",
        "        np.save(self.dict_path + 'word_to_index.npy', self.word_to_index)\n",
        "        np.save(self.dict_path + 'index_to_word.npy', self.index_to_word)\n",
        "        np.save(self.dict_path + 'classes.npy', self.classes)\n",
        "        \n",
        "        # Split training / validation docs and show statistics\n",
        "        num_training = int(len(self.training_docs)*self.data_split)\n",
        "        data_to_be_fetched = [i for i in range(len(self.training_docs))]\n",
        "        selected_training_index = data_to_be_fetched[:num_training] \n",
        "        if self.random:\n",
        "            selected_training_index = random.sample(data_to_be_fetched, num_training)\n",
        "        selected_validation_index = list(set(data_to_be_fetched).difference(set(selected_training_index)))\n",
        "        self.validation_docs = [self.training_docs[x] for x in selected_validation_index]\n",
        "        self.training_docs = [self.training_docs[x] for x in selected_training_index]\n",
        "        self.validation_labels = self.training_labels\n",
        "        print('\\n\\nDATASET: %d vocabularies, %d target classes'%(len(self.dictionary), len(self.classes)))\n",
        "        print('DATASET: %d for training, %d for validation'%(len(self.training_docs), len(self.validation_docs)))\n",
        "        \n",
        "        # Load test files if any\n",
        "        self.test_doc_files = self.get_filenames(params.test_path) if hasattr(params, 'test_path') else []\n",
        "        self.test_docs, self.test_labels = self.load_data(self.test_doc_files, update_dict=update_dict) # TBD: optimize the update dict flag\n",
        "        print('DATASET: %d for test from %s \\n'%(len(self.test_docs), params.test_path if hasattr(params, 'test_path') else '_'))\n",
        "        \n",
        "        self.data_shape_statistic() # show data shape static\n",
        "        if len(self.training_docs) > 0:# adapt grid table size to all training dataset docs \n",
        "            self.rows, self.cols, _, _ = self.cal_rows_cols(self.training_docs)  \n",
        "            print('\\nDATASHAPE: data set with maximum grid table of ({},{}), updated.\\n'.format(self.rows, self.cols))    \n",
        "        else:\n",
        "            self.rows, self.cols = self.rows_ulimit, self.cols_ulimit\n",
        "                \n",
        "        # Call self.next_batch() outside to generate a batch of grid tables data and labels\n",
        "        self.training_data_tobe_fetched = [i for i in range(len(self.training_docs))]\n",
        "        self.validation_data_tobe_fetched = [i for i in range(len(self.validation_docs))]        \n",
        "        self.test_data_tobe_fetched = [i for i in range(len(self.test_docs))]\n",
        "        \n",
        "    \n",
        "    def update_word_to_index(self):\n",
        "        # Create word_to_index document\n",
        "        if self.load_dictionary:\n",
        "            max_index = len(self.word_to_index.keys())\n",
        "            for word in self.dictionary:\n",
        "                if word not in self.word_to_index:\n",
        "                    max_index += 1\n",
        "                    self.word_to_index[word] = max_index\n",
        "                    self.index_to_word[max_index] = word            \n",
        "        else:   \n",
        "            self.word_to_index = dict(list(zip(self.dictionary.keys(), list(range(self.num_words))))) \n",
        "            self.index_to_word = dict(list(zip(list(range(self.num_words)), self.dictionary.keys())))\n",
        "    \n",
        "    def update_docs_dictionary(self, docs, lower_limit, remove_lowfreq_words):\n",
        "        # assign docs words that appear less than @lower_limit times to word [UNK]\n",
        "        if remove_lowfreq_words: \n",
        "            for doc in docs:\n",
        "                for line in doc:\n",
        "                    [file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                        [image_w, image_h], max_row_words, max_col_words] = line \n",
        "                    if self.dictionary[dressed_text] < lower_limit:\n",
        "                        line = [file_name, '[UNK]', self.word_to_index['[UNK]'], [x_left, y_top, x_right, y_bottom], \\\n",
        "                                [image_w, image_h], max_row_words, max_col_words]\n",
        "                        self.dictionary[dressed_text] -= 1\n",
        "                        self.dictionary['[UNK]'] += 1\n",
        "    \n",
        "    def next_batch(self):\n",
        "        batch_size = self.batch_size\n",
        "        \n",
        "        while True:\n",
        "            if len(self.training_data_tobe_fetched) < batch_size:\n",
        "                self.training_data_tobe_fetched = [i for i in range(len(self.training_docs))]            \n",
        "            selected_index = random.sample(self.training_data_tobe_fetched, batch_size)\n",
        "            self.training_data_tobe_fetched = list(set(self.training_data_tobe_fetched).difference(set(selected_index)))\n",
        "    \n",
        "            training_docs = [self.training_docs[x] for x in selected_index]\n",
        "            \n",
        "            ## data augmentation in each batch if self.data_augmentation==True\n",
        "            rows, cols, pre_rows, pre_cols = self.cal_rows_cols(training_docs, extra_augmentation=self.data_augmentation_extra, dropout=self.data_augmentation_dropout)\n",
        "            if self.data_augmentation_extra:\n",
        "                print('Training grid AUGMENT size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, cols, pre_rows, pre_cols))\n",
        "            \n",
        "            grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "                self.positional_mapping(training_docs, self.training_labels, rows, cols)   \n",
        "            if updated_cols > cols:\n",
        "                print('Training grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, updated_cols, rows, cols))\n",
        "                grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "                    self.positional_mapping(training_docs, self.training_labels, rows, updated_cols, update_col=False)  \n",
        "            \n",
        "            ## load image and generate corresponding @ps_1dindices\n",
        "            images, ps_1d_indices = [], []\n",
        "\n",
        "            if self.use_cutie2:\n",
        "                images, ps_1d_indices = self.positional_sampling(self.doc_path, file_names, ps_indices_x, ps_indices_y, updated_cols)   \n",
        "                #print(\"image fetched {}\".format(len(images)))          \n",
        "                if len(images) == batch_size:\n",
        "                    break\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "                 'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "                 'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "                 'file_name': file_names, 'shape': [rows,cols]}\n",
        "        return batch\n",
        "    \n",
        "    def fetch_validation_data(self):\n",
        "        batch_size = 1\n",
        "        \n",
        "        while True:\n",
        "            if len(self.validation_data_tobe_fetched) == 0:\n",
        "                self.validation_data_tobe_fetched = [i for i in range(len(self.validation_docs))]            \n",
        "            selected_index = random.sample(self.validation_data_tobe_fetched, 1)\n",
        "            self.validation_data_tobe_fetched = list(set(self.validation_data_tobe_fetched).difference(set(selected_index)))\n",
        "    \n",
        "            validation_docs = [self.validation_docs[x] for x in selected_index]\n",
        "            \n",
        "            ## fixed validation shape leads to better result (to be verified)\n",
        "            real_rows, real_cols, _, _ = self.cal_rows_cols(validation_docs, extra_augmentation=False)\n",
        "            rows = max(self.rows_target, real_rows)\n",
        "            cols = max(self.rows_target, real_cols)\n",
        "            \n",
        "            grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "                self.positional_mapping(validation_docs, self.validation_labels, rows, cols)   \n",
        "            if updated_cols > cols:\n",
        "                print('Validation grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, updated_cols, rows, cols))\n",
        "                grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "                    self.positional_mapping(validation_docs, self.validation_labels, rows, updated_cols, update_col=False)     \n",
        "            \n",
        "            ## load image and generate corresponding @ps_1dindices\n",
        "            images, ps_1d_indices = [], []\n",
        "            if self.use_cutie2:\n",
        "                images, ps_1d_indices = self.positional_sampling(self.doc_path, file_names, ps_indices_x, ps_indices_y, updated_cols)  \n",
        "                if len(images) == batch_size:\n",
        "                    break        \n",
        "            else:\n",
        "                break\n",
        "            \n",
        "        def build_gt_pyramid(self, gt_classes):\n",
        "            gt_classes = np.array(gt_classes)\n",
        "            \n",
        "            rate = 4 # self.pooling_factor\n",
        "            b, h, w = np.shape(gt_classes)\n",
        "            same_padding_left = (rate-w%rate)//2 if w%rate else 0\n",
        "            same_padding_right = rate-(rate-w%rate)//2 if w%rate else 0\n",
        "            same_padding_top = (rate-h%rate)//2 if h%rate else 0\n",
        "            same_padding_bottom = rate-(rate-h%rate)//2 if h%rate else 0\n",
        "            for gt_class in gt_classes:\n",
        "                pad_v =  np.pad(gt_class, ((same_padding_top, same_padding_bottom), (0,0)), 'constant', constant_values=((0,0),(0,0)))\n",
        "                pad_h =  np.pad(gt_class, ((0,0), (same_padding_left, same_padding_right)), 'constant', constant_values=((0,0),(0,0)))\n",
        "                \n",
        "                ## find mask range for each single entity\n",
        "                num_entities = np.max(gt_classes) / self.num_classes\n",
        "                entity_ranges = [[] for _ in range(0,num_entities)]\n",
        "                for i in range(1, num_entities):\n",
        "                    if i % self.num_classes: # only consider non <DontCare> classes\n",
        "                        range_y, range_x = np.where(gt_classes==i)\n",
        "                        # entity_ranges[i] = [top, left, bottom, right, height, width]\n",
        "                        entity_ranges[i] = [min(range_y), min(range_x), max(range_y), max(range_x), \n",
        "                                            max(range_y) - min(range_y), max(range_x) - min(range_x)] \n",
        "                           \n",
        "                \n",
        "        \n",
        "        batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "                 'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "                 'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "                 'file_name': file_names, 'shape': [rows,cols]}\n",
        "        return batch\n",
        "    \n",
        "    def fetch_test_data(self): \n",
        "        batch_size = 1\n",
        "        \n",
        "        while True:\n",
        "            if len(self.test_data_tobe_fetched) == 0:\n",
        "                self.test_data_tobe_fetched = [i for i in range(len(self.test_docs))]\n",
        "                return None\n",
        "                        \n",
        "            selected_index = self.test_data_tobe_fetched[0]\n",
        "            self.test_data_tobe_fetched = list(set(self.test_data_tobe_fetched).difference(set([selected_index])))\n",
        "    \n",
        "            test_docs = [self.test_docs[selected_index]]\n",
        "            \n",
        "            real_rows, real_cols, _, _ = self.cal_rows_cols(test_docs, extra_augmentation=False)\n",
        "            rows = max(self.rows_target, real_rows) # small shaped documents have better performance with shape 64\n",
        "            cols = max(self.cols_target, real_cols) # large shaped docuemnts have better performance with shape 80\n",
        "                \n",
        "            grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "                self.positional_mapping(test_docs, self.test_labels, rows, cols)   \n",
        "            if updated_cols > cols:\n",
        "                print('Test grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, updated_cols, rows, cols))\n",
        "                grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "                    self.positional_mapping(test_docs, self.test_labels, rows, updated_cols, update_col=False)    \n",
        "                    \n",
        "            ## load image and generate corresponding @ps_1dindices\n",
        "            images, ps_1d_indices = [], []\n",
        "            if self.use_cutie2:\n",
        "                images, ps_1d_indices = self.positional_sampling(self.doc_test_path, file_names, ps_indices_x, ps_indices_y, updated_cols)          \n",
        "                if len(images) == batch_size:\n",
        "                    break          \n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "                 'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "                 'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "                 'file_name': file_names, 'shape': [rows,cols]}\n",
        "        return batch\n",
        "    \n",
        "    def form_label_matrix(self, gt_classes, target_h, target_w):\n",
        "        \"\"\"\n",
        "        build gt_classes and gt_masks with given target featuremap shape (height, width)\n",
        "        by inspecting bboxes regions (x,y,w,h)\n",
        "        for table / row / column identity segmentation\n",
        "        \"\"\"\n",
        "        def has_entity_with_augmentation(entity_ranges, roi, use_jittering=False):                    \n",
        "            ## find mask with maximum overlap\n",
        "            max_iou = 0\n",
        "            max_idx = None\n",
        "            roi_t, roi_l, roi_b, roi_r = roi\n",
        "            roi_h = roi_b - roi_t\n",
        "            roi_w = roi_r - roi_l\n",
        "            roi_cy = roi_t + roi_h/2\n",
        "            roi_cx = roi_l + roi_w/2\n",
        "            for idx, entity in enumerate(entity_ranges):\n",
        "                if len(entity):\n",
        "                    t, l, b, r, h, w = entity\n",
        "                    if l>roi_l and r<roi_r and t>roi_t and b<roi_b: # overlap 1\n",
        "                        iou = h*w / (roi_h*roi_w)\n",
        "                    elif l<roi_l and r>roi_r and t<roi_t and b>roi_b: # overlap 2\n",
        "                        iou = roi_h*roi_w / (h*w)\n",
        "                    elif l>roi_r or t>roi_b or b<roi_t or r<roi_l: # no intersection\n",
        "                        continue\n",
        "                    else:\n",
        "                        iou = min(h*w, roi_h*roi_w) / max(h*w, roi_h*roi_w)\n",
        "                        \n",
        "                    # TBD: add jittering augmentation method  \n",
        "                    if use_jittering:\n",
        "                        pass                          \n",
        "                    if iou > max_iou:\n",
        "                        max_idx = idx\n",
        "                        max_iou = iou\n",
        "                        \n",
        "            ## check centrality / containment / uniqueness\n",
        "            t, l, b, r, h, w = entity[idx]\n",
        "            cy = t + h/2\n",
        "            cx = l + w/2\n",
        "            if roi_t+h/3 < cy and cy < toi_b-h/3 and roi_l+w/3 < cx and cx < roi_r-w/3: # centrality\n",
        "                if (w > h and roi_w > w*0.9) or (w < h and roi_h > h*0.9): # containment\n",
        "                    if True: # uniqueness is already checked with maixmum IOU\n",
        "                        return True\n",
        "            return False                 \n",
        "    \n",
        "        shape = gt_classes.shape\n",
        "        rate_v = shape[0] / target_h\n",
        "        rate_h = shape[1] / target_w\n",
        "        dst_classes = [[[] for i in range(target_h)] for j in range(target_w)]\n",
        "        dst_masks = [[[] for i in range(target_h)] for j in range(target_w)]\n",
        "        for i in range(target_h):\n",
        "            for j in range(target_w):\n",
        "                roi = [rate_h*j, rate_v*i, rate_h*(j+1), rate_v*(i+1)] # [top, left, bottom, right]\n",
        "                \n",
        "                dst_classes[i][j] = has_entity_with_augmentation(entity_ranges, roi, False)\n",
        "                \n",
        "                mask = gt_classes[roi[1]:roi[3], roi[0]:roi[2]]\n",
        "                dst_masks[i][j] = mask if dst_classes[i][j] else np.zeros(np.shape(mask))\n",
        "        \n",
        "        return np.array(dst_classes), np.array(dst_masks)\n",
        "        \n",
        "    def data_shape_statistic(self):        \n",
        "        def shape_statistic(docs):\n",
        "            res_all = defaultdict(int)\n",
        "            res_row = defaultdict(int)\n",
        "            res_col = defaultdict(int)\n",
        "            for doc in docs:\n",
        "                rows, cols, _, _ = self.cal_rows_cols([doc])\n",
        "                res_all[rows] += 1\n",
        "                res_all[cols] += 1\n",
        "                res_row[rows] += 1\n",
        "                res_col[cols] += 1\n",
        "            res_all = sorted(res_all.items(), key=lambda x:x[0], reverse=True)\n",
        "            res_row = sorted(res_row.items(), key=lambda x:x[0], reverse=True)\n",
        "            res_col = sorted(res_col.items(), key=lambda x:x[0], reverse=True)\n",
        "            return res_all, res_row, res_col\n",
        "    \n",
        "        tss, tss_r, tss_c = shape_statistic(self.training_docs) # training shape static\n",
        "        vss, vss_r, vss_c = shape_statistic(self.validation_docs)\n",
        "        tess, tess_r, tess_c = shape_statistic(self.test_docs)\n",
        "        print(\"Training statistic: \", tss)\n",
        "        print(\"\\t num: \", len(self.training_docs))\n",
        "        print(\"\\t rows statistic: \", tss_r)\n",
        "        print(\"\\t cols statistic: \", tss_c)\n",
        "        print(\"\\nValidation statistic: \", vss)\n",
        "        print(\"\\t num: \", len(self.validation_docs))\n",
        "        print(\"\\t rows statistic: \", vss_r)\n",
        "        print(\"\\t cols statistic: \", vss_c)\n",
        "        print(\"\\nTest statistic: \", tess)\n",
        "        print(\"\\t num: \", len(self.test_docs))\n",
        "        print(\"\\t rows statistic: \", tess_r)\n",
        "        print(\"\\t cols statistic: \", tess_c)\n",
        "        \n",
        "        ## remove data samples not matching the training principle\n",
        "        def data_laundry(docs):\n",
        "            idx = 0\n",
        "            while idx < len(docs):\n",
        "                rows, cols, _, _ = self.cal_rows_cols([docs[idx]])\n",
        "                if rows > self.rows_ulimit or cols > self.cols_ulimit:\n",
        "                    del docs[idx]\n",
        "                else:\n",
        "                    idx += 1\n",
        "        if self.data_laundry:\n",
        "            print(\"\\nRemoving grids with shape larger than ({},{}).\".format(self.rows_ulimit, self.cols_ulimit))\n",
        "            data_laundry(self.training_docs)\n",
        "            data_laundry(self.validation_docs)\n",
        "            data_laundry(self.training_docs)\n",
        "        \n",
        "            tss, tss_r, tss_c = shape_statistic(self.training_docs) # training shape static\n",
        "            vss, vss_r, vss_c = shape_statistic(self.validation_docs)\n",
        "            tess, tess_r, tess_c = shape_statistic(self.test_docs)\n",
        "            print(\"Training statistic after laundary: \", tss)\n",
        "            print(\"\\t num: \", len(self.training_docs))\n",
        "            print(\"\\t rows statistic: \", tss_r)\n",
        "            print(\"\\t cols statistic: \", tss_c)\n",
        "            print(\"Validation statistic after laundary: \", vss)\n",
        "            print(\"\\t num: \", len(self.validation_docs))\n",
        "            print(\"\\t rows statistic: \", vss_r)\n",
        "            print(\"\\t cols statistic: \", vss_c)\n",
        "            print(\"Test statistic after laundary: \", tess)\n",
        "            print(\"\\t num: \", len(self.test_docs))\n",
        "            print(\"\\t rows statistic: \", tess_r)\n",
        "            print(\"\\t cols statistic: \", tess_c)\n",
        "    \n",
        "    def positional_mapping(self, docs, labels, rows, cols):\n",
        "        \"\"\"\n",
        "        docs in format:\n",
        "        [[file_name, text, word_id, [x_left, y_top, x_right, y_bottom], [left, top, right, bottom], max_row_words, max_col_words] ]\n",
        "        return grid_tables, gird_labels, dict bboxes {file_name:[]}, file_names\n",
        "        \"\"\"\n",
        "        grid_tables = []\n",
        "        gird_labels = []\n",
        "        ps_indices_x = [] # positional sampling indices\n",
        "        ps_indices_y = [] # positional sampling indices\n",
        "        bboxes = {}\n",
        "        label_mapids = []\n",
        "        bbox_mapids = [] # [{}, ] bbox identifier, each id with one or multiple bbox/bboxes\n",
        "        file_names = []\n",
        "        for doc in docs:\n",
        "            items = []\n",
        "            cols_e = 2 * cols # use @cols_e larger than required @cols as buffer\n",
        "            grid_table = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            grid_label = np.zeros([rows, cols_e], dtype=np.int8)\n",
        "            ps_x = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            ps_y = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            bbox = [[] for c in range(cols_e) for r in range(rows)]\n",
        "            bbox_id, bbox_mapid = 0, {} # one word in one or many positions in a bbox is mapped in bbox_mapid\n",
        "            label_mapid = [[] for _ in range(self.num_classes)] # each class is connected to several bboxes (words)\n",
        "            drawing_board = np.zeros([rows, cols_e], dtype=str)\n",
        "            for item in doc:\n",
        "                file_name = item[0]\n",
        "                text = item[1]\n",
        "                word_id = item[2]\n",
        "                x_left, y_top, x_right, y_bottom = item[3][:]\n",
        "                left, top, right, bottom = item[4][:]\n",
        "                \n",
        "                dict_id = self.word_to_index[text]                \n",
        "                entity_id, class_id = self.dress_class(file_name, word_id, labels)\n",
        "                \n",
        "                bbox_id += 1\n",
        "#                 if self.fill_bbox: # TBD: overlap avoidance\n",
        "#                     top = int(rows * y_top / image_h)\n",
        "#                     bottom = int(rows * y_bottom / image_h)\n",
        "#                     left = int(cols * x_left / image_w)\n",
        "#                     right = int(cols * x_right / image_w)\n",
        "#                     grid_table[top:bottom, left:right] = dict_id  \n",
        "#                     grid_label[top:bottom, left:right] = class_id  \n",
        "#                      \n",
        "#                     label_mapid[class_id].append(bbox_id)\n",
        "#                     for row in range(top, bottom):\n",
        "#                         for col in range(left, right):\n",
        "#                             bbox_mapid[row*cols+col] = bbox_id\n",
        "#                      \n",
        "#                     for y in range(top, bottom):\n",
        "#                         for x in range(left, right):\n",
        "#                             bbox[y][x] = [x_left, y_top, x_right-x_left, y_bottom-y_top]\n",
        "                label_mapid[class_id].append(bbox_id)    \n",
        "                \n",
        "                #v_c = (y_top - top + (y_bottom-y_top)/2) / (bottom-top)\n",
        "                #h_c = (x_left - left + (x_right-x_left)/2) / (right-left)\n",
        "                #v_c = (y_top + (y_bottom-y_top)/2) / bottom\n",
        "                #h_c = (x_left + (x_right-x_left)/2) / right \n",
        "                #v_c = (y_top-top) / (bottom-top)\n",
        "                #h_c = (x_left-left) / (right-left)\n",
        "                #v_c = (y_top) / (bottom)\n",
        "                #h_c = (x_left) / (right)\n",
        "                box_y = y_top + (y_bottom-y_top)/2\n",
        "                box_x = x_left # h_l is used for image feature map positional sampling\n",
        "                v_c = (y_top - top + (y_bottom-y_top)/2) / (bottom-top)\n",
        "                h_c = (x_left - left + (x_right-x_left)/2) / (right-left) # h_c is used for sorting items\n",
        "                row = int(rows * v_c) \n",
        "                col = int(cols * h_c) \n",
        "                items.append([row, col, [box_y, box_x], [v_c, h_c], file_name, dict_id, class_id, entity_id, bbox_id, [x_left, y_top, x_right-x_left, y_bottom-y_top]])                       \n",
        "            \n",
        "            items.sort(key=lambda x: (x[0], x[3], x[5])) # sort according to row > h_c > bbox_id\n",
        "            for item in items:\n",
        "                row, col, [box_y, box_x], [v_c, h_c], file_name, dict_id, class_id, entity_id, bbox_id, box = item\n",
        "                entity_class_id = entity_id*self.num_classes + class_id\n",
        "                \n",
        "                while col < cols and grid_table[row, col] != 0:\n",
        "                    col += 1            \n",
        "                \n",
        "                ptr = 0\n",
        "                if col == cols: # shift to find slot to drop the current item\n",
        "                    col -= 1\n",
        "                    while ptr<cols and grid_table[row, ptr] != 0:\n",
        "                        ptr += 1\n",
        "                    if ptr == cols:\n",
        "                        grid_table[row, :-1] = grid_table[row, 1:]\n",
        "                    else:\n",
        "                        grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                \n",
        "                grid_table[row, col] = dict_id\n",
        "                grid_label[row, col] = entity_class_id\n",
        "                ps_x[row, col] = box_x\n",
        "                ps_y[row, col] = box_y\n",
        "                bbox_mapid[row*cols+col] = bbox_id     \n",
        "                bbox[row*cols+col] = box\n",
        "\n",
        "                # # self.pm_strategy 0: skip if overlap\n",
        "                # # self.pm_strategy 1: shift to find slot if overlap\n",
        "                # # self.pm_strategy 2: expand grid table if overlap\n",
        "                # if self.pm_strategy == 0:\n",
        "                #     if col == cols:                     \n",
        "                #         print('overlap in {} row {} r{}c{}!'.\n",
        "                #               format(file_name, row, rows, cols))\n",
        "                #         #print(grid_table[row,:])\n",
        "                #         #print('overlap in {} <{}> row {} r{}c{}!'.\n",
        "                #         #      format(file_name, self.index_to_word[dict_id], row, rows, cols))\n",
        "                #     else:\n",
        "                #         grid_table[row, col] = dict_id\n",
        "                #         grid_label[row, col] = entity_class_id                       \n",
        "                #         bbox_mapid[row*cols+col] = bbox_id                       \n",
        "                #         bbox[row*cols+col] = box   \n",
        "                # elif self.pm_strategy==1 or self.pm_strategy==2:\n",
        "                #     ptr = 0\n",
        "                #     if col == cols: # shift to find slot to drop the current item\n",
        "                #         col -= 1\n",
        "                #         while ptr<cols and grid_table[row, ptr] != 0:\n",
        "                #             ptr += 1\n",
        "                #         if ptr == cols:\n",
        "                #             grid_table[row, :-1] = grid_table[row, 1:]\n",
        "                #         else:\n",
        "                #             grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                        \n",
        "                #     if self.pm_strategy == 2:\n",
        "                #         while col < cols_e and grid_table[row, col] != 0:\n",
        "                #             col += 1\n",
        "                #         if col > cols: # update maximum cols in current grid\n",
        "                #             print(grid_table[row,:col])\n",
        "                #             print('overlap in {} <{}> row {} r{}c{}!'.\n",
        "                #                   format(file_name, self.index_to_word[dict_id], row, rows, cols))\n",
        "                #             cols = col\n",
        "                #         if col == cols_e:      \n",
        "                #             print('overlap!')\n",
        "                    \n",
        "                #     grid_table[row, col] = dict_id\n",
        "                #     grid_label[row, col] = entity_class_id\n",
        "                #     ps_x[row, col] = box_x\n",
        "                #     ps_y[row, col] = box_y\n",
        "                #     bbox_mapid[row*cols+col] = bbox_id     \n",
        "                #     bbox[row*cols+col] = box\n",
        "                \n",
        "            cols = self.fit_shape(cols)\n",
        "            grid_table = grid_table[..., :cols]\n",
        "            grid_label = grid_label[..., :cols]\n",
        "            ps_x = np.array(ps_x[..., :cols])\n",
        "            ps_y = np.array(ps_y[..., :cols])\n",
        "            \n",
        "            # if DEBUG:\n",
        "            #     self.grid_visualization(file_name, grid_table, grid_label)\n",
        "            \n",
        "            grid_tables.append(np.expand_dims(grid_table, -1)) \n",
        "            gird_labels.append(grid_label) \n",
        "            ps_indices_x.append(ps_x)\n",
        "            ps_indices_y.append(ps_y)\n",
        "            bboxes[file_name] = bbox\n",
        "            label_mapids.append(label_mapid)\n",
        "            bbox_mapids.append(bbox_mapid)\n",
        "            file_names.append(file_name)\n",
        "            \n",
        "        return grid_tables, gird_labels, bboxes, label_mapids, bbox_mapids, file_names, cols, ps_indices_x, ps_indices_y\n",
        "    \n",
        "    def positional_sampling(self, path, file_names, ps_indices_x, ps_indices_y, updated_cols):\n",
        "        images, ps_1d_indices = [], []\n",
        "        \n",
        "        ## load image and generate corresponding @ps_1dindices\n",
        "        max_h, max_w = 0, updated_cols\n",
        "        for i in range(len(file_names)):\n",
        "            file_name = file_names[i]\n",
        "            file_path = join(path, file_name) # TBD: ensure image is upright\n",
        "            ps_1d_x = np.array(ps_indices_x[i], dtype=np.float32).reshape([-1])\n",
        "            ps_1d_y = np.array(ps_indices_y[i], dtype=np.float32).reshape([-1])\n",
        "            \n",
        "            image = cv2.imread(file_path)\n",
        "            if image is not None:\n",
        "                h, w, _ = image.shape # [h,w,c]\n",
        "                factor = max_w / w\n",
        "                \n",
        "                h = int(h*factor)\n",
        "                ps_1d_x *= factor # TBD: implement more accurate mapping method rather than nearest neighbor, since the .4 or .6 leads to two different sampling results\n",
        "                ps_1d_y *= factor                \n",
        "                \n",
        "                ps_1d = np.int32(np.floor(ps_1d_x) + np.floor(ps_1d_y) * max_w)\n",
        "                max_items = max_w * h - 1\n",
        "                for i in range(len(ps_1d)):\n",
        "                    if ps_1d[i] > max_items - 1:\n",
        "                        ps_1d[i] = max_items - 1\n",
        "                    \n",
        "                \n",
        "                image = cv2.resize(image, (max_w, h))\n",
        "                image = (image-127.5) / 255\n",
        "            else:\n",
        "                print('{} ignored due to image file not found.'.format(file_path))\n",
        "                image, ps_1d = None, None\n",
        "                break\n",
        "                \n",
        "            if image is not None and ps_1d is not None: # ignore data with no images                 \n",
        "                ps_1d_indices.append(ps_1d)\n",
        "                images.append(image)\n",
        "                h,w,c = image.shape\n",
        "                if h > max_h:\n",
        "                    max_h = h\n",
        "            else:\n",
        "                pass\n",
        "                #print('{} ignored due to image file not found.'.format(file_path))\n",
        "                \n",
        "        ## pad image to the same shape\n",
        "        for i,image in enumerate(images): \n",
        "            pad_img = np.zeros([max_h, max_w, 3], dtype=image.dtype)\n",
        "            pad_img[:image.shape[0], :, :] = image\n",
        "            images[i] = pad_img\n",
        "        \n",
        "        return images, ps_1d_indices\n",
        "    \n",
        "    def load_data(self, data_files, update_dict=False):\n",
        "        \"\"\"\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        load doc words with location and class returned in format: \n",
        "        [[file_name, text, word_id, [x_left, y_top, x_right, y_bottom], [left, top, right, bottom], max_row_words, max_col_words] ]\n",
        "        \"\"\"\n",
        "        label_dressed = {}\n",
        "        doc_dressed = []\n",
        "        if not data_files:\n",
        "            print(\"no data file found.\")        \n",
        "        for file in data_files:\n",
        "            with open(file, encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                file_id = data['global_attributes']['file_id']\n",
        "                \n",
        "                label = self.collect_label(file_id, data['fileds'])\n",
        "                # ignore corrupted data\n",
        "                if not label:\n",
        "                    continue                \n",
        "                label_dressed.update(label) \n",
        "                \n",
        "                data = self.collect_data(file_id, data['text_boxes'], update_dict)\n",
        "                for i in data:\n",
        "                    doc_dressed.append(i)\n",
        "                    \n",
        "        return doc_dressed, label_dressed       \n",
        "    \n",
        "    def cal_rows_cols(self, docs, extra_augmentation=False, dropout=False):                  \n",
        "        max_row = self.encoding_factor\n",
        "        max_col = self.encoding_factor\n",
        "        for doc in docs:\n",
        "            for line in doc: \n",
        "                _, _, _, _, _, max_row_words, max_col_words = line\n",
        "                if max_row_words > max_row:\n",
        "                    max_row = max_row_words\n",
        "                if max_col_words > max_col:\n",
        "                    max_col = max_col_words\n",
        "        \n",
        "        pre_rows = self.fit_shape(max_row) #(max_row//self.encoding_factor+1) * self.encoding_factor\n",
        "        pre_cols = self.fit_shape(max_col) #(max_col//self.encoding_factor+1) * self.encoding_factor\n",
        "        \n",
        "        rows, cols = 0, 0\n",
        "        if extra_augmentation:\n",
        "            pad_row = int(random.gauss(0, self.da_extra_rows*self.encoding_factor)) #abs(random.gauss(0, u))\n",
        "            pad_col = int(random.gauss(0, self.da_extra_cols*self.encoding_factor)) #random.randint(0, u)\n",
        "            \n",
        "            if self.augment_strategy == 1: # strategy 1: augment data by increasing grid shape sizes\n",
        "                pad_row = abs(pad_row)\n",
        "                pad_col = abs(pad_col)\n",
        "                rows = self.fit_shape(max_row+pad_row) # apply upper boundary to avoid OOM\n",
        "                cols = self.fit_shape(max_col+pad_col) # apply upper boundary to avoid OOM\n",
        "            elif self.augment_strategy == 2 or self.augment_strategy == 3: # strategy 2: augment by increasing or decreasing the target gird shape size\n",
        "                rows = self.fit_shape(max(self.rows_target+pad_row, max_row)) # protect grid shape\n",
        "                cols = self.fit_shape(max(self.cols_target+pad_col, max_col)) # protect grid shape\n",
        "            else:\n",
        "                raise Exception('unknown augment strategy')\n",
        "            rows = min(rows, self.rows_ulimit) # apply upper boundary to avoid OOM\n",
        "            cols = min(cols, self.cols_ulimit) # apply upper boundary to avoid OOM                                \n",
        "        else:\n",
        "            rows = pre_rows\n",
        "            cols = pre_cols\n",
        "        return rows, cols, pre_rows, pre_cols \n",
        "    \n",
        "    def fit_shape(self, shape): # modify shape size to fit the encoding factor\n",
        "        while shape % self.encoding_factor:\n",
        "            shape += 1\n",
        "        return shape\n",
        "    \n",
        "    def expand_shape(self, shape): # expand shape size with step 2\n",
        "        return self.fit_shape(shape+1)\n",
        "        \n",
        "    def collect_data(self, file_name, content, update_dict):\n",
        "        \"\"\"\n",
        "        dress and preserve only interested data.\n",
        "        \"\"\"          \n",
        "        content_dressed = []\n",
        "        left, top, right, bottom, buffer = 9999, 9999, 0, 0, 2\n",
        "        for line in content:\n",
        "            bbox = line['bbox'] # handle data corrupt\n",
        "            if len(bbox) == 0:\n",
        "                continue\n",
        "            if line['text'] in self.special_dict: # ignore potential overlap causing characters\n",
        "                continue\n",
        "            \n",
        "            x_left, y_top, x_right, y_bottom = self.dress_bbox(bbox)        \n",
        "            # TBD: the real image size is better for calculating the relative x/y/w/h\n",
        "            if x_left < left: left = x_left - buffer\n",
        "            if y_top < top: top = y_top - buffer\n",
        "            if x_right > right: right = x_right + buffer\n",
        "            if y_bottom > bottom: bottom = y_bottom + buffer\n",
        "            \n",
        "            word_id = line['word_id']\n",
        "            dressed_texts = self.dress_text(line['text'], update_dict)\n",
        "            \n",
        "            num_block = len(dressed_texts)\n",
        "            for i, dressed_text in enumerate(dressed_texts): # handling tokenized text, separate bbox\n",
        "                new_left = int(x_left + (x_right-x_left) / num_block * (i))\n",
        "                new_right = int(x_left + (x_right-x_left) / num_block * (i+1))\n",
        "                content_dressed.append([file_name, dressed_text, word_id, [new_left, y_top, new_right, y_bottom]])\n",
        "            \n",
        "        # initial calculation of maximum number of words in rows/cols in terms of image size\n",
        "        num_words_row = [0 for _ in range(bottom)] # number of words in each row\n",
        "        num_words_col = [0 for _ in range(right)] # number of words in each column\n",
        "        for line in content_dressed:\n",
        "            _, _, _, [x_left, y_top, x_right, y_bottom] = line\n",
        "            for y in range(y_top, y_bottom):\n",
        "                num_words_row[y] += 1\n",
        "            for x in range(x_left, x_right):\n",
        "                num_words_col[x] += 1\n",
        "        max_row_words = self.fit_shape(max(num_words_row))\n",
        "        max_col_words = 0#self.fit_shape(max(num_words_col))\n",
        "        \n",
        "        # further expansion of maximum number of words in rows/cols in terms of grid shape\n",
        "        max_rows = max(self.encoding_factor, max_row_words)\n",
        "        max_cols = max(self.encoding_factor, max_col_words)\n",
        "        DONE = False\n",
        "        while not DONE:\n",
        "            DONE = True\n",
        "            grid_table = np.zeros([max_rows, max_cols], dtype=np.int32)\n",
        "            for line in content_dressed:\n",
        "                _, _, _, [x_left, y_top, x_right, y_bottom] = line\n",
        "                row = int(max_rows * (y_top - top + (y_bottom-y_top)/2) / (bottom-top))\n",
        "                col = int(max_cols * (x_left - left + (x_right-x_left)/2) / (right-left))\n",
        "                #row = int(max_rows * (y_top + (y_bottom-y_top)/2) / (bottom))\n",
        "                #col = int(max_cols * (x_left + (x_right-x_left)/2) / (right))\n",
        "                #row = int(max_rows * (y_top-top) / (bottom-top))\n",
        "                #col = int(max_cols * (x_left-left) / (right-left))\n",
        "                #row = int(max_rows * (y_top) / (bottom))\n",
        "                #col = int(max_cols * (x_left) / (right))\n",
        "                #row = int(max_rows * (y_top + (y_bottom-y_top)/2) / bottom)  \n",
        "                #col = int(max_cols * (x_left + (x_right-x_left)/2) / right) \n",
        "                \n",
        "                while col < max_cols and grid_table[row, col] != 0: # shift to find slot to drop the current item\n",
        "                    col += 1\n",
        "                if col == max_cols: # shift to find slot to drop the current item\n",
        "                    col -= 1\n",
        "                    ptr = 0\n",
        "                    while ptr<max_cols and grid_table[row, ptr] != 0:\n",
        "                        ptr += 1\n",
        "                    if ptr == max_cols: # overlap cannot be solved in current row, then expand the grid\n",
        "                        max_cols = self.expand_shape(max_cols)\n",
        "                        DONE = False\n",
        "                        break\n",
        "                    \n",
        "                    grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                \n",
        "                if DONE:\n",
        "                    if row > max_rows or col>max_cols:\n",
        "                        print('wrong')\n",
        "                    grid_table[row, col] = 1\n",
        "        \n",
        "        max_rows = self.fit_shape(max_rows)\n",
        "        max_cols = self.fit_shape(max_cols)\n",
        "        \n",
        "        #print('{} collected in shape: {},{}'.format(file_name, max_rows, max_cols))\n",
        "        \n",
        "        # segment grid into two parts if number of cols is larger than self.cols_target\n",
        "        data = []\n",
        "        if self.segment_grid and max_cols > self.cols_segment:\n",
        "            content_dressed_left = []\n",
        "            content_dressed_right = []\n",
        "            cnt = defaultdict(int) # counter for number of words in a specific row\n",
        "            cnt_l, cnt_r = defaultdict(int), defaultdict(int) # update max_cols if larger than self.cols_segment\n",
        "            left_boundary = max_cols - self.cols_segment\n",
        "            right_boundary = self.cols_segment\n",
        "            for i, line in enumerate(content_dressed):\n",
        "                file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom] = line\n",
        "                \n",
        "                row = int(max_rows * (y_top + (y_bottom-y_top)/2) / bottom)\n",
        "                cnt[row] += 1                \n",
        "                if cnt[row] <= left_boundary:\n",
        "                    cnt_l[row] += 1\n",
        "                    content_dressed_left.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, self.cols_segment])\n",
        "                elif left_boundary < cnt[row] <= right_boundary:\n",
        "                    cnt_l[row] += 1\n",
        "                    cnt_r[row] += 1\n",
        "                    content_dressed_left.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, self.cols_segment])\n",
        "                    content_dressed_right.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max(max(cnt_r.values()), self.cols_segment)])\n",
        "                else:\n",
        "                    cnt_r[row] += 1\n",
        "                    content_dressed_right.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max(max(cnt_r.values()), self.cols_segment)])\n",
        "            #print(sorted(cnt.items(), key=lambda x:x[1], reverse=True))\n",
        "            #print(sorted(cnt_l.items(), key=lambda x:x[1], reverse=True))\n",
        "            #print(sorted(cnt_r.items(), key=lambda x:x[1], reverse=True))\n",
        "            if max(cnt_l.values()) < 2*self.cols_segment:\n",
        "                data.append(content_dressed_left)\n",
        "            if max(cnt_r.values()) < 2*self.cols_segment: # avoid OOM, which tends to happen in the right side\n",
        "                data.append(content_dressed_right)\n",
        "        else:\n",
        "            for i, line in enumerate(content_dressed): # append height/width/numofwords to the list\n",
        "                file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom] = line\n",
        "                content_dressed[i] = [file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max_cols ]\n",
        "            data.append(content_dressed)\n",
        "        return data\n",
        "    \n",
        "    def collect_label(self, file_id, content):\n",
        "        \"\"\"\n",
        "        dress and preserve only interested data.\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        \"\"\"\n",
        "        label_dressed = dict()\n",
        "        label_dressed[file_id] = {cls:[] for cls in self.classes[1:]}\n",
        "        for line in content:\n",
        "            cls = line['field_name']\n",
        "            if cls in self.classes:\n",
        "                #identity = line.get('identity', 0) \n",
        "                label_dressed[file_id][cls].append( {'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''} )\n",
        "                label_dressed[file_id][cls][-1]['key_id'] = line.get('key_id', [])\n",
        "                label_dressed[file_id][cls][-1]['value_id'] = line['value_id'] # value_id\n",
        "                label_dressed[file_id][cls][-1]['key_text'] = line.get('key_text', []) \n",
        "                label_dressed[file_id][cls][-1]['value_text'] = line['value_text'] # value_text\n",
        "                \n",
        "        # handle corrupted data\n",
        "        for cls in label_dressed[file_id]: \n",
        "            for idx, label in enumerate(label_dressed[file_id][cls]):\n",
        "                if len(label) == 0: # no relevant class in sample @file_id\n",
        "                    continue\n",
        "                if (len(label['key_text'])>0 and len(label['key_id'])==0) or \\\n",
        "                   (len(label['value_text'])>0 and len(label['value_id'])==0):\n",
        "                    return None\n",
        "            \n",
        "        return label_dressed\n",
        "\n",
        "    def dress_class(self, file_name, word_id, labels):\n",
        "        \"\"\"\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        \"\"\"\n",
        "        if file_name in labels:\n",
        "            for cls, cls_labels in labels[file_name].items():\n",
        "                for idx, cls_label in enumerate(cls_labels):\n",
        "                    for key, values in cls_label.items():\n",
        "                        if (key=='key_id' or key=='value_id') and word_id in values:\n",
        "                            if key == 'key_id':\n",
        "                                if self.data_mode == 0:\n",
        "                                    return idx, self.classes.index(cls) * 2 - 1 # odd\n",
        "                                elif self.data_mode == 1:\n",
        "                                    return idx, self.classes.index(cls)\n",
        "                                else: # ignore key_id when self.data_mode is not 0 or 1\n",
        "                                    return 0, 0\n",
        "                            elif key == 'value_id':\n",
        "                                if self.data_mode == 0:\n",
        "                                    return idx, self.classes.index(cls) * 2 # even \n",
        "                                else: # when self.data_mode is 1 or 2\n",
        "                                    return idx, self.classes.index(cls) \n",
        "            return 0, 0 # 0 is of class type 'DontCare'\n",
        "        print(\"No matched labels found for {}\".format(file_name))\n",
        "    \n",
        "    def dress_text(self, text, update_dict):\n",
        "        \"\"\"\n",
        "        three cases covered: \n",
        "        alphabetic string, numeric string, special character\n",
        "        \"\"\"\n",
        "        string = text.lower()\n",
        "        for i, c in enumerate(string):\n",
        "            if is_number(c):\n",
        "                string = string[:i] + '0' + string[i+1:]\n",
        "                \n",
        "        strings = [string]\n",
        "        # if self.tokenize:\n",
        "        #     strings = self.tokenizer.tokenize(strings[0])\n",
        "        #     #print(string, '-->', strings)\n",
        "            \n",
        "        for idx, string in enumerate(strings):            \n",
        "            if string.isalpha():\n",
        "                if string in self.special_dict:\n",
        "                    string = self.special_dict[string]\n",
        "                # TBD: convert a word to its most similar word in a known vocabulary\n",
        "            elif is_number(string):\n",
        "                pass\n",
        "            elif len(string)==1: # special character\n",
        "                pass\n",
        "            else:\n",
        "                # TBD: seperate string as parts for alpha and number combinated strings\n",
        "                #string = re.findall('[a-z]+', string)\n",
        "                pass            \n",
        "            \n",
        "            if string not in self.dictionary.keys():\n",
        "                if update_dict:\n",
        "                    self.dictionary[string] = 0\n",
        "                else:\n",
        "                    #print('unknown text: ' + string)\n",
        "                    string = '[UNK]' # TBD: take special care to unmet words\\\n",
        "            self.dictionary[string] += 1\n",
        "            \n",
        "            strings[idx] = string\n",
        "        return strings\n",
        "            \n",
        "    def dress_bbox(self, bbox):\n",
        "        positions = np.array(bbox).reshape([-1])\n",
        "        x_left = max(0, min(positions[0::2]))\n",
        "        x_right = max(positions[0::2])\n",
        "        y_top = max(0, min(positions[1::2]))\n",
        "        y_bottom = max(positions[1::2])\n",
        "        w = x_right - x_left\n",
        "        h = y_bottom - y_top\n",
        "        return int(x_left), int(y_top), int(x_right), int(y_bottom)       \n",
        "    \n",
        "    def get_filenames(self, data_path):\n",
        "        files = []\n",
        "        for dirpath,dirnames,filenames in walk(data_path):\n",
        "            for filename in filenames:\n",
        "                file = join(dirpath,filename)\n",
        "                if file.endswith('csv') or file.endswith('json'):\n",
        "                    files.append(file)\n",
        "        return files       \n",
        "            \n",
        "    # def grid_visualization(self, file_name, grid, label):\n",
        "    #     import cv2\n",
        "    #     height, width = np.shape(grid)\n",
        "    #     grid_box_h, grid_box_w = 20, 40\n",
        "    #     palette = np.zeros([height*grid_box_h, width*grid_box_w, 3], np.uint8)\n",
        "    #     font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    #     gt_color = [[255, 250, 240], [152, 245, 255], [127, 255, 212], [100, 149, 237], \n",
        "    #                 [192, 255, 62], [175, 238, 238], [255, 130, 171], [240, 128, 128], [255, 105, 180]]\n",
        "    #     cv2.putText(palette, file_name+\"({},{})\".format(height,width), (grid_box_h,grid_box_w), font, 0.6, [255,0,0])  \n",
        "    #     for h in range(height):\n",
        "    #         cv2.line(palette, (0,h*grid_box_h), (width*grid_box_w, h*grid_box_h), (100,100,100))\n",
        "    #         for w in range(width):\n",
        "    #             if grid[h,w]:\n",
        "    #                 org = (int((w+1)*grid_box_w*0.7),int((h+1)*grid_box_h*0.9))\n",
        "    #                 color = gt_color[label[h,w]]\n",
        "    #                 cv2.putText(palette, self.index_to_word[grid[h,w]], org, font, 0.4, color)        \n",
        "        \n",
        "    #     img = cv2.imread(self.doc_path+'/'+file_name)\n",
        "    #     if img is not None:\n",
        "    #         shape = list(img.shape)\n",
        "    #         max_len = 768\n",
        "    #         factor = max_len / max(shape)\n",
        "    #         shape[0], shape[1] = [int(s*factor) for s in shape[:2]]\n",
        "    #         img = cv2.resize(img, (shape[1], shape[0]))  \n",
        "    #         cv2.imshow(\"img\", img)\n",
        "    #     cv2.imshow(\"grid\", palette)\n",
        "    #     cv2.waitKey(0)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oggp9nxa6NuG"
      },
      "source": [
        "# 2. Set up Model and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu-39pT9WRQV"
      },
      "source": [
        "# Set up layers for the model framework\n",
        "\n",
        "def layer(op):\n",
        "    def layer_decorated(self, *args, **kwargs):\n",
        "        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))        \n",
        "        if len(self.layer_inputs) == 0:\n",
        "            raise RuntimeError('No input variables found for layers %s' % name)\n",
        "        elif len(self.layer_inputs) == 1:\n",
        "            layer_input = self.layer_inputs[0]\n",
        "        else:\n",
        "            layer_input = list(self.layer_inputs)            \n",
        "            \n",
        "        layer_output = op(self, layer_input, *args, **kwargs)\n",
        "        \n",
        "        self.layers[name] = layer_output\n",
        "        self.feed(layer_output)\n",
        "        \n",
        "        return self\n",
        "    return layer_decorated\n",
        "    \n",
        "    \n",
        "class Model(object):\n",
        "    def __init__(self, trainable=True):\n",
        "        self.layers = dict()      \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "    \n",
        "    \n",
        "    # def build_loss(self):\n",
        "    #     raise NotImplementedError('Must be subclassed.')\n",
        "    \n",
        "    \n",
        "    def setup(self):        \n",
        "        raise NotImplementedError('Must be subclassed.')\n",
        "     \n",
        "    \n",
        "    @layer\n",
        "    def embed(self, layer_input, vocabulary_size, embedding_size, name, dropout=1, trainable=True):\n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_embedding = tf.random_uniform_initializer(-1.0, 1.0)\n",
        "            embeddings = self.make_var('weights', [vocabulary_size, embedding_size], init_embedding, None, trainable)\n",
        "            shape = tf.shape(layer_input)\n",
        "            \n",
        "            reshaped_input = tf.reshape(layer_input, [-1])\n",
        "            e = tf.nn.embedding_lookup(embeddings, reshaped_input)\n",
        "            e = tf.nn.dropout(e, dropout)\n",
        "            reshaped_e = tf.reshape(e, [shape[0], shape[1], shape[2], embedding_size])\n",
        "            return reshaped_e\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def bert_embed(self, layer_input, vocab_size, embedding_size=768, use_one_hot_embeddings=False, \n",
        "                   initializer_range=0.02, name=\"embeddings\", trainable=False):\n",
        "        with tf.compat.v1.variable_scope(\"bert\"):\n",
        "          with tf.compat.v1.variable_scope(\"embeddings\"):\n",
        "            # Perform embedding lookup on the word ids.\n",
        "            (embedding_output, embedding_table) = self.embedding_lookup(\n",
        "                input_ids=layer_input, vocab_size=vocab_size, embedding_size=embedding_size,\n",
        "                initializer_range=initializer_range,\n",
        "                word_embedding_name=\"word_embeddings\",\n",
        "                use_one_hot_embeddings=use_one_hot_embeddings,\n",
        "                trainable=trainable)\n",
        "            self.embedding_table = embedding_table # the inherited class need a self.embedding_table variable\n",
        "            return embedding_output        \n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def positional_sampling(self, layer_input, feature_dimension, name='positional_sampling'):\n",
        "        featuremap = layer_input[0]\n",
        "        batch_indices = layer_input[1]\n",
        "        grid = layer_input[2]        \n",
        "        \n",
        "        shape_grid = tf.shape(grid)\n",
        "        \n",
        "        featuremap_flat = tf.reshape(featuremap, [shape_grid[0], -1, feature_dimension])        \n",
        "        batch_indices_flat = tf.reshape(batch_indices, [shape_grid[0], -1])        \n",
        "        batch_ps_flat = tf.batch_gather(featuremap_flat, batch_indices_flat)\n",
        "        \n",
        "        b, h, w, c = shape_grid[0], shape_grid[1], shape_grid[2], feature_dimension\n",
        "        return tf.reshape(batch_ps_flat, [b,h,w,c])\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def sepconv(self, layer_input, k_h, k_w, cardinality, compression, name, activation='relu', trainable=True):\n",
        "        \"\"\" customized seperable convolution\n",
        "        \"\"\"\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')\n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape().as_list()[-1]\n",
        "            \n",
        "            layer_output = []\n",
        "            c = c_i / cardinality / compression\n",
        "            for _ in range(cardinality):\n",
        "                a = self.convolution(convolve, activate, layer_input, 1, 1, c_i, c,\n",
        "                                     init_weights, init_biases, regularizer, trainable, '0_{}'.format(_))                \n",
        "                a = self.convolution(convolve, activate, a, k_h, k_w, c, c, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '1_{}'.format(_))\n",
        "                a = self.convolution(convolve, activate, a, 1, 1, c, c_i, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '2_{}'.format(_))\n",
        "                layer_output.append(a)\n",
        "            layer_output = tf.add_n(layer_output)\n",
        "            return tf.add(layer_output, layer_input)\n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def up_sepconv(self, layer_input, k_h, k_w, cardinality, compression, name, activation='relu', trainable=True):\n",
        "        \"\"\" customized upscale seperable convolution\n",
        "        \"\"\"\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')        \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            shape = tf.shape(layer_input)\n",
        "            h = shape[1]\n",
        "            w = shape[2]\n",
        "            layer_input = tf.image.resize_nearest_neighbor(layer_input, [2*h, 2*w])\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape().as_list()[-1]\n",
        "            \n",
        "            layer_output = []\n",
        "            c = c_i / cardinality / compression\n",
        "            for _ in range(cardinality):\n",
        "                a = self.convolution(convolve, activate, layer_input, 1, 1, c_i, c,\n",
        "                                     init_weights, init_biases, regularizer, trainable, '0_{}'.format(_))                \n",
        "                a = self.convolution(convolve, activate, a, k_h, k_w, c, c, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '1_{}'.format(_))\n",
        "                a = self.convolution(convolve, activate, a, 1, 1, c, c_i, \n",
        "                                     init_weights, init_biases, regularizer, trainable, '2_{}'.format(_))\n",
        "                layer_output.append(a)\n",
        "            layer_output = tf.add_n(layer_output)\n",
        "            return tf.add(layer_output, layer_input)\n",
        "        \n",
        "        \n",
        "    @layer\n",
        "    def dense_block(self, layer_input, k_h, k_w, c_o, depth, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')\n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)  \n",
        "            \n",
        "            layer_tmp = layer_input\n",
        "            for d in range(depth):          \n",
        "                c_i = layer_tmp.get_shape()[-1]\n",
        "                a = self.convolution(convolve, activate, layer_tmp, 1, 1, c_i, c_i//2,\n",
        "                                     init_weights, init_biases, regularizer, trainable)\n",
        "                \n",
        "                a = self.convolution(convolve, activate, a, k_h, k_w, c_i, c_o, \n",
        "                                     init_weights, init_biases, regularizer, trainable)\n",
        "                \n",
        "                layer_tmp = tf.concat([a, layer_input], 3)\n",
        "                \n",
        "            return layer_tmp\n",
        "            \n",
        "        \n",
        "    @layer\n",
        "    def conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,s_h,s_w,1], 'SAME')\n",
        "        #convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, 2, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "     \n",
        "     \n",
        "    @layer\n",
        "    def dilate_conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, rate, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, rate, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def dilate_module(self, layer_input, k_h, k_w, c_o, s_h, s_w, rate, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, rate, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def up_conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, name, factor=2, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,s_h,s_w,1], 'SAME')\n",
        "        #convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, 2, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')        \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            shape = tf.shape(layer_input)\n",
        "            h = shape[1]\n",
        "            w = shape[2]\n",
        "            layer_input = tf.image.resize_nearest_neighbor(layer_input, [factor*h, factor*w])\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def attention(self, layer_input, num_heads, name, att_dropout=0.0, hidden_dropout=0.1, trainable=True):\n",
        "    #     \"\"\"\n",
        "    #     implement self attention with residual addition,\n",
        "    #     layer_input[0] and layer_input[1] should have the same shape for residual addition \n",
        "    #     \"\"\"\n",
        "    #     f = layer_input[0]\n",
        "    #     x = layer_input[1]\n",
        "        \n",
        "    #     convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "    #     with tf.variable_scope(name) as scope:\n",
        "    #         init_weights = tf.truncated_normal_initializer(0.0, 0.02)\n",
        "    #         regularizer = self.l2_regularizer(self.weight_decay)\n",
        "    #         shape = tf.shape(f)\n",
        "    #         c_i = f.get_shape()[-1]\n",
        "    #         c_o = f.get_shape()[-1]\n",
        "    #         c_a = c_o // num_heads # attention kernel depth, size per head\n",
        "            \n",
        "    #         query = self.make_var('weights_query', [1, 1, c_i, c_a], init_weights, regularizer, trainable)\n",
        "    #         query_layer = convolve(f, query) # [B, H, W, c_a]\n",
        "    #         query_layer = tf.reshape(query_layer, [shape[0], -1, c_a]) # [B, H*W, c_a]\n",
        "            \n",
        "    #         key = self.make_var('weights_key', [1, 1, c_i, c_a], init_weights, regularizer, trainable)\n",
        "    #         key_layer = convolve(f, key) # [B, H, W, c_a]\n",
        "    #         key_layer = tf.reshape(key_layer, [shape[0], -1, c_a]) # [B, H*W, c_a]\n",
        "            \n",
        "    #         value = self.make_var('weights_value', [1, 1, c_i, c_o], init_weights, regularizer, trainable)\n",
        "    #         value_layer = convolve(f, value) \n",
        "    #         value_layer = tf.reshape(value_layer, [shape[0], -1, c_o])# [B, H*W, c_o]\n",
        "            \n",
        "    #         attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) # [B, H*W, H*W]\n",
        "    #         attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(c_a.value)))\n",
        "            \n",
        "    #         attention_probs = tf.nn.softmax(attention_scores)\n",
        "    #         #attention_probs = dropout(attention_probs, att_dropout)\n",
        "            \n",
        "    #         context_layer = tf.matmul(attention_probs, value_layer) # [B, H*W, c_o]\n",
        "    #         context_layer = tf.reshape(context_layer, shape) # [B, H, W, c_o]\n",
        "            \n",
        "    #         kernel = self.make_var('output', [1, 1, c_o, c_o], init_weights, regularizer, trainable)\n",
        "    #         attention_output = convolve(context_layer, kernel) \n",
        "    #         #attention_output = dropout(attention_output, hidden_dropout)\n",
        "    #         attention_output = attention_output + x\n",
        "            \n",
        "    #         return tf.contrib.layers.instance_norm(attention_output, center=False, scale=False)\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def concat(self, layer_input, axis, name):\n",
        "        return tf.concat(layer_input, axis)\n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def add(self, layer_input, name):\n",
        "    #     return tf.math.add_n(layer_input)\n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def max_pool(self, layer_input, k_h, k_w, s_h, s_w, name, padding='SAME'):\n",
        "        return tf.nn.max_pool(layer_input, [1,k_h,k_w,1], [1,s_h,s_w,1], name=name, padding=padding)\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def global_pool(self, layer_input, name):\n",
        "        shape = tf.shape(layer_input)\n",
        "        h = shape[1]\n",
        "        w = shape[2]\n",
        "        output = tf.reduce_mean(layer_input, [1,2], keepdims=True, name=name)\n",
        "        return tf.image.resize_nearest_neighbor(output, [h, w])\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def softmax(self, layer_input, name):\n",
        "        return tf.nn.softmax(layer_input, name=name)      \n",
        "    \n",
        "    \n",
        "    # def embedding_lookup(self, input_ids, vocab_size, embedding_size=768,\n",
        "    #                      initializer_range=0.02, word_embedding_name=\"word_embeddings\",\n",
        "    #                      use_one_hot_embeddings=False, trainable=False):\n",
        "    #     \"\"\"Looks up words embeddings for id tensor.\n",
        "        \n",
        "    #     Args:\n",
        "    #       input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
        "    #         ids.\n",
        "    #       vocab_size: int. Size of the embedding vocabulary.\n",
        "    #       embedding_size: int. Width of the word embeddings.\n",
        "    #       initializer_range: float. Embedding initialization range.\n",
        "    #       word_embedding_name: string. Name of the embedding table.\n",
        "    #       use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
        "    #         embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n",
        "    #         for TPUs.\n",
        "        \n",
        "    #     Returns:\n",
        "    #       float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "    #     \"\"\"\n",
        "    #     bert_vocab_size = 119547\n",
        "    #     # This function assumes that the input is of shape [batch_size, seq_length,\n",
        "    #     # num_inputs].\n",
        "    #     #\n",
        "    #     # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
        "    #     # reshape to [batch_size, seq_length, 1].\n",
        "    #     if input_ids.shape.ndims == 3: # originally 2\n",
        "    #         input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "        \n",
        "    #     bert_embedding_table = embedding_table = tf.get_variable(\n",
        "    #         name=word_embedding_name,\n",
        "    #         shape=[bert_vocab_size, embedding_size],\n",
        "    #         initializer=tf.truncated_normal_initializer(stddev=initializer_range),\n",
        "    #         trainable=trainable)\n",
        "    #     if vocab_size > bert_vocab_size: # handle dict augmentation\n",
        "    #         embedding_table_plus = tf.get_variable(\n",
        "    #             name=word_embedding_name + '_plus',\n",
        "    #             shape=[vocab_size-bert_vocab_size, embedding_size],\n",
        "    #             initializer=tf.truncated_normal_initializer(stddev=initializer_range),\n",
        "    #             trainable=True)\n",
        "    #         embedding_table = tf.concat([embedding_table, embedding_table_plus], 0)        \n",
        "        \n",
        "    #     if use_one_hot_embeddings:\n",
        "    #         flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "    #         one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    #         output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "    #     else:\n",
        "    #         output = tf.nn.embedding_lookup(embedding_table, input_ids)\n",
        "        \n",
        "    #     input_shape = self.get_shape_list(input_ids)\n",
        "        \n",
        "    #     output = tf.reshape(output,\n",
        "    #                         input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "    #     return (output, bert_embedding_table)\n",
        "    \n",
        "    # def get_shape_list(self, tensor, expected_rank=None, name=None):\n",
        "    #     \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "        \n",
        "    #     Args:\n",
        "    #       tensor: A tf.Tensor object to find the shape of.\n",
        "    #       expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "    #         specified and the `tensor` has a different rank, and exception will be\n",
        "    #         thrown.\n",
        "    #       name: Optional name of the tensor for the error message.\n",
        "        \n",
        "    #     Returns:\n",
        "    #       A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    #       be returned as python integers, and dynamic dimensions will be returned\n",
        "    #       as tf.Tensor scalars.\n",
        "    #     \"\"\"\n",
        "    #     if name is None:\n",
        "    #       name = tensor.name\n",
        "        \n",
        "    #     if expected_rank is not None:\n",
        "    #       assert_rank(tensor, expected_rank, name)\n",
        "        \n",
        "    #     shape = tensor.shape.as_list()\n",
        "        \n",
        "    #     non_static_indexes = []\n",
        "    #     for (index, dim) in enumerate(shape):\n",
        "    #       if dim is None:\n",
        "    #         non_static_indexes.append(index)\n",
        "        \n",
        "    #     if not non_static_indexes:\n",
        "    #       return shape\n",
        "        \n",
        "    #     dyn_shape = tf.shape(tensor)\n",
        "    #     for index in non_static_indexes:\n",
        "    #       shape[index] = dyn_shape[index]\n",
        "    #     return shape\n",
        "    \n",
        "    \n",
        "    def convolution(self, convolve, activate, input, k_h, k_w, c_i, c_o, init_weights, init_biases, \n",
        "                    regularizer, trainable, name=''):   \n",
        "        kernel = self.make_var('weights'+name, [k_h, k_w, c_i, c_o], init_weights, regularizer, trainable) \n",
        "        biases = self.make_var('biases'+name, [c_o], init_biases, None, trainable)\n",
        "        tf.summary.histogram('w', kernel)\n",
        "        tf.summary.histogram('b', biases)\n",
        "        # test with different orders: convolve/activate/normalize; normalize/convolve/activate; convolve/normalize/activate\n",
        "        wx = convolve(input, kernel)\n",
        "        a = activate(tf.nn.bias_add(wx, biases))\n",
        "        a = tf.contrib.layers.instance_norm(a, center=False, scale=False)\n",
        "        return a\n",
        "    \n",
        "    \n",
        "    def l2_regularizer(self, weight_decay=0.0005, scope=None):\n",
        "        def regularizer(tensor):\n",
        "            with tf.name_scope(scope, default_name='l2_regularizer', values=[tensor]):\n",
        "                factor = tf.convert_to_tensor(weight_decay, name='weight_decay')\n",
        "                return tf.multiply(factor, tf.nn.l2_loss(tensor), name='decayed_value')\n",
        "        return regularizer\n",
        "    \n",
        "    \n",
        "    def make_var(self, name, shape, initializer=None, regularizer=None, trainable=True):\n",
        "        return tf.compat.v1.get_variable(name, shape, initializer=initializer, regularizer=regularizer, trainable=trainable)      \n",
        "    \n",
        "    \n",
        "    def feed(self, *args):\n",
        "        assert len(args) != 0\n",
        "        \n",
        "        self.layer_inputs = []\n",
        "        for layer in args:\n",
        "            if isinstance(layer, str):\n",
        "                try:\n",
        "                    layer = self.layers[layer]\n",
        "                    print(layer)\n",
        "                except KeyError:\n",
        "                    print(list(self.layers.keys()))\n",
        "                    raise KeyError('Unknown layer name fed: %s' % layer)\n",
        "            self.layer_inputs.append(layer)\n",
        "        return self\n",
        "        \n",
        "        \n",
        "    def get_output(self, layer):\n",
        "        try:\n",
        "            layer = self.layers[layer]\n",
        "        except KeyError:\n",
        "            print(list(self.layers.keys()))\n",
        "            raise KeyError('Unknown layer name fed: %s' % layer)\n",
        "        return layer\n",
        "        \n",
        "        \n",
        "    def get_unique_name(self, prefix):\n",
        "        id = sum(t.startswith(prefix) for t,_ in list(self.layers.items())) + 1\n",
        "        return '%s_%d' % (prefix, id)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_T3mimzmHL6"
      },
      "source": [
        "class CUTIE(Model):\n",
        "    \"\"\" Set up mode framework\n",
        "    \"\"\"\n",
        "    def __init__(self, num_vocabs, num_classes, params, trainable=True):\n",
        "        self.name = \"CUTIE_benchmark\"\n",
        "        \n",
        "        self.data = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None, 1], name='grid_table')\n",
        "        self.gt_classes = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None], name='gt_classes')\n",
        "        self.use_ghm = tf.equal(1, params.use_ghm) if hasattr(params, 'use_ghm') else tf.equal(1, 0) #params.use_ghm \n",
        "        self.activation = 'sigmoid' if (hasattr(params, 'use_ghm') and params.use_ghm) else 'relu'\n",
        "        self.ghm_weights = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, num_classes], name='ghm_weights')        \n",
        "        self.layers = dict({'data': self.data, 'gt_classes': self.gt_classes, 'ghm_weights': self.ghm_weights}) \n",
        "         \n",
        "        self.num_vocabs = num_vocabs\n",
        "        self.num_classes = num_classes     \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.embedding_size = params.embedding_size\n",
        "        self.weight_decay = params.weight_decay if hasattr(params, 'weight_decay') else 0.0\n",
        "        self.hard_negative_ratio = params.hard_negative_ratio if hasattr(params, 'hard_negative_ratio') else 0.0\n",
        "        self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 0\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "        \n",
        "    \n",
        "    def setup(self):        \n",
        "        # Input\n",
        "        (self.feed('data')\n",
        "             .embed(self.num_vocabs, self.embedding_size, name='embedding'))  \n",
        "        \n",
        "        # Encoder\n",
        "        (self.feed('embedding')\n",
        "             .conv(3, 5, 64, 1, 1, name='encoder1_1')\n",
        "             .conv(3, 5, 128, 1, 1, name='encoder1_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool1')\n",
        "             .conv(3, 5, 128, 1, 1, name='encoder2_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder2_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool2')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder3_1')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder3_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool3')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder4_1')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder4_2'))\n",
        "        \n",
        "        # Decoder\n",
        "        (self.feed('encoder4_2')\n",
        "             .up_conv(3, 5, 512, 1, 1, name='up1')\n",
        "             .conv(3, 5, 256, 1, 1, name='decoder1_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='decoder1_2')\n",
        "             .up_conv(3, 5, 256, 1, 1, name='up2')\n",
        "             .conv(3, 5, 128, 1, 1, name='decoder2_1')\n",
        "             .conv(3, 5, 128, 1, 1, name='decoder2_2')\n",
        "             .up_conv(3, 5, 128, 1, 1, name='up3')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder3_1')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder3_2'))\n",
        "        \n",
        "        # Classification\n",
        "        (self.feed('decoder3_2')\n",
        "             .conv(1, 1, self.num_classes, 1, 1, activation=self.activation, name='cls_logits')\n",
        "             .softmax(name='softmax'))  \n",
        "        \n",
        "    # def disp_results(self, data_input, data_label, model_output, threshold):\n",
        "    #     data_input_flat = data_input.reshape([-1]) # [b * h * w]\n",
        "    #     labels = [] # [b * h * w, classes]\n",
        "    #     for item in data_label.reshape([-1]):\n",
        "    #         labels.append([i==item for i in range(self.num_classes)])\n",
        "    #     logits = model_output.reshape([-1, self.num_classes]) # [b * h * w, classes] \n",
        "        \n",
        "    #     # ignore none word input\n",
        "    #     labels_flat = []\n",
        "    #     results_flat = []\n",
        "    #     for idx, item in enumerate(data_input_flat):\n",
        "    #         if item != 0: \n",
        "    #             labels_flat.extend(labels[idx])\n",
        "    #             results_flat.extend(logits[idx] > threshold)\n",
        "        \n",
        "    #     num_p = sum(labels_flat)\n",
        "    #     num_n = sum([1-label for label in labels_flat])   \n",
        "    #     num_all = len(results_flat)     \n",
        "    #     num_correct = sum([True for i in range(num_all) if labels_flat[i] == results_flat[i]])        \n",
        "        \n",
        "    #     labels_flat_p = [label!=0 for label in labels_flat]\n",
        "    #     labels_flat_n = [label==0 for label in labels_flat]\n",
        "    #     num_tp = sum([labels_flat_p[i] * results_flat[i] for i in range(num_all)])\n",
        "    #     num_tn = sum([labels_flat_n[i] * (not results_flat[i]) for i in range(num_all)])\n",
        "    #     num_fp = num_n - num_tp\n",
        "    #     num_fn = num_p - num_tp\n",
        "        \n",
        "    #     # accuracy, precision, recall\n",
        "    #     accuracy = num_correct / num_all\n",
        "    #     precision = num_tp / (num_tp + num_fp)\n",
        "    #     recall = num_tp / (num_tp + num_fn)\n",
        "        \n",
        "    #     return accuracy, precision, recall\n",
        "        \n",
        "        \n",
        "    # def inference(self):\n",
        "    #     return self.get_output('softmax') #cls_logits\n",
        "        \n",
        "    \n",
        "    def build_loss(self):\n",
        "        labels = self.get_output('gt_classes')\n",
        "        cls_logits = self.get_output('cls_logits')         \n",
        "        cls_logits = tf.cond(self.use_ghm, lambda: cls_logits*self.get_output('ghm_weights'), \n",
        "                             lambda: cls_logits, name=\"GradientHarmonizingMechanism\")      \n",
        "        \n",
        "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=cls_logits)\n",
        "            \n",
        "        with tf.compat.v1.variable_scope('HardNegativeMining'):\n",
        "            labels = tf.reshape(labels, [-1])  \n",
        "            cross_entropy = tf.reshape(cross_entropy, [-1])\n",
        "            \n",
        "            fg_idx = tf.where(tf.not_equal(labels, 0))\n",
        "            fgs = tf.gather(cross_entropy, fg_idx)\n",
        "            bg_idx = tf.where(tf.equal(labels, 0))\n",
        "            bgs = tf.gather(cross_entropy, bg_idx)\n",
        "             \n",
        "            num = self.hard_negative_ratio * tf.shape(fgs)[0]\n",
        "            num_bg = tf.cond(tf.shape(bgs)[0]<num, lambda:tf.shape(bgs)[0], lambda:num)\n",
        "            sorted_bgs, _ = tf.nn.top_k(tf.transpose(bgs), num_bg, sorted=True)\n",
        "            cross_entropy = fgs + sorted_bgs\n",
        "        \n",
        "        # Total loss\n",
        "        model_loss = tf.reduce_mean(cross_entropy)\n",
        "        regularization_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), name='regularization')\n",
        "        total_loss = model_loss + regularization_loss\n",
        "        \n",
        "        tf.summary.scalar('model_loss', model_loss)\n",
        "        tf.summary.scalar('regularization_loss', regularization_loss)\n",
        "        tf.summary.scalar('total_loss', total_loss)\n",
        "        \n",
        "        logits = self.get_output('cls_logits')\n",
        "        softmax_logits = self.get_output('softmax') #cls_logits\n",
        "        return model_loss, regularization_loss, total_loss, logits, softmax_logits \n",
        "    \n",
        "    # def build_multi_loss(self):\n",
        "    #     labels = self.get_output('gt_classes')\n",
        "    #     cls_logits = self.get_output('cls_logits')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrtT9uFkpt0S"
      },
      "source": [
        "class CUTIERes(CUTIE):\n",
        "    \"\"\" Set up CUTIE-B model\n",
        "    \"\"\"\n",
        "    def __init__(self, num_vocabs, num_classes, params, trainable=True):\n",
        "        self.name = \"CUTIE_atrousSPP\" # \n",
        "        \n",
        "        self.data_grid = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None, 1], name='data_grid')\n",
        "        self.gt_classes = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None], name='gt_classes') \n",
        "        self.data_image = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, 3], name='data_image') # not used in CUTIEv1\n",
        "        self.ps_1d_indices = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name='ps_1d_indices') # not used in CUTIEv1\n",
        "        \n",
        "        self.use_ghm = tf.equal(1, params.use_ghm) if hasattr(params, 'use_ghm') else tf.equal(1, 0) #params.use_ghm \n",
        "        self.activation = 'sigmoid' if (hasattr(params, 'use_ghm') and params.use_ghm) else 'relu'\n",
        "        self.dropout = params.data_augmentation_dropout if hasattr(params, 'data_augmentation_dropout') else 1\n",
        "        self.ghm_weights = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, num_classes], name='ghm_weights')        \n",
        "        self.layers = dict({'data_grid': self.data_grid, 'gt_classes': self.gt_classes, 'ghm_weights':self.ghm_weights})\n",
        "\n",
        "        self.num_vocabs = num_vocabs\n",
        "        self.num_classes = num_classes     \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.embedding_size = params.embedding_size\n",
        "        self.weight_decay = params.weight_decay if hasattr(params, 'weight_decay') else 0.0\n",
        "        self.hard_negative_ratio = params.hard_negative_ratio if hasattr(params, 'hard_negative_ratio') else 0.0\n",
        "        self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 0\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "        \n",
        "    \n",
        "    def setup(self):        \n",
        "        # Input\n",
        "        (self.feed('data_grid')\n",
        "             .embed(self.num_vocabs, self.embedding_size, name='embedding', dropout=self.dropout))  \n",
        "        \n",
        "        # Encoder\n",
        "        (self.feed('embedding')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_2')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_3')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_4')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 2, name='encoder1_5')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 4, name='encoder1_6')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 8, name='encoder1_7')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 16, name='encoder1_8'))\n",
        "        \n",
        "        # Atrous Spatial Pyramid Pooling module\n",
        "        #(self.feed('encoder1_8')\n",
        "        #     .conv(1, 1, 256, 1, 1, name='aspp_0'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 4, name='aspp_1'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 8, name='aspp_2'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 16, name='aspp_3'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .global_pool(name='aspp_4'))\n",
        "        (self.feed('aspp_1', 'aspp_2', 'aspp_3', 'aspp_4')\n",
        "             .concat(3, name='aspp_concat')\n",
        "             .conv(1, 1, 256, 1, 1, name='aspp_1x1'))\n",
        "        \n",
        "        # Combine low level features\n",
        "        (self.feed('encoder1_1', 'aspp_1x1')\n",
        "             .concat(3, name='concat1')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder1_1'))\n",
        "        \n",
        "        # Classification\n",
        "        (self.feed('decoder1_1') \n",
        "             .conv(1, 1, self.num_classes, 1, 1, activation=self.activation, name='cls_logits') # sigmoid for ghm\n",
        "             .softmax(name='softmax'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5-GaLCEGmTv"
      },
      "source": [
        "### Parse arguments needed for the model\n",
        "parser = argparse.ArgumentParser(description='CUTIE parameters')\n",
        "\n",
        "# Dummy parser arguments for notebook\n",
        "parser.add_argument('-f')\n",
        "\n",
        "# Data\n",
        "parser.add_argument('--use_cutie2', type=bool, default=False) # True to read image from doc_path \n",
        "parser.add_argument('--doc_path', type=str, default='ExpressExpenseJson')\n",
        "parser.add_argument('--save_prefix', type=str, default='ExpressExpense', help='prefix for ckpt') # TBD: save log/models with prefix\n",
        "parser.add_argument('--test_path', type=str, default='ExpressExpenseJsonTest') # leave empty if no test data provided\n",
        "\n",
        "# Checkpoint\n",
        "# parser.add_argument('--restore_ckpt', type=bool, default=False) \n",
        "# parser.add_argument('--restore_bertembedding_only', type=bool, default=False) # effective when restore_ckpt is True\n",
        "# parser.add_argument('--embedding_file', type=str, default='bert/multi_cased_L-12_H-768_A-12/bert_model.ckpt') \n",
        "parser.add_argument('--ckpt_path', type=str, default='checkpoint/')\n",
        "# parser.add_argument('--ckpt_file', type=str, default='CUTIE_atrousSPP_d20000c5(r80c80)_iter_29201.ckpt')  \n",
        "\n",
        "# Dict\n",
        "parser.add_argument('--load_dict', type=bool, default=True, help='True to work based on an existing dict') \n",
        "parser.add_argument('--load_dict_from_path', type=str, default='dict/') # 40000 or 20000TC or table\n",
        "# parser.add_argument('--tokenize', type=bool, default=False) # tokenize input text ### default = True\n",
        "# parser.add_argument('--text_case', type=bool, default=False) # case sensitive ### default = True == case sensitive\n",
        "parser.add_argument('--update_dict', type=bool, default=False)\n",
        "parser.add_argument('--dict_path', type=str, default='dict/ExpressExpense') # not used if load_dict is True\n",
        "\n",
        "# Data manipulation\n",
        "parser.add_argument('--segment_grid', type=bool, default=False) # segment grid into two parts if grid is larger than cols_target\n",
        "parser.add_argument('--rows_segment', type=int, default=72) \n",
        "parser.add_argument('--cols_segment', type=int, default=72) \n",
        "parser.add_argument('--augment_strategy', type=int, default=1) # 1 for increasing grid shape size, 2 for gaussian around target shape\n",
        "# parser.add_argument('--positional_mapping_strategy', type=int, default=1)\n",
        "parser.add_argument('--rows_target', type=int, default=64) \n",
        "parser.add_argument('--cols_target', type=int, default=64) \n",
        "parser.add_argument('--rows_ulimit', type=int, default=80) # used when data augmentation is true\n",
        "parser.add_argument('--cols_ulimit', type=int, default=80) \n",
        "# parser.add_argument('--fill_bbox', type=bool, default=False) # fill bbox with dict_id / label_id\n",
        "\n",
        "parser.add_argument('--data_augmentation_extra', type=bool, default=True) # randomly expand rows/cols\n",
        "parser.add_argument('--data_augmentation_dropout', type=float, default=0.9) ### default=1\n",
        "parser.add_argument('--data_augmentation_extra_rows', type=int, default=16) \n",
        "parser.add_argument('--data_augmentation_extra_cols', type=int, default=16) \n",
        "\n",
        "# Training\n",
        "parser.add_argument('--batch_size', type=int, default=4) # default=32\n",
        "parser.add_argument('--iterations', type=int, default=200) # default=40000\n",
        "parser.add_argument('--lr_decay_step', type=int, default=13000) \n",
        "parser.add_argument('--learning_rate', type=float, default=0.0001)\n",
        "parser.add_argument('--lr_decay_factor', type=float, default=0.1) \n",
        "\n",
        "# Loss optimization\n",
        "parser.add_argument('--hard_negative_ratio', type=int, help='the ratio between negative and positive losses', default=3) \n",
        "parser.add_argument('--use_ghm', type=int, default=0) # 1 to use GHM, 0 to not use\n",
        "parser.add_argument('--ghm_bins', type=int, default=30) # to be tuned\n",
        "parser.add_argument('--ghm_momentum', type=int, default=0) # 0 / 0.75\n",
        "\n",
        "# Log\n",
        "parser.add_argument('--log_path', type=str, default='log/') \n",
        "parser.add_argument('--log_disp_step', type=int, default=10) # default=200\n",
        "parser.add_argument('--log_save_step', type=int, default=10) # default=200\n",
        "parser.add_argument('--validation_step', type=int, default=10) # default=200\n",
        "parser.add_argument('--test_step', type=int, default=20) # default=400\n",
        "parser.add_argument('--ckpt_save_step', type=int, default=50) #default=1000\n",
        "\n",
        "# Model\n",
        "parser.add_argument('--embedding_size', type=int, default=128) # not used for bert embedding which has 768 as default\n",
        "parser.add_argument('--weight_decay', type=float, default=0.0005) \n",
        "parser.add_argument('--eps', type=float, default=1e-6) \n",
        "\n",
        "# Inference\n",
        "parser.add_argument('--c_threshold', type=float, default=0.5) \n",
        "params = parser.parse_args()\n",
        "\n",
        "# Set up variables\n",
        "edges = [float(x)/params.ghm_bins for x in range(params.ghm_bins+1)]\n",
        "edges[-1] += params.eps\n",
        "acc_sum = [0.0 for _ in range(params.ghm_bins)]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0lBjrjCZDMF",
        "outputId": "7d0c894a-c04f-47e0-d5f6-1cb5f2ef631e"
      },
      "source": [
        "# Call DataLoader to generate dictionary for training\n",
        "data_loader = DataLoader(params, update_dict=params.update_dict, load_dictionary=params.load_dict, data_split=0.75)\n",
        "num_words = max(20000, data_loader.num_words)\n",
        "num_classes = data_loader.num_classes\n",
        "# for _ in range(2000):\n",
        "#     a = data_loader.next_batch()\n",
        "#     b = data_loader.fetch_validation_data()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "DATASET: 2 vocabularies, 2 target classes\n",
            "DATASET: 112 for training, 38 for validation\n",
            "DATASET: 50 for test from ExpressExpenseJsonTest \n",
            "\n",
            "Training statistic:  [(56, 1), (36, 1), (32, 2), (29, 1), (28, 2), (26, 2), (25, 3), (24, 3), (23, 6), (22, 2), (21, 4), (20, 9), (19, 9), (18, 8), (17, 9), (16, 8), (15, 8), (14, 11), (13, 7), (12, 3), (11, 4), (10, 4), (9, 8), (8, 9), (7, 19), (6, 29), (5, 23), (4, 14), (3, 7), (2, 8)]\n",
            "\t num:  112\n",
            "\t rows statistic:  [(11, 3), (10, 3), (9, 5), (8, 9), (7, 18), (6, 28), (5, 23), (4, 12), (3, 4), (2, 7)]\n",
            "\t cols statistic:  [(56, 1), (36, 1), (32, 2), (29, 1), (28, 2), (26, 2), (25, 3), (24, 3), (23, 6), (22, 2), (21, 4), (20, 9), (19, 9), (18, 8), (17, 9), (16, 8), (15, 8), (14, 11), (13, 7), (12, 3), (11, 1), (10, 1), (9, 3), (7, 1), (6, 1), (4, 2), (3, 3), (2, 1)]\n",
            "\n",
            "Validation statistic:  [(30, 1), (25, 1), (24, 1), (23, 1), (22, 3), (21, 2), (20, 3), (19, 1), (18, 1), (17, 4), (16, 2), (15, 3), (14, 7), (13, 5), (11, 2), (9, 3), (8, 2), (7, 8), (6, 9), (5, 12), (4, 3), (3, 1), (2, 1)]\n",
            "\t num:  38\n",
            "\t rows statistic:  [(9, 3), (8, 2), (7, 8), (6, 9), (5, 12), (4, 3), (2, 1)]\n",
            "\t cols statistic:  [(30, 1), (25, 1), (24, 1), (23, 1), (22, 3), (21, 2), (20, 3), (19, 1), (18, 1), (17, 4), (16, 2), (15, 3), (14, 7), (13, 5), (11, 2), (3, 1)]\n",
            "\n",
            "Test statistic:  [(31, 1), (29, 1), (27, 1), (25, 2), (24, 1), (22, 4), (21, 2), (20, 2), (19, 4), (18, 6), (17, 3), (16, 2), (15, 1), (14, 5), (13, 3), (12, 5), (11, 2), (10, 2), (9, 5), (8, 2), (7, 8), (6, 17), (5, 9), (4, 7), (3, 1), (2, 2), (1, 2)]\n",
            "\t num:  50\n",
            "\t rows statistic:  [(17, 1), (10, 1), (9, 3), (7, 8), (6, 17), (5, 9), (4, 7), (3, 1), (2, 2), (1, 1)]\n",
            "\t cols statistic:  [(31, 1), (29, 1), (27, 1), (25, 2), (24, 1), (22, 4), (21, 2), (20, 2), (19, 4), (18, 6), (17, 2), (16, 2), (15, 1), (14, 5), (13, 3), (12, 5), (11, 2), (10, 1), (9, 2), (8, 2), (1, 1)]\n",
            "\n",
            "DATASHAPE: data set with maximum grid table of (11,56), updated.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkxQOhsPJbKx",
        "outputId": "f3eceff6-f6fa-46bb-8699-c77bfc422554"
      },
      "source": [
        "# Initilize CUTIE-B model\n",
        "network = CUTIERes(num_words, num_classes, params)\n",
        "model_loss, regularization_loss, total_loss, model_logits, model_output = network.build_loss()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"data_grid:0\", shape=(?, ?, ?, 1), dtype=int32)\n",
            "WARNING:tensorflow:From <ipython-input-15-64396451d12c>:48: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"embedding/Reshape_1:0\", shape=(?, ?, ?, 128), dtype=float32)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_2/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_3/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"ResizeNearestNeighbor:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_1x1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"decoder1_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 64), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-16-9e72360fdb89>:114: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJJ9IADvM4a3"
      },
      "source": [
        "# Operators\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.Variable(params.learning_rate, trainable=False)\n",
        "optimizer = tf.train.AdamOptimizer(lr)\n",
        "tvars = tf.trainable_variables()\n",
        "grads = tf.gradients(total_loss, tvars)\n",
        "clipped_grads, norm = tf.clip_by_global_norm(grads, 10.0)\n",
        "train_op = optimizer.apply_gradients(list(zip(clipped_grads, tvars)), global_step=global_step) \n",
        "with tf.control_dependencies([train_op]):\n",
        "    train_dummy = tf.constant(0)\n",
        "\n",
        "tf.contrib.training.add_gradients_summaries(zip(clipped_grads, tvars))\n",
        "summary_op = tf.summary.merge_all()    "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otG3z4soPLWv",
        "outputId": "a7fe9098-d22c-4b8e-b5e1-812b036ce870"
      },
      "source": [
        " # Calculate the number of parameters\n",
        "total_parameters = 0\n",
        "for variable in tf.trainable_variables():\n",
        "    shape = variable.get_shape()\n",
        "    variable_parameters = 1\n",
        "    for dim in shape:\n",
        "        variable_parameters *= dim.value\n",
        "    total_parameters += variable_parameters\n",
        "print(network.name, ': ', total_parameters/1000/1000, 'M parameters \\n')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUTIE_atrousSPP :  13.63885 M parameters \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayp49OUsTf4W"
      },
      "source": [
        "# Helper functions\n",
        "\n",
        "def cal_accuracy(c_threshold, data_loader, grid_table, gt_classes, model_output_val, label_mapids, bbox_mapids):\n",
        "    \"\"\" Calculate accuracy and related indicators\n",
        "    \"\"\"\n",
        "    #num_tp = 0\n",
        "    #num_fn = 0\n",
        "    res = ''\n",
        "    num_correct = 0\n",
        "    num_correct_strict = 0\n",
        "    num_correct_soft = 0\n",
        "    num_all = grid_table.shape[0] * (model_output_val.shape[-1]-1)\n",
        "    for b in range(grid_table.shape[0]):\n",
        "        data_input_flat = grid_table[b,:,:,0].reshape([-1])\n",
        "        labels = gt_classes[b,:,:].reshape([-1])\n",
        "        logits = model_output_val[b,:,:,:].reshape(([-1, data_loader.num_classes]))\n",
        "        label_mapid = label_mapids[b]\n",
        "        bbox_mapid = bbox_mapids[b]\n",
        "        rows, cols = grid_table.shape[1:3]\n",
        "        bbox_id = np.array([row*cols+col for row in range(rows) for col in range(cols)])\n",
        "        \n",
        "        # ignore inputs that are not word\n",
        "        indexes = np.where(data_input_flat != 0)[0]\n",
        "        print(f'len(data_input_flat): {len(data_input_flat)}')\n",
        "        print(f'indexes: {indexes}')\n",
        "        data_selected = data_input_flat[indexes]\n",
        "        labels_selected = labels[indexes]\n",
        "        logits_array_selected = logits[indexes]\n",
        "        bbox_id_selected = bbox_id[indexes]\n",
        "        \n",
        "        # calculate accuracy\n",
        "        #test_classes = [1,2,3,4,5]\n",
        "        #for c in test_classes:\n",
        "        for c in range(1, data_loader.num_classes):\n",
        "            labels_indexes = np.where(labels_selected == c)[0]\n",
        "            logits_indexes = np.where(logits_array_selected[:,c] > c_threshold)[0]\n",
        "            \n",
        "            labels_words = list(data_loader.index_to_word[i] for i in data_selected[labels_indexes])\n",
        "            logits_words = list(data_loader.index_to_word[i] for i in data_selected[logits_indexes])\n",
        "            \n",
        "            label_bbox_ids = label_mapid[c] # GT bbox_ids related to the type of class\n",
        "            logit_bbox_ids = [bbox_mapid[bbox] for bbox in bbox_id_selected[logits_indexes] if bbox in bbox_mapid]            \n",
        "            \n",
        "            #if np.array_equal(labels_indexes, logits_indexes):\n",
        "            if set(label_bbox_ids) == set(logit_bbox_ids): # decide as correct when all ids match\n",
        "                num_correct_strict += 1  \n",
        "                num_correct_soft += 1\n",
        "            elif set(label_bbox_ids).issubset(set(logit_bbox_ids)): # correct when gt is subset of gt\n",
        "                num_correct_soft += 1\n",
        "            try: # calculate prevalence with decimal precision\n",
        "                num_correct += np.shape(np.intersect1d(labels_indexes, logits_indexes))[0] / np.shape(labels_indexes)[0]\n",
        "            except ZeroDivisionError:\n",
        "                if np.shape(labels_indexes)[0] == 0:\n",
        "                    num_correct += 1\n",
        "                else:\n",
        "                    num_correct += 0        \n",
        "            \n",
        "            # show results without the <DontCare> class                    \n",
        "            if b==0:\n",
        "                res += '\\n{}(GT/Inf):\\t\"'.format(data_loader.classes[c])\n",
        "                \n",
        "                # ground truth label\n",
        "                res += ' '.join(data_loader.index_to_word[i] for i in data_selected[labels_indexes])\n",
        "                res += '\" | \"'\n",
        "                res += ' '.join(data_loader.index_to_word[i] for i in data_selected[logits_indexes])\n",
        "                res += '\"'\n",
        "                \n",
        "                # wrong inferences results\n",
        "                if not np.array_equal(labels_indexes, logits_indexes): \n",
        "                    res += '\\n \\t FALSES =>>'\n",
        "                    logits_flat = logits_array_selected[:,c]\n",
        "                    fault_logits_indexes = np.setdiff1d(logits_indexes, labels_indexes)\n",
        "                    for i in range(len(data_selected)):\n",
        "                        if i not in fault_logits_indexes: # only show fault_logits_indexes\n",
        "                            continue\n",
        "                        w = data_loader.index_to_word[data_selected[i]]\n",
        "                        l = data_loader.classes[labels_selected[i]]\n",
        "                        res += ' \"%s\"/%s, '%(w, l)\n",
        "                        #res += ' \"%s\"/%.2f%s, '%(w, logits_flat[i], l)\n",
        "                        \n",
        "                #print(res)\n",
        "    prevalence = num_correct / num_all\n",
        "    accuracy_strict = num_correct_strict / num_all\n",
        "    accuracy_soft = num_correct_soft / num_all\n",
        "    return prevalence, accuracy_strict, accuracy_soft, res.encode(\"utf-8\")\n",
        "\n",
        "\n",
        "def save_ckpt(sess, path, save_prefix, data_loader, network, num_words, num_classes, iter):\n",
        "    \"\"\" Save checkpoint\n",
        "    \"\"\"\n",
        "    # print(f'function save_ckpt is called in iter {iter} for session {sess}')\n",
        "    ckpt_path = join(path, save_prefix)\n",
        "    # print(f'ckpt_path: {ckpt_path}')\n",
        "    if not exists(ckpt_path):\n",
        "        makedirs(ckpt_path)\n",
        "        print(f'ckpt_path: {ckpt_path} is created!')\n",
        "    filename = join(ckpt_path, network.name + '_d{:d}c{:d}(r{:d}c{:d})_iter_{:d}'.\n",
        "                            format(num_words, num_classes, data_loader.rows_ulimit, data_loader.cols_ulimit, iter) + '.ckpt')\n",
        "    # print(f'filename is {filename}')\n",
        "    ckpt_saver.save(sess, filename)\n",
        "    print('\\nCheckpoint saved to: {:s}\\n'.format(filename))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiDkia-nPPz5",
        "outputId": "39de03ea-537f-44e0-8964-ce0a05850a81"
      },
      "source": [
        "# Train the model\n",
        "loss_curve = []\n",
        "training_recall, validation_recall, test_recall = [], [], []\n",
        "training_acc_strict, validation_acc_strict, test_acc_strict = [], [], []\n",
        "training_acc_soft, validation_acc_soft, test_acc_soft = [], [], []\n",
        "\n",
        "ckpt_saver = tf.train.Saver(max_to_keep=200)\n",
        "summary_path = join(params.log_path, params.save_prefix, network.name)\n",
        "summary_writer = tf.summary.FileWriter(summary_path, tf.get_default_graph(), flush_secs=10)\n",
        "\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "config.gpu_options.allow_growth = True\n",
        "with tf.Session(config=config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    iter_start = 0\n",
        "    \n",
        "    # # Restore parameters\n",
        "    # if params.restore_ckpt:\n",
        "    #     if params.restore_bertembedding_only:\n",
        "    #         if 'bert' not in network.name:\n",
        "    #             raise Exception('no bert embedding was designed in the built model, \\\n",
        "    #                 switch restore_bertembedding_only off or built a related model')\n",
        "    #         try:\n",
        "    #             load_variable = {\"bert/embeddings/word_embeddings\": network.embedding_table}\n",
        "    #             ckpt_saver = tf.train.Saver(load_variable, max_to_keep=50)\n",
        "    #             ckpt_path = params.embedding_file\n",
        "    #             ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
        "    #             print('Restoring from {}...'.format(ckpt_path))\n",
        "    #             ckpt_saver.restore(sess, ckpt_path)\n",
        "    #             print('Restored from {}'.format(ckpt_path))\n",
        "    #         except:\n",
        "    #             raise Exception('Check your path {:s}'.format(ckpt_path))\n",
        "    #     else:\n",
        "    #         try:\n",
        "    #             ckpt_path = os.path.join(params.ckpt_path, params.ckpt_file)\n",
        "    #             ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
        "    #             print('Restoring from {}...'.format(ckpt_path))\n",
        "    #             ckpt_saver.restore(sess, ckpt_path)\n",
        "    #             print('Restored from {}'.format(ckpt_path))\n",
        "    #             stem = os.path.splitext(os.path.basename(ckpt_path))[0]\n",
        "    #             #iter_start = int(stem.split('_')[-1]) - 1\n",
        "    #             sess.run(global_step.assign(iter_start))\n",
        "    #         except:\n",
        "    #             raise Exception('Check your pretrained {:s}'.format(ckpt_path))\n",
        "        \n",
        "    # Iterations\n",
        "    print(\"Start Training\")\n",
        "    for iter in range(iter_start, params.iterations+1):\n",
        "        print(f'ITER: {iter}\\n')\n",
        "        timer_start = timeit.default_timer()\n",
        "        \n",
        "        # Learning rate decay\n",
        "        # print(f'iter%params.lr_decay_step = {iter%params.lr_decay_step}')\n",
        "        if iter!=0 and iter%params.lr_decay_step==0:\n",
        "            sess.run(tf.assign(lr, lr.eval()*params.lr_decay_factor))\n",
        "        \n",
        "        # Get data for one batch\n",
        "        # data =  next_batch(training_data_tobe_fetched, training_docs)\n",
        "        data = data_loader.next_batch()\n",
        "        # print(f'data for iter: {iter} = {data}')\n",
        "        feeds = [network.data_grid, network.gt_classes, network.data_image, network.ps_1d_indices, network.ghm_weights]\n",
        "        fetches = [model_loss, regularization_loss, total_loss, summary_op, train_dummy, model_logits, model_output]\n",
        "        h = sess.partial_run_setup(fetches, feeds)\n",
        "        \n",
        "        # One step inference \n",
        "        feed_dict = {\n",
        "            network.data_grid: data['grid_table'],\n",
        "            network.gt_classes: data['gt_classes']\n",
        "        }\n",
        "\n",
        "        # if params.use_cutie2:\n",
        "        #     feed_dict = {\n",
        "        #         network.data_grid: data['grid_table'],\n",
        "        #         network.gt_classes: data['gt_classes'],\n",
        "        #         network.data_image: data['data_image'],\n",
        "        #         network.ps_1d_indices: data['ps_1d_indices']\n",
        "        #     }\n",
        "        fetches = [model_logits, model_output]\n",
        "        (model_logit_val, model_output_val) = sess.partial_run(h, fetches, feed_dict)\n",
        "        \n",
        "        # One step training\n",
        "        ghm_weights = np.ones(np.shape(model_logit_val))\n",
        "        if params.use_ghm:\n",
        "            ghm_weights = calc_ghm_weights(np.array(model_logit_val), np.array(data['gt_classes']))\n",
        "        feed_dict = {\n",
        "            network.ghm_weights: ghm_weights,\n",
        "        }\n",
        "        fetches = [model_loss, regularization_loss, total_loss, summary_op, train_dummy]\n",
        "        (model_loss_val, regularization_loss_val, total_loss_val, summary_str, _) =\\\n",
        "            sess.partial_run(h, fetches=fetches, feed_dict=feed_dict)\n",
        "            \n",
        "        # Calculate training accuracy and display results\n",
        "        if iter%params.log_disp_step == 0: \n",
        "            timer_stop = timeit.default_timer()\n",
        "            print('\\t >>time per step: %.2fs <<'%(timer_stop - timer_start))\n",
        "            \n",
        "            recall, acc_strict, acc_soft, res = cal_accuracy(params.c_threshold, data_loader, np.array(data['grid_table']), \n",
        "                                                    np.array(data['gt_classes']), model_output_val, \n",
        "                                                    np.array(data['label_mapids']), np.array(data['bbox_mapids']))\n",
        "            loss_curve += [total_loss_val]\n",
        "            training_recall += [recall]        \n",
        "            training_acc_strict += [acc_strict]   \n",
        "            training_acc_soft += [acc_soft]       \n",
        "            \n",
        "            #print(res.decode())\n",
        "            print('\\nIter: %d/%d, total loss: %.4f, model loss: %.4f, regularization loss: %.4f'%\\\n",
        "                  (iter, params.iterations, total_loss_val, model_loss_val, regularization_loss_val))\n",
        "            print('LOSS CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.log_disp_step,w) for i,w in enumerate(loss_curve)]))\n",
        "            print('TRAINING ACC CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.log_disp_step,w) for i,w in enumerate(training_acc_strict)]))\n",
        "            print('TRAINING ACC (Recall/Acc): %.3f / %.3f (%.3f) | highest %.3f / %.3f (%.3f)'\\\n",
        "                  %(recall, acc_strict, acc_soft, max(training_recall), max(training_acc_strict), max(training_acc_soft)))\n",
        "            \n",
        "        # Calculate validation accuracy and display results\n",
        "        # print(f'iter%params.validation_step = {iter%params.validation_step}')\n",
        "        # print(f'len(data_loader.validation_docs) = {len(data_loader.validation_docs)}')\n",
        "        if iter%params.validation_step == 0 and len(data_loader.validation_docs):                \n",
        "            recalls, accs_strict, accs_soft = [], [], []\n",
        "            for _ in range(len(data_loader.validation_docs)):\n",
        "                data = data_loader.fetch_validation_data()\n",
        "                grid_tables = data['grid_table']\n",
        "                gt_classes = data['gt_classes']\n",
        "                \n",
        "                feed_dict = {\n",
        "                    network.data_grid: grid_tables,\n",
        "                }\n",
        "                if params.use_cutie2:\n",
        "                    feed_dict = {\n",
        "                        network.data_grid: grid_tables,\n",
        "                        network.data_image: data['data_image'],\n",
        "                        network.ps_1d_indices: data['ps_1d_indices']\n",
        "                    }\n",
        "                fetches = [model_output]                    \n",
        "                [model_output_val] = sess.run(fetches=fetches, feed_dict=feed_dict)  \n",
        "                recall, acc_strict, acc_soft, res = cal_accuracy(params.c_threshold, data_loader, np.array(grid_tables), \n",
        "                                                        np.array(gt_classes), model_output_val,\n",
        "                                                        np.array(data['label_mapids']), np.array(data['bbox_mapids']))\n",
        "                recalls += [recall]\n",
        "                accs_strict += [acc_strict] \n",
        "                accs_soft += [acc_soft]\n",
        "\n",
        "            recall = sum(recalls) / len(recalls)\n",
        "            acc_strict = sum(accs_strict) / len(accs_strict)\n",
        "            acc_soft = sum(accs_soft) / len(accs_soft)\n",
        "            validation_recall += [recall]\n",
        "            validation_acc_strict += [acc_strict]  \n",
        "            validation_acc_soft += [acc_soft]\n",
        "            #print(res.decode()) # show res from the last execution of the while loop \n",
        "\n",
        "            print('VALIDATION ACC (STRICT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.validation_step,w) for i,w in enumerate(validation_acc_strict)]))\n",
        "            print('VALIDATION ACC (SOFT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                              format(i*params.validation_step,w) for i,w in enumerate(validation_acc_soft)]))\n",
        "            print('TRAINING RECALL CURVE: ' + ' >'.join(['{:d}:{:.2f}'.\n",
        "                              format(i*params.log_disp_step,w) for i,w in enumerate(training_recall)]))\n",
        "            print('VALIDATION RECALL CURVE: ' + ' >'.join(['{:d}:{:.2f}'.\n",
        "                              format(i*params.validation_step,w) for i,w in enumerate(validation_recall)]))   \n",
        "            \n",
        "            idx = np.argmax(validation_acc_strict)\n",
        "            print('VALIDATION Statistic %d(%d) (Recall/Acc): %.3f / %.3f (%.3f) | highest %.3f / %.3f (%.3f) \\n'\n",
        "                  %(iter, idx*params.validation_step, recall, acc_strict, acc_soft, \n",
        "                    validation_recall[idx], validation_acc_strict[idx], validation_acc_soft[idx]))       \n",
        "            \n",
        "            # Save best performance checkpoint\n",
        "            # print(f'iter>=params.ckpt_save_step: {iter>=params.ckpt_save_step}')\n",
        "            # print(f'validation_acc_strict[-1] > max(validation_acc_strict[:-1]+[0]): { validation_acc_strict[-1] > max(validation_acc_strict[:-1]+[0])}')\n",
        "            if iter>=params.ckpt_save_step and validation_acc_strict[-1] > max(validation_acc_strict[:-1]+[0]):\n",
        "                # Save as iter+1 to indicate best validation\n",
        "                save_ckpt(sess, params.ckpt_path, params.save_prefix, data_loader, network, num_words, num_classes, iter+1)\n",
        "                print('\\nBest up-to-date performance validation checkpoint saved.\\n')\n",
        "                \n",
        "            #     print(\"saved best performace checkpoint\")\n",
        "            # print(\"DIDNT SAVE BEST PERFORANCE CHECKPOINT\")\n",
        "            \n",
        "        # Calculate test accuracy and display results\n",
        "        if params.test_path!='' and iter%params.test_step == 0 and len(data_loader.test_docs):\n",
        "            \n",
        "            recalls, accs_strict, accs_soft = [], [], []\n",
        "            while True:\n",
        "                data = data_loader.fetch_test_data()\n",
        "                if data == None:\n",
        "                    break\n",
        "                grid_tables = data['grid_table']\n",
        "                gt_classes = data['gt_classes']\n",
        "                \n",
        "                \n",
        "                feed_dict = {\n",
        "                    network.data_grid: grid_tables,\n",
        "                }\n",
        "                if params.use_cutie2:\n",
        "                    feed_dict = {\n",
        "                        network.data_grid: grid_tables,\n",
        "                        network.data_image: data['data_image'],\n",
        "                        network.ps_1d_indices: data['ps_1d_indices']\n",
        "                    }\n",
        "                fetches = [model_output]                    \n",
        "                [model_output_val] = sess.run(fetches=fetches, feed_dict=feed_dict)                    \n",
        "                recall, acc_strict, acc_soft, res = cal_accuracy(params.c_threshold, data_loader, np.array(grid_tables), \n",
        "                                                        np.array(gt_classes), model_output_val,\n",
        "                                                        np.array(data['label_mapids']), np.array(data['bbox_mapids']))\n",
        "                recalls += [recall]\n",
        "                accs_strict += [acc_strict] \n",
        "                accs_soft += [acc_soft]\n",
        "\n",
        "            recall = sum(recalls) / len(recalls)\n",
        "            acc_strict = sum(accs_strict) / len(accs_strict)\n",
        "            acc_soft = sum(accs_soft) / len(accs_soft)\n",
        "            test_recall += [recall]\n",
        "            test_acc_strict += [acc_strict]   \n",
        "            test_acc_soft += [acc_soft]\n",
        "            idx = np.argmax(test_acc_strict)\n",
        "            print('\\n TEST ACC (Recall/Acc): %.3f / %.3f (%.3f) | highest %.3f / %.3f (%.3f) \\n'\n",
        "                  %(recall, acc_strict, acc_soft, test_recall[idx], test_acc_strict[idx], test_acc_soft[idx]))\n",
        "            print('TEST ACC (STRICT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                            format(i*params.test_step,w) for i,w in enumerate(test_acc_strict)]))\n",
        "            print('TEST ACC (SOFT) CURVE: ' + ' >'.join(['{:d}:{:.3f}'.\n",
        "                            format(i*params.test_step,w) for i,w in enumerate(test_acc_soft)]))\n",
        "            print('TEST RECALL CURVE: ' + ' >'.join(['{:d}:{:.2f}'.\n",
        "                            format(i*params.test_step,w) for i,w in enumerate(test_recall)]))            \n",
        "            \n",
        "            # save best performance checkpoint\n",
        "            if iter>=params.ckpt_save_step and test_acc_strict[-1] > max(test_acc_strict[:-1]+[0]):\n",
        "                # save as iter+1 to indicate best test\n",
        "                save_ckpt(sess, params.ckpt_path, params.save_prefix, data_loader, network, num_words, num_classes, iter+2)\n",
        "                print('\\nBest up-to-date performance test checkpoint saved.\\n')\n",
        "                \n",
        "        # Save checkpoints\n",
        "        print(f'iter = {iter}')\n",
        "        print(f'params.log_save_step = {params.log_save_step}')\n",
        "        print(f'params.ckpt_save_step = {params.ckpt_save_step}')\n",
        "        print(f'iter>=params.log_save_step = {iter>=params.log_save_step}')\n",
        "        print(f'iter%params.ckpt_save_step = {iter%params.ckpt_save_step} ')\n",
        "        if iter>=params.log_save_step and iter%params.ckpt_save_step == 0:\n",
        "            save_ckpt(sess, params.ckpt_path, params.save_prefix, data_loader, network, num_words, num_classes, iter)\n",
        "            \n",
        "        #     print(\"save checkpoint 1\")\n",
        "        # print(\"save checkpoint 2\")\n",
        "\n",
        "        # Save logs\n",
        "        print(f'params.og_save_step = {params.log_save_step}')\n",
        "        print(f'iter%params.log_save_step = { iter%params.log_save_step} ')\n",
        "        if iter>=params.log_save_step and iter%params.log_save_step == 0:\n",
        "            summary_writer.add_summary(summary_str, iter+1)    \n",
        "        #     print(\"save logs 1\")\n",
        "        # print(\"save logs 2\")\n",
        "        # print(iter)\n",
        "\n",
        "pprint(params)\n",
        "pprint('Data rows/cols:{},{}'.format(data_loader.rows, data_loader.cols))\n",
        "summary_writer.close()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Training\n",
            "ITER: 0\n",
            "\n",
            "Training grid AUGMENT size: (11,21) from (11,19)\n",
            "\t >>time per step: 11.30s <<\n",
            "len(data_input_flat): 231\n",
            "indexes: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  17  19  22\n",
            "  23  24  26  28  29  30  32  43  44  45  46  47  48  49  50  51  63  64\n",
            "  65  67  68  72  73  75  78  81  82  84  85  86  87  88  89  90  91  92\n",
            "  99 100 101 102 103 104 105 106 107 110 114 116 124 125 137 138 143 144\n",
            " 145 146 153 159 164 166 167 180 195 197 199 202 215 216 217 218 220 221\n",
            " 223 224 225]\n",
            "len(data_input_flat): 231\n",
            "indexes: [  2   4   6   8  10  11  25  26  27  30  31  32  42  43  44  45  46  47\n",
            "  48  52  55  56  57  58  59  63  64  65  66  67  68  69  70  71  74  76\n",
            "  77  78  79  80  84  85  86  87  88  89  90  91  92  99 100 107 108 109\n",
            " 127 129 130 133 136 141 142 146 147 148 149 150 151 162 163 169 182 183\n",
            " 184 193 195 198 203 205 215 217 218 219 220 221 222 223 225]\n",
            "len(data_input_flat): 231\n",
            "indexes: [ 27  28  30  31  32  33  45  46  47  48  49  50  51  52  53  54  55  56\n",
            "  57  58  59  60  66  67  68  69  70  71  72  73  74  77  78  79  85  88\n",
            "  89  90  91  92  93  94 106 107 108 109 110 111 112 113 115 120 127 129\n",
            " 130 131 132 133 134 135 136 137 138 141 148 152 154 155 159 162 163 169\n",
            " 174 175 176 177 178 182 183 184 190 193 201 210]\n",
            "len(data_input_flat): 231\n",
            "indexes: [  8  10  12  13  14  15  16  17  18  24  26  27  28  29  30  31  32  33\n",
            "  34  35  36  37  38  39  40  41  42  48  49  50  51  52  53  54  55  56\n",
            "  57  58  59  60  65  68  69  70  71  72  73  74  75  76  77  84  87  89\n",
            "  90  91  92  93  94  95  96  97  98 106 110 111 112 113 115 117 118 119\n",
            " 120 130 131 132 133 134 135 136 139 151 152 153 154 155 156 159 168 172\n",
            " 174 175 176 177 178 179 180 181 213 214 215 216 217 218 219 220 221 222\n",
            " 223 224 225]\n",
            "\n",
            "Iter: 0/200, total loss: 4.6763, model loss: 4.4620, regularization loss: 0.2143\n",
            "LOSS CURVE: 0:4.676\n",
            "TRAINING ACC CURVE: 0:0.000\n",
            "TRAINING ACC (Recall/Acc): 0.625 / 0.000 (0.250) | highest 0.625 / 0.000 (0.250)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368\n",
            "TRAINING RECALL CURVE: 0:0.62\n",
            "VALIDATION RECALL CURVE: 0:0.50\n",
            "VALIDATION Statistic 0(0) (Recall/Acc): 0.500 / 0.000 (0.368) | highest 0.500 / 0.000 (0.368) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:199: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "ITER: 101\n",
            "\n",
            "Training grid AUGMENT size: (11,42) from (7,28)\n",
            "iter = 101\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 1 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 102\n",
            "\n",
            "Training grid AUGMENT size: (25,33) from (8,19)\n",
            "iter = 102\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 2 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 103\n",
            "\n",
            "Training grid AUGMENT size: (43,37) from (6,21)\n",
            "iter = 103\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 3 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 104\n",
            "\n",
            "Training grid AUGMENT size: (18,30) from (7,21)\n",
            "iter = 104\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 4 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 105\n",
            "\n",
            "Training grid AUGMENT size: (20,31) from (11,21)\n",
            "iter = 105\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 5 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 106\n",
            "\n",
            "Training grid AUGMENT size: (15,39) from (9,28)\n",
            "iter = 106\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 6 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 107\n",
            "\n",
            "Training grid AUGMENT size: (18,35) from (8,25)\n",
            "iter = 107\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 7 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 108\n",
            "\n",
            "Training grid AUGMENT size: (19,22) from (9,20)\n",
            "iter = 108\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 8 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 109\n",
            "\n",
            "Training grid AUGMENT size: (36,54) from (6,32)\n",
            "iter = 109\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 9 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 110\n",
            "\n",
            "Training grid AUGMENT size: (14,36) from (11,36)\n",
            "\t >>time per step: 10.21s <<\n",
            "len(data_input_flat): 504\n",
            "indexes: [147 158 160 161 163 164 168 194 195 196 198 207 208 209 210 230 233 248\n",
            " 265 267 268 270 271 286 287 300 301 302 303 304 305 308 315 336 340 343\n",
            " 356 372 374 377 379 386 387 406 408 411 415 422 458 494 499]\n",
            "len(data_input_flat): 504\n",
            "indexes: [ 16  25  47  49  54  61  88 118 123 130 153 160 168 184 191 199 249 255\n",
            " 258 259 263 264 285 330 332 334 348 349 357 358 366 393 437 465 472 483]\n",
            "len(data_input_flat): 504\n",
            "indexes: [ 12  13  16  17  35  39  50  57  60  74  75  77  78  80  83  90  94  96\n",
            "  97 114 115 116 118 119 150 151 154 155 156 188 194 195 204 227 228 239\n",
            " 240 263 275 299 311 335 347 362 363 365 366 367 369 370 373 374 376 377\n",
            " 378 379 381 382 383 384 385 386 397 405 409 413 433 438 441 445 449 473\n",
            " 474 482 483 490]\n",
            "len(data_input_flat): 504\n",
            "indexes: [  7  15  24  31  50  52  55  60  85  86  89  90  91  92  93  94  95  96\n",
            "  98  99 100 125 126 129 130 142 188 189 190 191 192 193 201 208 211 212\n",
            " 213 224 225 226 227 228 229 230 231 232 233 234 235 236 241 247 251 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 273 283 296 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 309 319 320 321 332 333 335 338 355\n",
            " 356 367 368 369 372 373 375 379 391 392 393 403 427 433 446 448 453 469\n",
            " 470 482 488 495 496]\n",
            "\n",
            "Iter: 110/200, total loss: 1.6142, model loss: 1.3992, regularization loss: 0.2150\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00\n",
            "VALIDATION Statistic 110(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "iter = 110\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 10 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 111\n",
            "\n",
            "Training grid AUGMENT size: (8,23) from (7,20)\n",
            "iter = 111\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 11 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 112\n",
            "\n",
            "Training grid AUGMENT size: (20,33) from (9,28)\n",
            "iter = 112\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 12 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 113\n",
            "\n",
            "Training grid AUGMENT size: (8,30) from (7,24)\n",
            "iter = 113\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 13 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 114\n",
            "\n",
            "Training grid AUGMENT size: (30,80) from (9,56)\n",
            "iter = 114\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 14 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 115\n",
            "\n",
            "Training grid AUGMENT size: (23,24) from (9,21)\n",
            "iter = 115\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 15 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 116\n",
            "\n",
            "Training grid AUGMENT size: (13,21) from (6,18)\n",
            "iter = 116\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 16 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 117\n",
            "\n",
            "Training grid AUGMENT size: (10,29) from (6,18)\n",
            "iter = 117\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 17 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 118\n",
            "\n",
            "Training grid AUGMENT size: (17,40) from (6,28)\n",
            "iter = 118\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 18 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 119\n",
            "\n",
            "Training grid AUGMENT size: (28,35) from (10,21)\n",
            "iter = 119\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 19 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 120\n",
            "\n",
            "Training grid AUGMENT size: (8,35) from (6,23)\n",
            "\t >>time per step: 10.33s <<\n",
            "len(data_input_flat): 280\n",
            "indexes: [ 29  31  48  51  52  53  54  55  56  57  58  71  85  88  90  93 105 114\n",
            " 118 122 125 139 150 151 155 156 181 182 183 185 186 187 189 204 216 218\n",
            " 222 227 251 252]\n",
            "len(data_input_flat): 280\n",
            "indexes: [  8   9  10  11  44  75 110 123 145 158 159 162 163 165 166 167 168 170\n",
            " 180 193 197 200 201 202 204 205 214 220 221 224 225 226 227 228 230 231\n",
            " 233 236 245 261 278]\n",
            "len(data_input_flat): 280\n",
            "indexes: [  2   3   4  12  14  15  16  17  18  19  20  21  22  23  32  39  40  41\n",
            "  43  44  45  46  57  58  59  60  61  63  64  65  66  67  68  69  75  76\n",
            "  77  79  95  98  99 100 102 103 104 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 130 131 137 138 145 146 164 165 166 172 179 180\n",
            " 181 182 184 185 188 189 199 200 206 234 258 259 264 268 269]\n",
            "len(data_input_flat): 280\n",
            "indexes: [  7   9  10  16  17  23  24  43  49  50  54  57  60  71  74  78  82  86\n",
            "  87  97  99 101 113 131 141 142 143 144 149 150 155 169 171 172 176 179\n",
            " 184 186 206 214 218 237 240 243 248 250 276 277]\n",
            "\n",
            "Iter: 120/200, total loss: 1.6142, model loss: 1.3993, regularization loss: 0.2149\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000\n",
            "TRAINING ACC (Recall/Acc): 0.875 / 0.000 (0.750) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00\n",
            "VALIDATION Statistic 120(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 101  357  368  413  426  501  527  723  820  871  925  933  939 1009\n",
            " 1042 1110 1116 1249 1362 1438 1554 1562 1572 1619 1628 1659 1700 1712\n",
            " 1742 1754 1764 1851 1881 1892 1946 1952 2026 2066 2078 2087 2128 2138\n",
            " 2215 2234 2258 2265 2275 2313 2318 2347 2372 2388 2405 2423 2439 2521\n",
            " 2534 2543 2585 2615 2667 2833 2892 2935 3063 3086 3098 3183 3267 3275\n",
            " 3348 3356 3367 3399 3445 3475 3483 3489 3522 3568 3602 3616 3619 3621\n",
            " 3653 3683 3693 3707 3727 3739 3810 3821 3987 4071]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   31   43  149  158  167  544  553  681  693  709  724  820  837\n",
            "  874  918 1028 1309 1427 1438 1465 1483 1592 1612 1617 1628 1810 1818\n",
            " 1894 1913 1932 2060 2105 2213 2233 2251 2258 2267 2379 2389 2400 2425\n",
            " 2553 2571 2579 2592 2699 2706 2721 2745 2890 2903 2938 3018 3090 3131\n",
            " 3210 3217 3225 3232 3240 3258 3608 3637 3741 3830 3997 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   88   98  150  221  228  275  284  290  296  344  353  436  465\n",
            "  493  519  630  646  652  658  688  710  752  781  899  903  909  916\n",
            "  948 1033 1041 1091 1099 1108 1141 1154 1163 1205 1282 1288 1333 1346\n",
            " 1352 1357 1397 1426 1434 1474 1480 1487 1525 1609 1617 1671 1782 1794\n",
            " 1796 1801 1811 1860 1865 1874 1975 1985 1992 2001 2049 2054 2060 2066\n",
            " 2103 2180 2190 2237 2370 2378 2422 2498 2503 2551 2626 2630 2680 2886\n",
            " 2892 2935 3145 3678 3926 3939 3956 4055 4065]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   32   86  147  160  212  285  339  347  403  407  412  537  711\n",
            "  716  721  759  841  900  918  937  942  987 1030 1036 1060 1066 1094\n",
            " 1214 1221 1224 1229 1234 1321 1348 1352 1356 1361 1365 1449 1476 1481\n",
            " 1487 1577 1604 1610 1619 1705 1732 1736 1742 1748 1834 1859 1864 1961\n",
            " 2115 2123 2131 2214 3534 3541 3561 3587 3610 3616 3732 3739 3745 3753\n",
            " 3980 4057]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  75   92  138  145  194  267  274  322  330  476  583  598  612  651\n",
            "  662  796  842 1111 1117 1175 1182 1265 1312 1329 1376 1634 1713 1824\n",
            " 1841 1888 1907 2016 2032 2080 2208 2301 2783 2975 3057 3431 3606 3679\n",
            " 3726 3734 3751 3868 3872 3919 3926 3943 3990 3999 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  15  142  151  157  171  277  285  294  344  353  360  411  484  601\n",
            "  658  672  681  856  870  920  931  937 1006 1049 1060 1067 1209 1219\n",
            " 1398 1403 1412 1419 1431 1589 1593 1606 1737 1745 1753 1782 1787 1849\n",
            " 1852 1860 1867 1873 1878 2040 2051 2242 2251 2314 2324 2335 2341 2372\n",
            " 2525 2536 2544 2576 2627 2637 2646 2650 2658 2669 2677 2683 2722 2777\n",
            " 2830 2842 2850 2856 2868 2877 2884 2929 2935 2941 2947 2957 2967 2974\n",
            " 2983 3034 3045 3053 3061 3067 3076 3084 3090 3145 3156 3165 3176 3187\n",
            " 3194 3279 3288 3293 3302 3312 3355 3361 3366 3375 3384 3390 3394 3399\n",
            " 3410 3474 3484 3496 3590 3603 3617 3627 3638 3702 3715 3724 3730 3737\n",
            " 3750 3774 3910 3919 3923 3932 3943 3952 3963 4063]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  95  323  364  375  380  571  580  827 1033 1074 1083 1227 1266 1276\n",
            " 1299 1482 1487 1492 1521 1531 1739 1748 1752 1778 1787 1930 2003 2033\n",
            " 2044 2181 2300 2436 2454 2509 2556 2690 2806 2811 3098 3111 3396 3586\n",
            " 3592 3599 3605 3780 3793 3976]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  56  650  734  948 1358 1617 1927 2044 2340 2349 2438 2651 3158 3993\n",
            " 4008]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  35  139  150  164  175  180  295  345  464  470  479  489  598  606\n",
            "  615  726  734  743  846  852  859  873  931  978 1046 1097 1193 1199\n",
            " 1208 1219 1232 1317 1348 1357 1366 1456 1467 1479 1491 1605 1636 1657\n",
            " 1779 1786 1796 1810 1913 2051 2059 2070 2090 2098 2107 2233 2360 2377\n",
            " 2489 2502 2512 2522 2618 2628 2637 2647 2745 2758 2932 2954 3078 3088\n",
            " 3097 3192 3215 3238 3270 3398 3409 3514 3588 3728 3905 3909 3917 3923\n",
            " 4039]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24  408  418 2755 2763 2776 2792 2804 2812 3192 3199 3528 3705 4060]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76  540  553  592  672  681  724  786  796  804  811  919  931 1149\n",
            " 1157 1169 1204 1246 1253 1260 1274 1303 1348 1354 1360 1398 1404 1476\n",
            " 1483 1525 1533 1594 1603 1609 1616 1624 1630 1641 1650 1723 1729 1736\n",
            " 1793 1800 1812 1822 1851 1921 1927 1937 1978 2052 2058 2069 2182 2188\n",
            " 2194 2370 2381 2427 2499 2511 2555 2627 2632 2685 2885 2938 3219 3225\n",
            " 3235 3239 3244 3351 3365 3374 3612 3620 3920]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22  476  480  797  804 1019 1104 1112 1162 1168 1174 1275 1281 1293\n",
            " 1339 1457 1507 1531 1697 1705 1724 1824 1834 1907 1915 2142 2218 2527\n",
            " 2607 2835 2842 2849 2858 2963 3034 3041 3050 3154 3161 3167 3178 3281\n",
            " 3289 3359 3369 3790 3800 3807 3882 3919 3994 4001 4011 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 221  481  490  528  550  604  659  679  720  728  907  912  954  966\n",
            " 1003 1020 1027 1038 1048 1052 1056 1077 1113 1158 1217 1246 1481 1496\n",
            " 1525 1612 1653 1667 1732 1738 1782 1820 1860 1863 1869 1875 1910 1988\n",
            " 1995 2039 2116 2122 2167 2197 2244 2249 2255 2296 2323 2372 2379 2424\n",
            " 2456 2462 2500 2504 2511 2552 2628 2636 2708 2744 2756 2762 2844 2857\n",
            " 2869 2884 2894 2938 3013 3021 3130 3141 3149 3385 3406 3530 3579 3655\n",
            " 3666 3701 4041 4047 4053 4069 4086 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  102  278  287  296  407  480  487  596  612  718  800  815 1031\n",
            " 1044 1052 1058 1222 1230 1237 1248 1253 1263 1354 1372 1672 1679 1692\n",
            " 1701 1705 1729 1810 1821 1862 1867 1986 1992 2113 2120 2305 2311 2320\n",
            " 2497 2504 2625 2631 2978 2987 3006 3169 3196 3318 3332 3342 3356 3655\n",
            " 3670 3848 3855 3870 3976 3998 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  351  407  541  667  720  788  790  794  853  860  920  978  982\n",
            "  989 1114 1176 1183 1294 1297 1302 1369 1376 1422 1493 1496 1622 1676\n",
            " 1740 1750 1869 1873 1878 1941 2007 2010 2255 2271 2765 3023 3281 3291\n",
            " 3294 3327 3340 3422 3453 3468 3472 3475 3478 3483 3519 3599 3709 3727\n",
            " 3917 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29   33   95  108  415  425  574  702 1113 1123 1152 1314 1321 1693\n",
            " 1699 1705 1880 1883 1903 2006 2073 2095 2266 2591 2651 2721 3097 3102\n",
            " 3117 3737 3743 3746 3752 3991 3998 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 146  166  205  250  346  364  788  800  810  819  918  926  934  941\n",
            " 1052 1060 1065 1181 1190 1421 1432 1440 1448 1454 1547 1554 1564 1571\n",
            " 1576 1586 1677 1688 1927 1932 1939 1949 1958 1978 2055 2059 2067 2078\n",
            " 2106 2183 2187 2235 2491 2533 2540 2673 2684 2789 2796 2812 2952 2961\n",
            " 2974 2983 3000 3074 3275 3289 3297 3304 3316 3346 3465 3471 3481 3648\n",
            " 3666 3679 3794 3840 3968 3986]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  347 1040 1822 2038 2184 2193 2441 2680 2755 2756 2762 2770 2873\n",
            " 3128 3143 3386 3401 3461 3961 3971]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  102  219  234  344  353  362  474  483  490  609  711  755  765\n",
            "  899  910  934  947  954 1157 1162 1167 1211 1285 1290 1297 1304 1311\n",
            " 1339 1488 1496 1529 1616 1622 1657 1745 1756 1785 1871 1878 1914 2004\n",
            " 2011 2042 2063 2335 2364 2459 2492 2716 2748 2907 2940 3037 3068 3358\n",
            " 3363 3474 3489 3546 3564 3741 3751 3926 4004 4015]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 158  479  733  862  934 1159 1284 1524 2371 2381 2626 2632 2941 2953\n",
            " 3033 3066 3069 3324 3635 3645 3679 3934 3945 4064 4077]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25   28   31  157  216  221  224  228  349  528  533  552  596  616\n",
            "  619  636  655  658  746  787  823  828  857  879  900  968 1013 1018\n",
            " 1093 1195 1205 1213 1231 1234 1299 1303 1323 1388 1410 1413 1430 1432\n",
            " 1435 1438 1470 1478 1492 1495 1559 1561 1564 1578 1678 1683 1687 1706\n",
            " 1935 1937 1940 1943 1945 1950 1958 2062 2065 2069 2074 2127 2136 2143\n",
            " 2147 2254 2256 2261 2265 2269 2273 2276 2298 2320 2324 2330 2333 2353\n",
            " 2390 2400 2402 2446 2451 2459 2468 2488 2574 2589 2638 2643 2648 2652\n",
            " 2659 2695 2696 2752 2779 2820 2830 2833 2836 2839 2843 2947 2949 2952\n",
            " 3059 3067 3090 3105 3184 3252 3320 3379 3407 3435 3563 3569 3724 3755\n",
            " 3762 3850 3889 3989 3994 4008 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  175  333  343  351  357  365  377  475  482  489  600  604  608\n",
            "  618  708  721  732  776  849  904  974  987  994 1000 1011 1018 1104\n",
            " 1115 1127 1133 1139 1144 1226 1239 1353 1365 1478 1511 1613 1683 1691\n",
            " 1721 1854 1934 1941 1951 1977 2061 2067 2075 2106 2187 2197 2209 2235\n",
            " 2381 2386 2399 2429 3345 3842 3849 3853 3854 3858 4039 4051 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29  104  217  230  234  348  491  716  812  818  830  887  995 1034\n",
            " 1048 1080 1150 1161 1270 1278 1415 1423 1432 1531 1543 1548 1619 1659\n",
            " 1671 1680 1787 1915 2043 2183 2190 2299 2311 2427 2631 2810 3091 3098\n",
            " 3107 3177 3285 3297 3370 3482 3490 3852]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84   90  104  277  279  407  599  723  731  759  836  905  916  926\n",
            "  954 1028 1034 1043 1082 1166 1179 1210 1281 1506 1514 1526 1633 1640\n",
            " 1657 1763 1770 1785 1892 1977 2025 2089 2142 2224 2244 2252 2321 2370\n",
            " 2385 2658 2679 3168 3214 3222 3241 3336 3343 3355 3367 3377 3534 3540\n",
            " 3547 3555 3564 3651 3662 3670 3674 3690 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  348  356  364  599  605  611  618  638  728  737  745  860  868\n",
            " 1041 1099 1109 1119 1125 1130 1165 1227 1232 1233 1236 1292 1409 1493\n",
            " 1674 1684 1803 2128 2135 2217 2271 2275 2278 2282 2385 2409 2511 2534\n",
            " 2538 2665 2728 2837 2889 2894 3150 3158 3273 3280 3286 3358 3363 3371\n",
            " 3676 3681 3939 3992 4009]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39   82   90  206  212  220  228  236  343  355  363  644  696  773\n",
            "  899  952  958 1083 1092 1100 1221 1229 1233 1532 1541 1797 1803 1852\n",
            " 1922 1980 2243 2300 2562 2620 2696 2745 2959 2973 2992 3031 3047 3154\n",
            " 3159 3166 3177 3277 3285 3292 3300 3311 3406 3412 3419 3433 3601 3610\n",
            " 3618 3626 3726 3737 3754 4050 4058 4068 4076]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 106  153  204  209  300  355  393  407  604  887  995 1034 1041 1052\n",
            " 1192 1330 1337 1859 2617 2743 3066 3573 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   33   85  104  152  156  225  231  434  467  524  567  626  651\n",
            "  657  780  785  946 1271 1290 1297 1464 1480 1485 1591 1674 1682 1802\n",
            " 1812 1848 2233 2250 2426 2438 2875 2886 3320 3340 3357 3791 3942]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  227  275  283  418  424  473  595  606  613  623  973  979 1030\n",
            " 1046 1321 1331 1338 1348 1377 1407 1419 1604 1784 1797 1806 1976 1989\n",
            " 1994 2002 2169 2181 2190 2553 2584 2637 2809 2825 2994 3001 3078 3090\n",
            " 3496 3511 3537 3545 3686 3696 3703 3730 3741 3893 3930 3941 3985]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  60  189  224  282  286  293  344  409  413  417  421  509  540  546\n",
            "  671  674  747  751  766  786  791  794  808  858  876  879  894  915\n",
            "  919 1004 1008 1022 1056 1214 1234 1407 1425 1430 1454 1553 1557 1562\n",
            " 1681 1685 1743 1747 1874 1878 1881 2000 2004 2011 2031 2080 2128 2132\n",
            " 2159 2257 2262 2267 2287 2384 2392 2416 2510 2513 2518 2638 2640 2643\n",
            " 2768 2774 2896 2901 2927 3022 3026 3032 3152 3184 3278 3281 3285 3290\n",
            " 3537 3632 3727 3731 3735 3740 3760 3855 3951 4056]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   34   60  104  317  457  463  473  546  553  675  701  856  909\n",
            " 1165 1264 1392 1420 1448 1710 1769 1949 1954 2141 2154 2157 2214 2220\n",
            " 2424 2443 2448 2465 2470 2474 2533 2552 2590 2617 2658 2663 2692 2700\n",
            " 2744 2755 2762 2767 2785 2980 3001 3066 3105 3194 3207 3214 3234 3241\n",
            " 3266 3450 3490 3496 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  73   76  143  498  563  673  682  730  737  755  794  871  929  947\n",
            "  986 1053 1059 1113 1147 1282 1339 1410 1412 2432 2470 2600 2665 2792\n",
            " 2907 2920 2971 2978 2984 3099 3106 3115 3170 3177 3291 3306 3355 3363\n",
            " 3483 3491 3497 3547 3626 3636 3674 3676 3683 3740 3751 3764 3811 3819\n",
            " 3892 3931 4003 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 117  124  132  142  153  167  285  295  328  340  451  460  470  613\n",
            "  644  654  667  771  786 1095 1110 1116 1221 1229 1236 1242 1415 1423\n",
            " 1431 1532 1555 1572 1578 1588 1988 1994 2004 2040 2127 2137 2469 2488\n",
            " 2723 2744 2845 2857 2916 2939 3036 3044 3049 3065 3174 3192 3289 3299\n",
            " 3304 3321 3410 3418 3423 3430 3437 3445 3534 3541 3548 3553 3562 3571\n",
            " 3984 4024]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 220  296  303  398  492  548  599  648  690  735  840  889  921  935\n",
            "  944 1030 1040 1268 1286 1467 1657 1670 1689 1709 1798 1850 1926 1978\n",
            " 1996 2008 2063 2130 2161 2310 2362 2490 2500 2508 2632 2643 2682 2869\n",
            " 2876 3002 3012 3336 3350 3383 3542 3566 3591 3729 3740 3747 3753 3761\n",
            " 3852 3858 3865 3890 3936 3942 3948 4052 4062 4068 4075 4083]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   4   10   16   61   90   98  103  108  117  199  275  283  292  299\n",
            "  390  462  471  479  486  497  649  668  836  848  865 1218 1233 1272\n",
            " 1410 1418 1433 1464 1602 1611 1656 1794 1800 1810 1820 1848 1986 1995\n",
            " 2005 2040 2190 2230 2234 2242 2246 2390 2425 2435 2442 2618 2627 2635\n",
            " 2776 2787 2810 2819 2827 3008 3173 3181 3192 3361 3385 3557 3565 3577\n",
            " 3735 3747 3764 3782 3968 4062 4064 4067 4070 4074 4081 4086 4088 4089\n",
            " 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89   97  229  276  284  471  482  821  837  848 1012 1019 1027 1209\n",
            " 1221 1228 1555 1594 1604 1612 1750 1786 1794 1801 1807 1943 1978 1992\n",
            " 2130 2170 2182 2492 2502 2552 2690 2747 3131 3139 3481 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  845  929 1212 1335 1341 1549 1558 1686 2437 3972]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  57   65  125  189  344  350  354  359  409  412  416  486  538  544\n",
            "  548  746  753  834  874  877  882  937  944  948 1024 1076 1136 1170\n",
            " 1184 1191 1204 1290 1297 1354 1395 1425 1470 1482 1523 1612 1651 1661\n",
            " 1688 1715 1739 1746 1842 1846 1870 1880 1970 1974 1982 1994 2005 2046\n",
            " 2100 2118 2120 2122 2124 2127 2129 2131 2133 2137 2142 2146 2149 2151\n",
            " 2154 2250 2292 2296 2339 2375 2383 2423 2500 2503 2506 2508 2511 2515\n",
            " 2518 2522 2526 2528 2530 2533 2535 2537 2539 2544 2550 2695 2741 2746\n",
            " 3142 3152 3161 3193 3550 3564 3688 3946 3951]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  151  294  301  351  395  409  466  482  541  604 1051 1057 1206\n",
            " 1251 1283 1295 1306 1549 1699 1849 1862 1868 1873 1883 2071 2082 2090\n",
            " 2105 2125 2361 2374 2381 2387 2393 2403 2682 2694 2703 2715 2724 3049\n",
            " 3065 3240 3259 3496 3641 3753 3872 3920 3946 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  103  283  294  306  478  490  664  674  680  687 1031 1042 1050\n",
            " 1413 1422 1438 1447 1627 1708 2067 2083 2247 2255 2361 2503 2510 2522\n",
            " 2553 2695 2702 2715 2745 3160 3198 3340 3588 3596 3608 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  16   32   75  151  202  219  225  280  285  328  412  476  540  787\n",
            "  796  806  855  860  865  915  921  927  933  989 1042 1049 1052 1055\n",
            " 1060 1063 1070 1092 1099 1173 1186 1194 1307 1314 1368 1417 1481 1493\n",
            " 1554 1590 1602 1610 1653 1666 1673 1683 1717 1866 1874 1882 1910 1974\n",
            " 2103 2122 2132 2230 2361 2377 2385 2441 2451 2488 2616 2630 2746 2754\n",
            " 2883 2936 3013 3023 3032 3065 3146 3160 3324 3330 3444 3514 3523 3932\n",
            " 3974 4055 4063 4072]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 149  164  534  542  548  553  733  856  866  872 1046 1059 1276 1332\n",
            " 1351 1496 1540 1547 1666 1671 1978 1988 1996 2003 2006 2012 2123 2130\n",
            " 2139 2170 2184 2298 2316 2491 2810 3195 3323 4004]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [2080]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  32 2948 2957 3279 3314 3598 3616 3626 3640 3938 3975 3990 4009 4013\n",
            " 4029]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81   90  204  211  220  337  349  457  466  540  547  663  913 1168\n",
            " 1176 1196 1224 1350 1355 1362 1387 1516 1542 1549 1644 1676 1684 1734\n",
            " 1837 1862 1867 1873 1966 1998 2044 2054 2172 2284 2685 3044 3054 3150\n",
            " 3240 3338 3495 3592 3932]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24   29   83   88  155  159  163  216  220  224  280  285  462  467\n",
            "  487  525  529  615  618  653  657  681  844  909  914  918  921  924\n",
            "  928  999 1102 1110 1130 1194 1227 1232 1258 1366 1386 1433 1451 1560\n",
            " 1578 1802 1805 1869 1876 1885 1894 1898 1900 1994 2027 2122 2126 2134\n",
            " 2155 2252 2283 2390 2410 2475 2520 2584 2602 2817 2956 2986 3051 3082\n",
            " 3255 3306 3338 3408 3448 3449 3518 3545 3560 3577 3594 3765 3804 3810\n",
            " 3814 3864 3893 3938 3992 3998 4002 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  40   82   92  167  218  297  341  352  451  461  564  680  700  712\n",
            "  984 1002 1097 1248 1306 1316 1331 1337 1357 1438 1489 1545 1553 1588\n",
            " 1677 1690 1700 1715 1721 1809 1823 1886 1893 1907 1913 1931 1941 1952\n",
            " 2035 2041 2064 2079 2131 2163 2169 2258 2291 2297 2382 2395 2419 2425\n",
            " 2513 2546 2553 2612 2617 2639 2650 2659 2722 2740 2746 2767 2778 2842\n",
            " 2868 2873 2895 2958 2968 2984 2999 3337 3346 3364 3471 3493 3595 3605\n",
            " 3618 3725 3739 3756 3853 3866 3884 3980 3994 4012]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  265  324  338  345  416  451  462  472  579  592  786  836  901\n",
            " 1155 1414 1458 3979 3995 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 160  172  463  471  716  784  793  911  918  924  932 1165 1236 1260\n",
            " 1267 1295 1311 1432 1487 1739 1746 1754 1825 1833 2180 2427 2520 2531\n",
            " 2638 2683 2777 2853 2904 2914 2953 2959 3003 3009 3278 3330 3387 3394\n",
            " 3581 3595 3604 3613 3906 3922 3978]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 165  204  295  303  347  352  400  487  530  540  629  674  682  686\n",
            "  723  775  809  857  859  913 1017 1286 1291 1338 1346 1432 1480 1604\n",
            " 1611 1722 1736 1850 1927 1942 2043 2117 2235 2249 2427 2434 2442 2448\n",
            " 2683 2693 2701 2951 3275 3286 3294 3330 3480 3529 3615 3623 3656 3671\n",
            " 3990 3997 4007]\n",
            "\n",
            " TEST ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.630 / 0.000 (0.500) \n",
            "\n",
            "TEST ACC (STRICT) CURVE: 0:0.000 >20:0.000 >40:0.000 >60:0.000 >80:0.000 >100:0.000 >120:0.000\n",
            "TEST ACC (SOFT) CURVE: 0:0.500 >20:0.860 >40:0.900 >60:1.000 >80:1.000 >100:1.000 >120:1.000\n",
            "TEST RECALL CURVE: 0:0.63 >20:0.92 >40:0.94 >60:1.00 >80:1.00 >100:1.00 >120:1.00\n",
            "iter = 120\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 20 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 121\n",
            "\n",
            "Training grid AUGMENT size: (26,20) from (11,19)\n",
            "iter = 121\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 21 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 122\n",
            "\n",
            "Training grid AUGMENT size: (37,43) from (8,25)\n",
            "iter = 122\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 22 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 123\n",
            "\n",
            "Training grid AUGMENT size: (8,18) from (6,17)\n",
            "iter = 123\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 23 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 124\n",
            "\n",
            "Training grid AUGMENT size: (23,42) from (11,25)\n",
            "iter = 124\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 24 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 125\n",
            "\n",
            "Training grid AUGMENT size: (21,39) from (10,20)\n",
            "iter = 125\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 25 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 126\n",
            "\n",
            "Training grid AUGMENT size: (16,47) from (10,26)\n",
            "iter = 126\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 26 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 127\n",
            "\n",
            "Training grid AUGMENT size: (48,23) from (7,23)\n",
            "iter = 127\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 27 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 128\n",
            "\n",
            "Training grid AUGMENT size: (38,31) from (5,23)\n",
            "iter = 128\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 28 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 129\n",
            "\n",
            "Training grid AUGMENT size: (10,40) from (8,36)\n",
            "iter = 129\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 29 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 130\n",
            "\n",
            "Training grid AUGMENT size: (12,33) from (7,32)\n",
            "\t >>time per step: 9.91s <<\n",
            "len(data_input_flat): 396\n",
            "indexes: [ 10  11  13  14  15  17  18  19  20  22  23  24  27  37  42  46  51  60\n",
            "  63  70  71  73  74  76  80  81  84  86  93 101 102 104 108 109 113 117\n",
            " 120 128 129 134 135 138 139 162 163 167 194 201 237 241 243 245 246 249\n",
            " 250 251 252 254 255 256 260 265 266 267 271 272 278 279 298 304 311 321\n",
            " 328 333 338 342 347 371]\n",
            "len(data_input_flat): 396\n",
            "indexes: [  4  27  30 108 110 112 114 116 118 121 142 145 146 149 150 152 168 169\n",
            " 170 171 172 173 174 175 176 177 179 182 184 190 191 192 194 195 196 200\n",
            " 201 203 204 214 218 221 223 225 226 227 228 229 233 234 235 237 259 266\n",
            " 268 291 292 293 300 307 324 326 343 347 348 352 368]\n",
            "len(data_input_flat): 396\n",
            "indexes: [  2   6   9  12  34  35  37  50  51  57  62  67  71  73  77  82  95 108\n",
            " 116 137 139 142 143 170 172 174 199 202 211 227 235 260 267 269 293 301\n",
            " 302 306 307 312 322 327 332 336 341 345 353 365 385]\n",
            "len(data_input_flat): 396\n",
            "indexes: [ 78  80  81  83  84  85 107 110 111 112 113 114 115 116 117 118 119 120\n",
            " 121 122 139 140 142 143 152 154 155 170 171 172 174 175 176 177 179 180\n",
            " 183 184 185 187 188 189 190 203 204 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 221 222 223 247 248 249 255 256 257 278 279 281 284 286 287\n",
            " 310 313 341 343 345 348]\n",
            "\n",
            "Iter: 130/200, total loss: 1.6068, model loss: 1.3920, regularization loss: 0.2147\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00\n",
            "VALIDATION Statistic 130(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "iter = 130\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 30 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 131\n",
            "\n",
            "Training grid AUGMENT size: (19,43) from (11,26)\n",
            "iter = 131\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 31 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 132\n",
            "\n",
            "Training grid AUGMENT size: (8,34) from (8,23)\n",
            "iter = 132\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 32 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 133\n",
            "\n",
            "Training grid AUGMENT size: (19,29) from (9,22)\n",
            "iter = 133\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 33 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 134\n",
            "\n",
            "Training grid AUGMENT size: (24,65) from (7,32)\n",
            "iter = 134\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 34 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 135\n",
            "\n",
            "Training grid AUGMENT size: (23,24) from (7,24)\n",
            "iter = 135\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 35 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 136\n",
            "\n",
            "Training grid AUGMENT size: (24,42) from (8,20)\n",
            "iter = 136\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 36 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 137\n",
            "\n",
            "Training grid AUGMENT size: (24,47) from (8,17)\n",
            "iter = 137\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 37 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 138\n",
            "\n",
            "Training grid AUGMENT size: (31,26) from (6,25)\n",
            "iter = 138\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 38 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 139\n",
            "\n",
            "Training grid AUGMENT size: (9,30) from (6,23)\n",
            "iter = 139\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 39 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 140\n",
            "\n",
            "Training grid AUGMENT size: (7,54) from (6,23)\n",
            "\t >>time per step: 10.67s <<\n",
            "len(data_input_flat): 378\n",
            "indexes: [  4  23  27  33  37  39  59  62  66  75  76  81  83  86 113 114 115 119\n",
            " 120 154 159 165 166 168 171 172 173 174 210 211 219 223 234 265 266 283\n",
            " 290 291 292 311 317 319 320 321 337 350 355 362 371]\n",
            "len(data_input_flat): 378\n",
            "indexes: [  3   4   5  19  21  22  23  25  26  27  28  29  30  31  47  50  51  63\n",
            "  64  65  66  69  70  71  74  91  92  93  94  95  99 100 101 102 103 105\n",
            " 106 107 117 118 120 121 122 125 126 146 147 156 159 160 170 171 173 174\n",
            " 175 176 177 179 180 181 186 190 200 201 202 211 212 213 223 225 226 227\n",
            " 230 232 236 238 254 255 256 278 307 318 344 345 354 359 361]\n",
            "len(data_input_flat): 378\n",
            "indexes: [ 15  19  23  32  43 109 167 322 337 339 342 351 365 366]\n",
            "len(data_input_flat): 378\n",
            "indexes: [ 14  19  22  23  32  33  34  35  36  37  69  71  72  75  79  80  81  86\n",
            "  88  96 100 101 102 103 123 125 127 128 131 132 134 135 136 141 154 155\n",
            " 156 157 158 179 181 186 188 189 190 191 206 207 208 209 235 241 242 245\n",
            " 251 262 285 287 293 299 300 306 326 332 338 340 344 345 348 350 352 354\n",
            " 356 357 358 359 362 363 366]\n",
            "\n",
            "Iter: 140/200, total loss: 1.6098, model loss: 1.3953, regularization loss: 0.2145\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607 >140:1.610\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000\n",
            "TRAINING ACC (Recall/Acc): 0.875 / 0.000 (0.750) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000 >140:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00 >140:0.88\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00 >140:1.00\n",
            "VALIDATION Statistic 140(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 101  357  368  413  426  501  527  723  820  871  925  933  939 1009\n",
            " 1042 1110 1116 1249 1362 1438 1554 1562 1572 1619 1628 1659 1700 1712\n",
            " 1742 1754 1764 1851 1881 1892 1946 1952 2026 2066 2078 2087 2128 2138\n",
            " 2215 2234 2258 2265 2275 2313 2318 2347 2372 2388 2405 2423 2439 2521\n",
            " 2534 2543 2585 2615 2667 2833 2892 2935 3063 3086 3098 3183 3267 3275\n",
            " 3348 3356 3367 3399 3445 3475 3483 3489 3522 3568 3602 3616 3619 3621\n",
            " 3653 3683 3693 3707 3727 3739 3810 3821 3987 4071]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   31   43  149  158  167  544  553  681  693  709  724  820  837\n",
            "  874  918 1028 1309 1427 1438 1465 1483 1592 1612 1617 1628 1810 1818\n",
            " 1894 1913 1932 2060 2105 2213 2233 2251 2258 2267 2379 2389 2400 2425\n",
            " 2553 2571 2579 2592 2699 2706 2721 2745 2890 2903 2938 3018 3090 3131\n",
            " 3210 3217 3225 3232 3240 3258 3608 3637 3741 3830 3997 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   88   98  150  221  228  275  284  290  296  344  353  436  465\n",
            "  493  519  630  646  652  658  688  710  752  781  899  903  909  916\n",
            "  948 1033 1041 1091 1099 1108 1141 1154 1163 1205 1282 1288 1333 1346\n",
            " 1352 1357 1397 1426 1434 1474 1480 1487 1525 1609 1617 1671 1782 1794\n",
            " 1796 1801 1811 1860 1865 1874 1975 1985 1992 2001 2049 2054 2060 2066\n",
            " 2103 2180 2190 2237 2370 2378 2422 2498 2503 2551 2626 2630 2680 2886\n",
            " 2892 2935 3145 3678 3926 3939 3956 4055 4065]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   32   86  147  160  212  285  339  347  403  407  412  537  711\n",
            "  716  721  759  841  900  918  937  942  987 1030 1036 1060 1066 1094\n",
            " 1214 1221 1224 1229 1234 1321 1348 1352 1356 1361 1365 1449 1476 1481\n",
            " 1487 1577 1604 1610 1619 1705 1732 1736 1742 1748 1834 1859 1864 1961\n",
            " 2115 2123 2131 2214 3534 3541 3561 3587 3610 3616 3732 3739 3745 3753\n",
            " 3980 4057]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  75   92  138  145  194  267  274  322  330  476  583  598  612  651\n",
            "  662  796  842 1111 1117 1175 1182 1265 1312 1329 1376 1634 1713 1824\n",
            " 1841 1888 1907 2016 2032 2080 2208 2301 2783 2975 3057 3431 3606 3679\n",
            " 3726 3734 3751 3868 3872 3919 3926 3943 3990 3999 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  15  142  151  157  171  277  285  294  344  353  360  411  484  601\n",
            "  658  672  681  856  870  920  931  937 1006 1049 1060 1067 1209 1219\n",
            " 1398 1403 1412 1419 1431 1589 1593 1606 1737 1745 1753 1782 1787 1849\n",
            " 1852 1860 1867 1873 1878 2040 2051 2242 2251 2314 2324 2335 2341 2372\n",
            " 2525 2536 2544 2576 2627 2637 2646 2650 2658 2669 2677 2683 2722 2777\n",
            " 2830 2842 2850 2856 2868 2877 2884 2929 2935 2941 2947 2957 2967 2974\n",
            " 2983 3034 3045 3053 3061 3067 3076 3084 3090 3145 3156 3165 3176 3187\n",
            " 3194 3279 3288 3293 3302 3312 3355 3361 3366 3375 3384 3390 3394 3399\n",
            " 3410 3474 3484 3496 3590 3603 3617 3627 3638 3702 3715 3724 3730 3737\n",
            " 3750 3774 3910 3919 3923 3932 3943 3952 3963 4063]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  95  323  364  375  380  571  580  827 1033 1074 1083 1227 1266 1276\n",
            " 1299 1482 1487 1492 1521 1531 1739 1748 1752 1778 1787 1930 2003 2033\n",
            " 2044 2181 2300 2436 2454 2509 2556 2690 2806 2811 3098 3111 3396 3586\n",
            " 3592 3599 3605 3780 3793 3976]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  56  650  734  948 1358 1617 1927 2044 2340 2349 2438 2651 3158 3993\n",
            " 4008]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  35  139  150  164  175  180  295  345  464  470  479  489  598  606\n",
            "  615  726  734  743  846  852  859  873  931  978 1046 1097 1193 1199\n",
            " 1208 1219 1232 1317 1348 1357 1366 1456 1467 1479 1491 1605 1636 1657\n",
            " 1779 1786 1796 1810 1913 2051 2059 2070 2090 2098 2107 2233 2360 2377\n",
            " 2489 2502 2512 2522 2618 2628 2637 2647 2745 2758 2932 2954 3078 3088\n",
            " 3097 3192 3215 3238 3270 3398 3409 3514 3588 3728 3905 3909 3917 3923\n",
            " 4039]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24  408  418 2755 2763 2776 2792 2804 2812 3192 3199 3528 3705 4060]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76  540  553  592  672  681  724  786  796  804  811  919  931 1149\n",
            " 1157 1169 1204 1246 1253 1260 1274 1303 1348 1354 1360 1398 1404 1476\n",
            " 1483 1525 1533 1594 1603 1609 1616 1624 1630 1641 1650 1723 1729 1736\n",
            " 1793 1800 1812 1822 1851 1921 1927 1937 1978 2052 2058 2069 2182 2188\n",
            " 2194 2370 2381 2427 2499 2511 2555 2627 2632 2685 2885 2938 3219 3225\n",
            " 3235 3239 3244 3351 3365 3374 3612 3620 3920]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22  476  480  797  804 1019 1104 1112 1162 1168 1174 1275 1281 1293\n",
            " 1339 1457 1507 1531 1697 1705 1724 1824 1834 1907 1915 2142 2218 2527\n",
            " 2607 2835 2842 2849 2858 2963 3034 3041 3050 3154 3161 3167 3178 3281\n",
            " 3289 3359 3369 3790 3800 3807 3882 3919 3994 4001 4011 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 221  481  490  528  550  604  659  679  720  728  907  912  954  966\n",
            " 1003 1020 1027 1038 1048 1052 1056 1077 1113 1158 1217 1246 1481 1496\n",
            " 1525 1612 1653 1667 1732 1738 1782 1820 1860 1863 1869 1875 1910 1988\n",
            " 1995 2039 2116 2122 2167 2197 2244 2249 2255 2296 2323 2372 2379 2424\n",
            " 2456 2462 2500 2504 2511 2552 2628 2636 2708 2744 2756 2762 2844 2857\n",
            " 2869 2884 2894 2938 3013 3021 3130 3141 3149 3385 3406 3530 3579 3655\n",
            " 3666 3701 4041 4047 4053 4069 4086 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  102  278  287  296  407  480  487  596  612  718  800  815 1031\n",
            " 1044 1052 1058 1222 1230 1237 1248 1253 1263 1354 1372 1672 1679 1692\n",
            " 1701 1705 1729 1810 1821 1862 1867 1986 1992 2113 2120 2305 2311 2320\n",
            " 2497 2504 2625 2631 2978 2987 3006 3169 3196 3318 3332 3342 3356 3655\n",
            " 3670 3848 3855 3870 3976 3998 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  351  407  541  667  720  788  790  794  853  860  920  978  982\n",
            "  989 1114 1176 1183 1294 1297 1302 1369 1376 1422 1493 1496 1622 1676\n",
            " 1740 1750 1869 1873 1878 1941 2007 2010 2255 2271 2765 3023 3281 3291\n",
            " 3294 3327 3340 3422 3453 3468 3472 3475 3478 3483 3519 3599 3709 3727\n",
            " 3917 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29   33   95  108  415  425  574  702 1113 1123 1152 1314 1321 1693\n",
            " 1699 1705 1880 1883 1903 2006 2073 2095 2266 2591 2651 2721 3097 3102\n",
            " 3117 3737 3743 3746 3752 3991 3998 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 146  166  205  250  346  364  788  800  810  819  918  926  934  941\n",
            " 1052 1060 1065 1181 1190 1421 1432 1440 1448 1454 1547 1554 1564 1571\n",
            " 1576 1586 1677 1688 1927 1932 1939 1949 1958 1978 2055 2059 2067 2078\n",
            " 2106 2183 2187 2235 2491 2533 2540 2673 2684 2789 2796 2812 2952 2961\n",
            " 2974 2983 3000 3074 3275 3289 3297 3304 3316 3346 3465 3471 3481 3648\n",
            " 3666 3679 3794 3840 3968 3986]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  347 1040 1822 2038 2184 2193 2441 2680 2755 2756 2762 2770 2873\n",
            " 3128 3143 3386 3401 3461 3961 3971]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  102  219  234  344  353  362  474  483  490  609  711  755  765\n",
            "  899  910  934  947  954 1157 1162 1167 1211 1285 1290 1297 1304 1311\n",
            " 1339 1488 1496 1529 1616 1622 1657 1745 1756 1785 1871 1878 1914 2004\n",
            " 2011 2042 2063 2335 2364 2459 2492 2716 2748 2907 2940 3037 3068 3358\n",
            " 3363 3474 3489 3546 3564 3741 3751 3926 4004 4015]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 158  479  733  862  934 1159 1284 1524 2371 2381 2626 2632 2941 2953\n",
            " 3033 3066 3069 3324 3635 3645 3679 3934 3945 4064 4077]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25   28   31  157  216  221  224  228  349  528  533  552  596  616\n",
            "  619  636  655  658  746  787  823  828  857  879  900  968 1013 1018\n",
            " 1093 1195 1205 1213 1231 1234 1299 1303 1323 1388 1410 1413 1430 1432\n",
            " 1435 1438 1470 1478 1492 1495 1559 1561 1564 1578 1678 1683 1687 1706\n",
            " 1935 1937 1940 1943 1945 1950 1958 2062 2065 2069 2074 2127 2136 2143\n",
            " 2147 2254 2256 2261 2265 2269 2273 2276 2298 2320 2324 2330 2333 2353\n",
            " 2390 2400 2402 2446 2451 2459 2468 2488 2574 2589 2638 2643 2648 2652\n",
            " 2659 2695 2696 2752 2779 2820 2830 2833 2836 2839 2843 2947 2949 2952\n",
            " 3059 3067 3090 3105 3184 3252 3320 3379 3407 3435 3563 3569 3724 3755\n",
            " 3762 3850 3889 3989 3994 4008 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  175  333  343  351  357  365  377  475  482  489  600  604  608\n",
            "  618  708  721  732  776  849  904  974  987  994 1000 1011 1018 1104\n",
            " 1115 1127 1133 1139 1144 1226 1239 1353 1365 1478 1511 1613 1683 1691\n",
            " 1721 1854 1934 1941 1951 1977 2061 2067 2075 2106 2187 2197 2209 2235\n",
            " 2381 2386 2399 2429 3345 3842 3849 3853 3854 3858 4039 4051 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29  104  217  230  234  348  491  716  812  818  830  887  995 1034\n",
            " 1048 1080 1150 1161 1270 1278 1415 1423 1432 1531 1543 1548 1619 1659\n",
            " 1671 1680 1787 1915 2043 2183 2190 2299 2311 2427 2631 2810 3091 3098\n",
            " 3107 3177 3285 3297 3370 3482 3490 3852]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84   90  104  277  279  407  599  723  731  759  836  905  916  926\n",
            "  954 1028 1034 1043 1082 1166 1179 1210 1281 1506 1514 1526 1633 1640\n",
            " 1657 1763 1770 1785 1892 1977 2025 2089 2142 2224 2244 2252 2321 2370\n",
            " 2385 2658 2679 3168 3214 3222 3241 3336 3343 3355 3367 3377 3534 3540\n",
            " 3547 3555 3564 3651 3662 3670 3674 3690 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  348  356  364  599  605  611  618  638  728  737  745  860  868\n",
            " 1041 1099 1109 1119 1125 1130 1165 1227 1232 1233 1236 1292 1409 1493\n",
            " 1674 1684 1803 2128 2135 2217 2271 2275 2278 2282 2385 2409 2511 2534\n",
            " 2538 2665 2728 2837 2889 2894 3150 3158 3273 3280 3286 3358 3363 3371\n",
            " 3676 3681 3939 3992 4009]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39   82   90  206  212  220  228  236  343  355  363  644  696  773\n",
            "  899  952  958 1083 1092 1100 1221 1229 1233 1532 1541 1797 1803 1852\n",
            " 1922 1980 2243 2300 2562 2620 2696 2745 2959 2973 2992 3031 3047 3154\n",
            " 3159 3166 3177 3277 3285 3292 3300 3311 3406 3412 3419 3433 3601 3610\n",
            " 3618 3626 3726 3737 3754 4050 4058 4068 4076]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 106  153  204  209  300  355  393  407  604  887  995 1034 1041 1052\n",
            " 1192 1330 1337 1859 2617 2743 3066 3573 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   33   85  104  152  156  225  231  434  467  524  567  626  651\n",
            "  657  780  785  946 1271 1290 1297 1464 1480 1485 1591 1674 1682 1802\n",
            " 1812 1848 2233 2250 2426 2438 2875 2886 3320 3340 3357 3791 3942]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  227  275  283  418  424  473  595  606  613  623  973  979 1030\n",
            " 1046 1321 1331 1338 1348 1377 1407 1419 1604 1784 1797 1806 1976 1989\n",
            " 1994 2002 2169 2181 2190 2553 2584 2637 2809 2825 2994 3001 3078 3090\n",
            " 3496 3511 3537 3545 3686 3696 3703 3730 3741 3893 3930 3941 3985]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  60  189  224  282  286  293  344  409  413  417  421  509  540  546\n",
            "  671  674  747  751  766  786  791  794  808  858  876  879  894  915\n",
            "  919 1004 1008 1022 1056 1214 1234 1407 1425 1430 1454 1553 1557 1562\n",
            " 1681 1685 1743 1747 1874 1878 1881 2000 2004 2011 2031 2080 2128 2132\n",
            " 2159 2257 2262 2267 2287 2384 2392 2416 2510 2513 2518 2638 2640 2643\n",
            " 2768 2774 2896 2901 2927 3022 3026 3032 3152 3184 3278 3281 3285 3290\n",
            " 3537 3632 3727 3731 3735 3740 3760 3855 3951 4056]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   34   60  104  317  457  463  473  546  553  675  701  856  909\n",
            " 1165 1264 1392 1420 1448 1710 1769 1949 1954 2141 2154 2157 2214 2220\n",
            " 2424 2443 2448 2465 2470 2474 2533 2552 2590 2617 2658 2663 2692 2700\n",
            " 2744 2755 2762 2767 2785 2980 3001 3066 3105 3194 3207 3214 3234 3241\n",
            " 3266 3450 3490 3496 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  73   76  143  498  563  673  682  730  737  755  794  871  929  947\n",
            "  986 1053 1059 1113 1147 1282 1339 1410 1412 2432 2470 2600 2665 2792\n",
            " 2907 2920 2971 2978 2984 3099 3106 3115 3170 3177 3291 3306 3355 3363\n",
            " 3483 3491 3497 3547 3626 3636 3674 3676 3683 3740 3751 3764 3811 3819\n",
            " 3892 3931 4003 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 117  124  132  142  153  167  285  295  328  340  451  460  470  613\n",
            "  644  654  667  771  786 1095 1110 1116 1221 1229 1236 1242 1415 1423\n",
            " 1431 1532 1555 1572 1578 1588 1988 1994 2004 2040 2127 2137 2469 2488\n",
            " 2723 2744 2845 2857 2916 2939 3036 3044 3049 3065 3174 3192 3289 3299\n",
            " 3304 3321 3410 3418 3423 3430 3437 3445 3534 3541 3548 3553 3562 3571\n",
            " 3984 4024]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 220  296  303  398  492  548  599  648  690  735  840  889  921  935\n",
            "  944 1030 1040 1268 1286 1467 1657 1670 1689 1709 1798 1850 1926 1978\n",
            " 1996 2008 2063 2130 2161 2310 2362 2490 2500 2508 2632 2643 2682 2869\n",
            " 2876 3002 3012 3336 3350 3383 3542 3566 3591 3729 3740 3747 3753 3761\n",
            " 3852 3858 3865 3890 3936 3942 3948 4052 4062 4068 4075 4083]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   4   10   16   61   90   98  103  108  117  199  275  283  292  299\n",
            "  390  462  471  479  486  497  649  668  836  848  865 1218 1233 1272\n",
            " 1410 1418 1433 1464 1602 1611 1656 1794 1800 1810 1820 1848 1986 1995\n",
            " 2005 2040 2190 2230 2234 2242 2246 2390 2425 2435 2442 2618 2627 2635\n",
            " 2776 2787 2810 2819 2827 3008 3173 3181 3192 3361 3385 3557 3565 3577\n",
            " 3735 3747 3764 3782 3968 4062 4064 4067 4070 4074 4081 4086 4088 4089\n",
            " 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89   97  229  276  284  471  482  821  837  848 1012 1019 1027 1209\n",
            " 1221 1228 1555 1594 1604 1612 1750 1786 1794 1801 1807 1943 1978 1992\n",
            " 2130 2170 2182 2492 2502 2552 2690 2747 3131 3139 3481 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  845  929 1212 1335 1341 1549 1558 1686 2437 3972]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  57   65  125  189  344  350  354  359  409  412  416  486  538  544\n",
            "  548  746  753  834  874  877  882  937  944  948 1024 1076 1136 1170\n",
            " 1184 1191 1204 1290 1297 1354 1395 1425 1470 1482 1523 1612 1651 1661\n",
            " 1688 1715 1739 1746 1842 1846 1870 1880 1970 1974 1982 1994 2005 2046\n",
            " 2100 2118 2120 2122 2124 2127 2129 2131 2133 2137 2142 2146 2149 2151\n",
            " 2154 2250 2292 2296 2339 2375 2383 2423 2500 2503 2506 2508 2511 2515\n",
            " 2518 2522 2526 2528 2530 2533 2535 2537 2539 2544 2550 2695 2741 2746\n",
            " 3142 3152 3161 3193 3550 3564 3688 3946 3951]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  151  294  301  351  395  409  466  482  541  604 1051 1057 1206\n",
            " 1251 1283 1295 1306 1549 1699 1849 1862 1868 1873 1883 2071 2082 2090\n",
            " 2105 2125 2361 2374 2381 2387 2393 2403 2682 2694 2703 2715 2724 3049\n",
            " 3065 3240 3259 3496 3641 3753 3872 3920 3946 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  103  283  294  306  478  490  664  674  680  687 1031 1042 1050\n",
            " 1413 1422 1438 1447 1627 1708 2067 2083 2247 2255 2361 2503 2510 2522\n",
            " 2553 2695 2702 2715 2745 3160 3198 3340 3588 3596 3608 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  16   32   75  151  202  219  225  280  285  328  412  476  540  787\n",
            "  796  806  855  860  865  915  921  927  933  989 1042 1049 1052 1055\n",
            " 1060 1063 1070 1092 1099 1173 1186 1194 1307 1314 1368 1417 1481 1493\n",
            " 1554 1590 1602 1610 1653 1666 1673 1683 1717 1866 1874 1882 1910 1974\n",
            " 2103 2122 2132 2230 2361 2377 2385 2441 2451 2488 2616 2630 2746 2754\n",
            " 2883 2936 3013 3023 3032 3065 3146 3160 3324 3330 3444 3514 3523 3932\n",
            " 3974 4055 4063 4072]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 149  164  534  542  548  553  733  856  866  872 1046 1059 1276 1332\n",
            " 1351 1496 1540 1547 1666 1671 1978 1988 1996 2003 2006 2012 2123 2130\n",
            " 2139 2170 2184 2298 2316 2491 2810 3195 3323 4004]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [2080]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  32 2948 2957 3279 3314 3598 3616 3626 3640 3938 3975 3990 4009 4013\n",
            " 4029]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81   90  204  211  220  337  349  457  466  540  547  663  913 1168\n",
            " 1176 1196 1224 1350 1355 1362 1387 1516 1542 1549 1644 1676 1684 1734\n",
            " 1837 1862 1867 1873 1966 1998 2044 2054 2172 2284 2685 3044 3054 3150\n",
            " 3240 3338 3495 3592 3932]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24   29   83   88  155  159  163  216  220  224  280  285  462  467\n",
            "  487  525  529  615  618  653  657  681  844  909  914  918  921  924\n",
            "  928  999 1102 1110 1130 1194 1227 1232 1258 1366 1386 1433 1451 1560\n",
            " 1578 1802 1805 1869 1876 1885 1894 1898 1900 1994 2027 2122 2126 2134\n",
            " 2155 2252 2283 2390 2410 2475 2520 2584 2602 2817 2956 2986 3051 3082\n",
            " 3255 3306 3338 3408 3448 3449 3518 3545 3560 3577 3594 3765 3804 3810\n",
            " 3814 3864 3893 3938 3992 3998 4002 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  40   82   92  167  218  297  341  352  451  461  564  680  700  712\n",
            "  984 1002 1097 1248 1306 1316 1331 1337 1357 1438 1489 1545 1553 1588\n",
            " 1677 1690 1700 1715 1721 1809 1823 1886 1893 1907 1913 1931 1941 1952\n",
            " 2035 2041 2064 2079 2131 2163 2169 2258 2291 2297 2382 2395 2419 2425\n",
            " 2513 2546 2553 2612 2617 2639 2650 2659 2722 2740 2746 2767 2778 2842\n",
            " 2868 2873 2895 2958 2968 2984 2999 3337 3346 3364 3471 3493 3595 3605\n",
            " 3618 3725 3739 3756 3853 3866 3884 3980 3994 4012]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  265  324  338  345  416  451  462  472  579  592  786  836  901\n",
            " 1155 1414 1458 3979 3995 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 160  172  463  471  716  784  793  911  918  924  932 1165 1236 1260\n",
            " 1267 1295 1311 1432 1487 1739 1746 1754 1825 1833 2180 2427 2520 2531\n",
            " 2638 2683 2777 2853 2904 2914 2953 2959 3003 3009 3278 3330 3387 3394\n",
            " 3581 3595 3604 3613 3906 3922 3978]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 165  204  295  303  347  352  400  487  530  540  629  674  682  686\n",
            "  723  775  809  857  859  913 1017 1286 1291 1338 1346 1432 1480 1604\n",
            " 1611 1722 1736 1850 1927 1942 2043 2117 2235 2249 2427 2434 2442 2448\n",
            " 2683 2693 2701 2951 3275 3286 3294 3330 3480 3529 3615 3623 3656 3671\n",
            " 3990 3997 4007]\n",
            "\n",
            " TEST ACC (Recall/Acc): 0.990 / 0.000 (0.980) | highest 0.630 / 0.000 (0.500) \n",
            "\n",
            "TEST ACC (STRICT) CURVE: 0:0.000 >20:0.000 >40:0.000 >60:0.000 >80:0.000 >100:0.000 >120:0.000 >140:0.000\n",
            "TEST ACC (SOFT) CURVE: 0:0.500 >20:0.860 >40:0.900 >60:1.000 >80:1.000 >100:1.000 >120:1.000 >140:0.980\n",
            "TEST RECALL CURVE: 0:0.63 >20:0.92 >40:0.94 >60:1.00 >80:1.00 >100:1.00 >120:1.00 >140:0.99\n",
            "iter = 140\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 40 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 141\n",
            "\n",
            "Training grid AUGMENT size: (7,23) from (6,21)\n",
            "iter = 141\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 41 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 142\n",
            "\n",
            "Training grid AUGMENT size: (20,24) from (11,20)\n",
            "iter = 142\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 42 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 143\n",
            "\n",
            "Training grid AUGMENT size: (15,43) from (11,17)\n",
            "iter = 143\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 43 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 144\n",
            "\n",
            "Training grid AUGMENT size: (13,37) from (6,21)\n",
            "iter = 144\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 44 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 145\n",
            "\n",
            "Training grid AUGMENT size: (29,38) from (9,21)\n",
            "iter = 145\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 45 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 146\n",
            "\n",
            "Training grid AUGMENT size: (23,33) from (7,29)\n",
            "iter = 146\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 46 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 147\n",
            "\n",
            "Training grid AUGMENT size: (17,25) from (9,21)\n",
            "iter = 147\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 47 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 148\n",
            "\n",
            "Training grid AUGMENT size: (14,49) from (7,20)\n",
            "iter = 148\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 48 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 149\n",
            "\n",
            "Training grid AUGMENT size: (25,29) from (6,23)\n",
            "iter = 149\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 49 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 150\n",
            "\n",
            "Training grid AUGMENT size: (26,38) from (8,22)\n",
            "\t >>time per step: 12.21s <<\n",
            "len(data_input_flat): 988\n",
            "indexes: [ 10  13  22  26  29  46  49  50  52  53  55  63  65  88  91 101 155 158\n",
            " 174 179 183 184 187 188 194 230 232 236 240 262 272 278 308 312 317 338\n",
            " 339 346 350 354 384 388 415 453 461 467 491 534 538 567 573 605 648 654\n",
            " 672 686 690 692 697 710 723 728 735 738 758 762 767 804 811 814 820 838\n",
            " 844 875 883 956 963 967 971 975 982]\n",
            "len(data_input_flat): 988\n",
            "indexes: [ 20 328 606 683 761 793 876]\n",
            "len(data_input_flat): 988\n",
            "indexes: [ 13  20  52  87  99 123 124 128 131 138 144 155 157 159 166 170 171 241\n",
            " 245 249 306 311 322 327 363 382 388 404 407 409 411 414 418 421 425 438\n",
            " 487 491 506 510 517 520 538 574 576 579 603 613 618 624 641 651 655 659\n",
            " 663 667 679 704 727 731 735 756 766 771 775 793 816 820 824 841 845 849\n",
            " 878 882 930 985]\n",
            "len(data_input_flat): 988\n",
            "indexes: [242 244 245 246 248 280 283 286 317 319 322 325 355 356 359 360 363 391\n",
            " 396 398 434 436 471 473 501 504 513 516 519 521 539 576 579 584 597 613\n",
            " 616 617 619 635 651 658 673 702 711 730 735 739 742 749 780 787 817 825\n",
            " 847 850 853 857]\n",
            "\n",
            "Iter: 150/200, total loss: 1.6046, model loss: 1.3903, regularization loss: 0.2143\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607 >140:1.610 >150:1.605\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000 >140:1.000 >150:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00 >140:0.88 >150:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00 >140:1.00 >150:1.00\n",
            "VALIDATION Statistic 150(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "iter = 150\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 0 \n",
            "\n",
            "Checkpoint saved to: checkpoint/ExpressExpense/CUTIE_atrousSPP_d20000c2(r80c80)_iter_150.ckpt\n",
            "\n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 151\n",
            "\n",
            "Training grid AUGMENT size: (20,38) from (11,20)\n",
            "iter = 151\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 1 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 152\n",
            "\n",
            "Training grid AUGMENT size: (13,37) from (7,24)\n",
            "iter = 152\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 2 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 153\n",
            "\n",
            "Training grid AUGMENT size: (26,46) from (8,25)\n",
            "iter = 153\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 3 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 154\n",
            "\n",
            "Training grid AUGMENT size: (33,28) from (8,25)\n",
            "iter = 154\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 4 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 155\n",
            "\n",
            "Training grid AUGMENT size: (17,36) from (7,32)\n",
            "iter = 155\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 5 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 156\n",
            "\n",
            "Training grid AUGMENT size: (19,37) from (8,20)\n",
            "iter = 156\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 6 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 157\n",
            "\n",
            "Training grid AUGMENT size: (23,39) from (7,20)\n",
            "iter = 157\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 7 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 158\n",
            "\n",
            "Training grid AUGMENT size: (31,46) from (10,32)\n",
            "iter = 158\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 8 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 159\n",
            "\n",
            "Training grid AUGMENT size: (17,44) from (5,28)\n",
            "iter = 159\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 9 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 160\n",
            "\n",
            "Training grid AUGMENT size: (18,52) from (8,26)\n",
            "\t >>time per step: 9.19s <<\n",
            "len(data_input_flat): 936\n",
            "indexes: [ 73  80  86 126 127 128 132 133 134 137 138 168 175 197 202 219 224 228\n",
            " 236 250 254 255 281 283 288 305 322 325 326 327 329 331 332 333 337 374\n",
            " 377 378 381 382 385 389 410 426 430 432 436 439 462 479 486 494 514 515\n",
            " 532 533 535 539 567 585 591 618 636 643 665 671 690 723 748 750 755 757\n",
            " 760 763 766 767 771 800 809 818 823 860 865 919]\n",
            "len(data_input_flat): 936\n",
            "indexes: [ 36  50  74 114 175 180 181 186 187 195 196 202 227 233 239 250 260 261\n",
            " 270 271 276 279 282 283 287 292 305 307 313 314 322 334 338 368 375 380\n",
            " 382 385 387 392 393 411 420 423 424 427 428 431 434 438 439 474 483 486\n",
            " 490 491 496 535 538 541 543 545 567 583 591 595 638 641 644 646 649 652\n",
            " 671 690 693 697 698 742 745 750 758 805 813 823 827 829 858 875 878 909\n",
            " 910 930]\n",
            "len(data_input_flat): 936\n",
            "indexes: [ 16  19  22  23  31  68  75  79  84 118 121 126 128 133 139 214 218 241\n",
            " 267 273 300 305 319 325 333 343 435 436 445 455 477 479 481 484 489 508\n",
            " 533 542 560 582 591 612 634 635 640 641 647 664 665 687 695 716 792 821\n",
            " 840 874 890 903 928]\n",
            "len(data_input_flat): 936\n",
            "indexes: [ 14 122 128 176 200 209 231 234 236 237 239 240 270 272 274 275 276 277\n",
            " 278 281 282 285 286 287 290 293 323 327 330 333 334 338 342 373 377 381\n",
            " 384 385 388 392 396 443 445 447 450 451 452 475 490 495 497 501 502 503\n",
            " 527 530 536 538 540 544 545 555 579 580 583 593 598 605 607 631 633 659\n",
            " 683 688 693 710 712 798 805 839 840 843 846 858 859 868 869 883 899 910\n",
            " 911 920 921]\n",
            "\n",
            "Iter: 160/200, total loss: 1.6059, model loss: 1.3918, regularization loss: 0.2141\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607 >140:1.610 >150:1.605 >160:1.606\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000 >140:1.000 >150:1.000 >160:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00 >140:0.88 >150:1.00 >160:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00 >140:1.00 >150:1.00 >160:1.00\n",
            "VALIDATION Statistic 160(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 101  357  368  413  426  501  527  723  820  871  925  933  939 1009\n",
            " 1042 1110 1116 1249 1362 1438 1554 1562 1572 1619 1628 1659 1700 1712\n",
            " 1742 1754 1764 1851 1881 1892 1946 1952 2026 2066 2078 2087 2128 2138\n",
            " 2215 2234 2258 2265 2275 2313 2318 2347 2372 2388 2405 2423 2439 2521\n",
            " 2534 2543 2585 2615 2667 2833 2892 2935 3063 3086 3098 3183 3267 3275\n",
            " 3348 3356 3367 3399 3445 3475 3483 3489 3522 3568 3602 3616 3619 3621\n",
            " 3653 3683 3693 3707 3727 3739 3810 3821 3987 4071]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   31   43  149  158  167  544  553  681  693  709  724  820  837\n",
            "  874  918 1028 1309 1427 1438 1465 1483 1592 1612 1617 1628 1810 1818\n",
            " 1894 1913 1932 2060 2105 2213 2233 2251 2258 2267 2379 2389 2400 2425\n",
            " 2553 2571 2579 2592 2699 2706 2721 2745 2890 2903 2938 3018 3090 3131\n",
            " 3210 3217 3225 3232 3240 3258 3608 3637 3741 3830 3997 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   88   98  150  221  228  275  284  290  296  344  353  436  465\n",
            "  493  519  630  646  652  658  688  710  752  781  899  903  909  916\n",
            "  948 1033 1041 1091 1099 1108 1141 1154 1163 1205 1282 1288 1333 1346\n",
            " 1352 1357 1397 1426 1434 1474 1480 1487 1525 1609 1617 1671 1782 1794\n",
            " 1796 1801 1811 1860 1865 1874 1975 1985 1992 2001 2049 2054 2060 2066\n",
            " 2103 2180 2190 2237 2370 2378 2422 2498 2503 2551 2626 2630 2680 2886\n",
            " 2892 2935 3145 3678 3926 3939 3956 4055 4065]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   32   86  147  160  212  285  339  347  403  407  412  537  711\n",
            "  716  721  759  841  900  918  937  942  987 1030 1036 1060 1066 1094\n",
            " 1214 1221 1224 1229 1234 1321 1348 1352 1356 1361 1365 1449 1476 1481\n",
            " 1487 1577 1604 1610 1619 1705 1732 1736 1742 1748 1834 1859 1864 1961\n",
            " 2115 2123 2131 2214 3534 3541 3561 3587 3610 3616 3732 3739 3745 3753\n",
            " 3980 4057]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  75   92  138  145  194  267  274  322  330  476  583  598  612  651\n",
            "  662  796  842 1111 1117 1175 1182 1265 1312 1329 1376 1634 1713 1824\n",
            " 1841 1888 1907 2016 2032 2080 2208 2301 2783 2975 3057 3431 3606 3679\n",
            " 3726 3734 3751 3868 3872 3919 3926 3943 3990 3999 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  15  142  151  157  171  277  285  294  344  353  360  411  484  601\n",
            "  658  672  681  856  870  920  931  937 1006 1049 1060 1067 1209 1219\n",
            " 1398 1403 1412 1419 1431 1589 1593 1606 1737 1745 1753 1782 1787 1849\n",
            " 1852 1860 1867 1873 1878 2040 2051 2242 2251 2314 2324 2335 2341 2372\n",
            " 2525 2536 2544 2576 2627 2637 2646 2650 2658 2669 2677 2683 2722 2777\n",
            " 2830 2842 2850 2856 2868 2877 2884 2929 2935 2941 2947 2957 2967 2974\n",
            " 2983 3034 3045 3053 3061 3067 3076 3084 3090 3145 3156 3165 3176 3187\n",
            " 3194 3279 3288 3293 3302 3312 3355 3361 3366 3375 3384 3390 3394 3399\n",
            " 3410 3474 3484 3496 3590 3603 3617 3627 3638 3702 3715 3724 3730 3737\n",
            " 3750 3774 3910 3919 3923 3932 3943 3952 3963 4063]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  95  323  364  375  380  571  580  827 1033 1074 1083 1227 1266 1276\n",
            " 1299 1482 1487 1492 1521 1531 1739 1748 1752 1778 1787 1930 2003 2033\n",
            " 2044 2181 2300 2436 2454 2509 2556 2690 2806 2811 3098 3111 3396 3586\n",
            " 3592 3599 3605 3780 3793 3976]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  56  650  734  948 1358 1617 1927 2044 2340 2349 2438 2651 3158 3993\n",
            " 4008]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  35  139  150  164  175  180  295  345  464  470  479  489  598  606\n",
            "  615  726  734  743  846  852  859  873  931  978 1046 1097 1193 1199\n",
            " 1208 1219 1232 1317 1348 1357 1366 1456 1467 1479 1491 1605 1636 1657\n",
            " 1779 1786 1796 1810 1913 2051 2059 2070 2090 2098 2107 2233 2360 2377\n",
            " 2489 2502 2512 2522 2618 2628 2637 2647 2745 2758 2932 2954 3078 3088\n",
            " 3097 3192 3215 3238 3270 3398 3409 3514 3588 3728 3905 3909 3917 3923\n",
            " 4039]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24  408  418 2755 2763 2776 2792 2804 2812 3192 3199 3528 3705 4060]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76  540  553  592  672  681  724  786  796  804  811  919  931 1149\n",
            " 1157 1169 1204 1246 1253 1260 1274 1303 1348 1354 1360 1398 1404 1476\n",
            " 1483 1525 1533 1594 1603 1609 1616 1624 1630 1641 1650 1723 1729 1736\n",
            " 1793 1800 1812 1822 1851 1921 1927 1937 1978 2052 2058 2069 2182 2188\n",
            " 2194 2370 2381 2427 2499 2511 2555 2627 2632 2685 2885 2938 3219 3225\n",
            " 3235 3239 3244 3351 3365 3374 3612 3620 3920]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22  476  480  797  804 1019 1104 1112 1162 1168 1174 1275 1281 1293\n",
            " 1339 1457 1507 1531 1697 1705 1724 1824 1834 1907 1915 2142 2218 2527\n",
            " 2607 2835 2842 2849 2858 2963 3034 3041 3050 3154 3161 3167 3178 3281\n",
            " 3289 3359 3369 3790 3800 3807 3882 3919 3994 4001 4011 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 221  481  490  528  550  604  659  679  720  728  907  912  954  966\n",
            " 1003 1020 1027 1038 1048 1052 1056 1077 1113 1158 1217 1246 1481 1496\n",
            " 1525 1612 1653 1667 1732 1738 1782 1820 1860 1863 1869 1875 1910 1988\n",
            " 1995 2039 2116 2122 2167 2197 2244 2249 2255 2296 2323 2372 2379 2424\n",
            " 2456 2462 2500 2504 2511 2552 2628 2636 2708 2744 2756 2762 2844 2857\n",
            " 2869 2884 2894 2938 3013 3021 3130 3141 3149 3385 3406 3530 3579 3655\n",
            " 3666 3701 4041 4047 4053 4069 4086 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  102  278  287  296  407  480  487  596  612  718  800  815 1031\n",
            " 1044 1052 1058 1222 1230 1237 1248 1253 1263 1354 1372 1672 1679 1692\n",
            " 1701 1705 1729 1810 1821 1862 1867 1986 1992 2113 2120 2305 2311 2320\n",
            " 2497 2504 2625 2631 2978 2987 3006 3169 3196 3318 3332 3342 3356 3655\n",
            " 3670 3848 3855 3870 3976 3998 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  351  407  541  667  720  788  790  794  853  860  920  978  982\n",
            "  989 1114 1176 1183 1294 1297 1302 1369 1376 1422 1493 1496 1622 1676\n",
            " 1740 1750 1869 1873 1878 1941 2007 2010 2255 2271 2765 3023 3281 3291\n",
            " 3294 3327 3340 3422 3453 3468 3472 3475 3478 3483 3519 3599 3709 3727\n",
            " 3917 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29   33   95  108  415  425  574  702 1113 1123 1152 1314 1321 1693\n",
            " 1699 1705 1880 1883 1903 2006 2073 2095 2266 2591 2651 2721 3097 3102\n",
            " 3117 3737 3743 3746 3752 3991 3998 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 146  166  205  250  346  364  788  800  810  819  918  926  934  941\n",
            " 1052 1060 1065 1181 1190 1421 1432 1440 1448 1454 1547 1554 1564 1571\n",
            " 1576 1586 1677 1688 1927 1932 1939 1949 1958 1978 2055 2059 2067 2078\n",
            " 2106 2183 2187 2235 2491 2533 2540 2673 2684 2789 2796 2812 2952 2961\n",
            " 2974 2983 3000 3074 3275 3289 3297 3304 3316 3346 3465 3471 3481 3648\n",
            " 3666 3679 3794 3840 3968 3986]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  347 1040 1822 2038 2184 2193 2441 2680 2755 2756 2762 2770 2873\n",
            " 3128 3143 3386 3401 3461 3961 3971]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  102  219  234  344  353  362  474  483  490  609  711  755  765\n",
            "  899  910  934  947  954 1157 1162 1167 1211 1285 1290 1297 1304 1311\n",
            " 1339 1488 1496 1529 1616 1622 1657 1745 1756 1785 1871 1878 1914 2004\n",
            " 2011 2042 2063 2335 2364 2459 2492 2716 2748 2907 2940 3037 3068 3358\n",
            " 3363 3474 3489 3546 3564 3741 3751 3926 4004 4015]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 158  479  733  862  934 1159 1284 1524 2371 2381 2626 2632 2941 2953\n",
            " 3033 3066 3069 3324 3635 3645 3679 3934 3945 4064 4077]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25   28   31  157  216  221  224  228  349  528  533  552  596  616\n",
            "  619  636  655  658  746  787  823  828  857  879  900  968 1013 1018\n",
            " 1093 1195 1205 1213 1231 1234 1299 1303 1323 1388 1410 1413 1430 1432\n",
            " 1435 1438 1470 1478 1492 1495 1559 1561 1564 1578 1678 1683 1687 1706\n",
            " 1935 1937 1940 1943 1945 1950 1958 2062 2065 2069 2074 2127 2136 2143\n",
            " 2147 2254 2256 2261 2265 2269 2273 2276 2298 2320 2324 2330 2333 2353\n",
            " 2390 2400 2402 2446 2451 2459 2468 2488 2574 2589 2638 2643 2648 2652\n",
            " 2659 2695 2696 2752 2779 2820 2830 2833 2836 2839 2843 2947 2949 2952\n",
            " 3059 3067 3090 3105 3184 3252 3320 3379 3407 3435 3563 3569 3724 3755\n",
            " 3762 3850 3889 3989 3994 4008 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  175  333  343  351  357  365  377  475  482  489  600  604  608\n",
            "  618  708  721  732  776  849  904  974  987  994 1000 1011 1018 1104\n",
            " 1115 1127 1133 1139 1144 1226 1239 1353 1365 1478 1511 1613 1683 1691\n",
            " 1721 1854 1934 1941 1951 1977 2061 2067 2075 2106 2187 2197 2209 2235\n",
            " 2381 2386 2399 2429 3345 3842 3849 3853 3854 3858 4039 4051 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29  104  217  230  234  348  491  716  812  818  830  887  995 1034\n",
            " 1048 1080 1150 1161 1270 1278 1415 1423 1432 1531 1543 1548 1619 1659\n",
            " 1671 1680 1787 1915 2043 2183 2190 2299 2311 2427 2631 2810 3091 3098\n",
            " 3107 3177 3285 3297 3370 3482 3490 3852]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84   90  104  277  279  407  599  723  731  759  836  905  916  926\n",
            "  954 1028 1034 1043 1082 1166 1179 1210 1281 1506 1514 1526 1633 1640\n",
            " 1657 1763 1770 1785 1892 1977 2025 2089 2142 2224 2244 2252 2321 2370\n",
            " 2385 2658 2679 3168 3214 3222 3241 3336 3343 3355 3367 3377 3534 3540\n",
            " 3547 3555 3564 3651 3662 3670 3674 3690 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  348  356  364  599  605  611  618  638  728  737  745  860  868\n",
            " 1041 1099 1109 1119 1125 1130 1165 1227 1232 1233 1236 1292 1409 1493\n",
            " 1674 1684 1803 2128 2135 2217 2271 2275 2278 2282 2385 2409 2511 2534\n",
            " 2538 2665 2728 2837 2889 2894 3150 3158 3273 3280 3286 3358 3363 3371\n",
            " 3676 3681 3939 3992 4009]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39   82   90  206  212  220  228  236  343  355  363  644  696  773\n",
            "  899  952  958 1083 1092 1100 1221 1229 1233 1532 1541 1797 1803 1852\n",
            " 1922 1980 2243 2300 2562 2620 2696 2745 2959 2973 2992 3031 3047 3154\n",
            " 3159 3166 3177 3277 3285 3292 3300 3311 3406 3412 3419 3433 3601 3610\n",
            " 3618 3626 3726 3737 3754 4050 4058 4068 4076]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 106  153  204  209  300  355  393  407  604  887  995 1034 1041 1052\n",
            " 1192 1330 1337 1859 2617 2743 3066 3573 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   33   85  104  152  156  225  231  434  467  524  567  626  651\n",
            "  657  780  785  946 1271 1290 1297 1464 1480 1485 1591 1674 1682 1802\n",
            " 1812 1848 2233 2250 2426 2438 2875 2886 3320 3340 3357 3791 3942]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  227  275  283  418  424  473  595  606  613  623  973  979 1030\n",
            " 1046 1321 1331 1338 1348 1377 1407 1419 1604 1784 1797 1806 1976 1989\n",
            " 1994 2002 2169 2181 2190 2553 2584 2637 2809 2825 2994 3001 3078 3090\n",
            " 3496 3511 3537 3545 3686 3696 3703 3730 3741 3893 3930 3941 3985]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  60  189  224  282  286  293  344  409  413  417  421  509  540  546\n",
            "  671  674  747  751  766  786  791  794  808  858  876  879  894  915\n",
            "  919 1004 1008 1022 1056 1214 1234 1407 1425 1430 1454 1553 1557 1562\n",
            " 1681 1685 1743 1747 1874 1878 1881 2000 2004 2011 2031 2080 2128 2132\n",
            " 2159 2257 2262 2267 2287 2384 2392 2416 2510 2513 2518 2638 2640 2643\n",
            " 2768 2774 2896 2901 2927 3022 3026 3032 3152 3184 3278 3281 3285 3290\n",
            " 3537 3632 3727 3731 3735 3740 3760 3855 3951 4056]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   34   60  104  317  457  463  473  546  553  675  701  856  909\n",
            " 1165 1264 1392 1420 1448 1710 1769 1949 1954 2141 2154 2157 2214 2220\n",
            " 2424 2443 2448 2465 2470 2474 2533 2552 2590 2617 2658 2663 2692 2700\n",
            " 2744 2755 2762 2767 2785 2980 3001 3066 3105 3194 3207 3214 3234 3241\n",
            " 3266 3450 3490 3496 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  73   76  143  498  563  673  682  730  737  755  794  871  929  947\n",
            "  986 1053 1059 1113 1147 1282 1339 1410 1412 2432 2470 2600 2665 2792\n",
            " 2907 2920 2971 2978 2984 3099 3106 3115 3170 3177 3291 3306 3355 3363\n",
            " 3483 3491 3497 3547 3626 3636 3674 3676 3683 3740 3751 3764 3811 3819\n",
            " 3892 3931 4003 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 117  124  132  142  153  167  285  295  328  340  451  460  470  613\n",
            "  644  654  667  771  786 1095 1110 1116 1221 1229 1236 1242 1415 1423\n",
            " 1431 1532 1555 1572 1578 1588 1988 1994 2004 2040 2127 2137 2469 2488\n",
            " 2723 2744 2845 2857 2916 2939 3036 3044 3049 3065 3174 3192 3289 3299\n",
            " 3304 3321 3410 3418 3423 3430 3437 3445 3534 3541 3548 3553 3562 3571\n",
            " 3984 4024]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 220  296  303  398  492  548  599  648  690  735  840  889  921  935\n",
            "  944 1030 1040 1268 1286 1467 1657 1670 1689 1709 1798 1850 1926 1978\n",
            " 1996 2008 2063 2130 2161 2310 2362 2490 2500 2508 2632 2643 2682 2869\n",
            " 2876 3002 3012 3336 3350 3383 3542 3566 3591 3729 3740 3747 3753 3761\n",
            " 3852 3858 3865 3890 3936 3942 3948 4052 4062 4068 4075 4083]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   4   10   16   61   90   98  103  108  117  199  275  283  292  299\n",
            "  390  462  471  479  486  497  649  668  836  848  865 1218 1233 1272\n",
            " 1410 1418 1433 1464 1602 1611 1656 1794 1800 1810 1820 1848 1986 1995\n",
            " 2005 2040 2190 2230 2234 2242 2246 2390 2425 2435 2442 2618 2627 2635\n",
            " 2776 2787 2810 2819 2827 3008 3173 3181 3192 3361 3385 3557 3565 3577\n",
            " 3735 3747 3764 3782 3968 4062 4064 4067 4070 4074 4081 4086 4088 4089\n",
            " 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89   97  229  276  284  471  482  821  837  848 1012 1019 1027 1209\n",
            " 1221 1228 1555 1594 1604 1612 1750 1786 1794 1801 1807 1943 1978 1992\n",
            " 2130 2170 2182 2492 2502 2552 2690 2747 3131 3139 3481 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  845  929 1212 1335 1341 1549 1558 1686 2437 3972]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  57   65  125  189  344  350  354  359  409  412  416  486  538  544\n",
            "  548  746  753  834  874  877  882  937  944  948 1024 1076 1136 1170\n",
            " 1184 1191 1204 1290 1297 1354 1395 1425 1470 1482 1523 1612 1651 1661\n",
            " 1688 1715 1739 1746 1842 1846 1870 1880 1970 1974 1982 1994 2005 2046\n",
            " 2100 2118 2120 2122 2124 2127 2129 2131 2133 2137 2142 2146 2149 2151\n",
            " 2154 2250 2292 2296 2339 2375 2383 2423 2500 2503 2506 2508 2511 2515\n",
            " 2518 2522 2526 2528 2530 2533 2535 2537 2539 2544 2550 2695 2741 2746\n",
            " 3142 3152 3161 3193 3550 3564 3688 3946 3951]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  151  294  301  351  395  409  466  482  541  604 1051 1057 1206\n",
            " 1251 1283 1295 1306 1549 1699 1849 1862 1868 1873 1883 2071 2082 2090\n",
            " 2105 2125 2361 2374 2381 2387 2393 2403 2682 2694 2703 2715 2724 3049\n",
            " 3065 3240 3259 3496 3641 3753 3872 3920 3946 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  103  283  294  306  478  490  664  674  680  687 1031 1042 1050\n",
            " 1413 1422 1438 1447 1627 1708 2067 2083 2247 2255 2361 2503 2510 2522\n",
            " 2553 2695 2702 2715 2745 3160 3198 3340 3588 3596 3608 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  16   32   75  151  202  219  225  280  285  328  412  476  540  787\n",
            "  796  806  855  860  865  915  921  927  933  989 1042 1049 1052 1055\n",
            " 1060 1063 1070 1092 1099 1173 1186 1194 1307 1314 1368 1417 1481 1493\n",
            " 1554 1590 1602 1610 1653 1666 1673 1683 1717 1866 1874 1882 1910 1974\n",
            " 2103 2122 2132 2230 2361 2377 2385 2441 2451 2488 2616 2630 2746 2754\n",
            " 2883 2936 3013 3023 3032 3065 3146 3160 3324 3330 3444 3514 3523 3932\n",
            " 3974 4055 4063 4072]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 149  164  534  542  548  553  733  856  866  872 1046 1059 1276 1332\n",
            " 1351 1496 1540 1547 1666 1671 1978 1988 1996 2003 2006 2012 2123 2130\n",
            " 2139 2170 2184 2298 2316 2491 2810 3195 3323 4004]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [2080]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  32 2948 2957 3279 3314 3598 3616 3626 3640 3938 3975 3990 4009 4013\n",
            " 4029]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81   90  204  211  220  337  349  457  466  540  547  663  913 1168\n",
            " 1176 1196 1224 1350 1355 1362 1387 1516 1542 1549 1644 1676 1684 1734\n",
            " 1837 1862 1867 1873 1966 1998 2044 2054 2172 2284 2685 3044 3054 3150\n",
            " 3240 3338 3495 3592 3932]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24   29   83   88  155  159  163  216  220  224  280  285  462  467\n",
            "  487  525  529  615  618  653  657  681  844  909  914  918  921  924\n",
            "  928  999 1102 1110 1130 1194 1227 1232 1258 1366 1386 1433 1451 1560\n",
            " 1578 1802 1805 1869 1876 1885 1894 1898 1900 1994 2027 2122 2126 2134\n",
            " 2155 2252 2283 2390 2410 2475 2520 2584 2602 2817 2956 2986 3051 3082\n",
            " 3255 3306 3338 3408 3448 3449 3518 3545 3560 3577 3594 3765 3804 3810\n",
            " 3814 3864 3893 3938 3992 3998 4002 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  40   82   92  167  218  297  341  352  451  461  564  680  700  712\n",
            "  984 1002 1097 1248 1306 1316 1331 1337 1357 1438 1489 1545 1553 1588\n",
            " 1677 1690 1700 1715 1721 1809 1823 1886 1893 1907 1913 1931 1941 1952\n",
            " 2035 2041 2064 2079 2131 2163 2169 2258 2291 2297 2382 2395 2419 2425\n",
            " 2513 2546 2553 2612 2617 2639 2650 2659 2722 2740 2746 2767 2778 2842\n",
            " 2868 2873 2895 2958 2968 2984 2999 3337 3346 3364 3471 3493 3595 3605\n",
            " 3618 3725 3739 3756 3853 3866 3884 3980 3994 4012]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  265  324  338  345  416  451  462  472  579  592  786  836  901\n",
            " 1155 1414 1458 3979 3995 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 160  172  463  471  716  784  793  911  918  924  932 1165 1236 1260\n",
            " 1267 1295 1311 1432 1487 1739 1746 1754 1825 1833 2180 2427 2520 2531\n",
            " 2638 2683 2777 2853 2904 2914 2953 2959 3003 3009 3278 3330 3387 3394\n",
            " 3581 3595 3604 3613 3906 3922 3978]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 165  204  295  303  347  352  400  487  530  540  629  674  682  686\n",
            "  723  775  809  857  859  913 1017 1286 1291 1338 1346 1432 1480 1604\n",
            " 1611 1722 1736 1850 1927 1942 2043 2117 2235 2249 2427 2434 2442 2448\n",
            " 2683 2693 2701 2951 3275 3286 3294 3330 3480 3529 3615 3623 3656 3671\n",
            " 3990 3997 4007]\n",
            "\n",
            " TEST ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.630 / 0.000 (0.500) \n",
            "\n",
            "TEST ACC (STRICT) CURVE: 0:0.000 >20:0.000 >40:0.000 >60:0.000 >80:0.000 >100:0.000 >120:0.000 >140:0.000 >160:0.000\n",
            "TEST ACC (SOFT) CURVE: 0:0.500 >20:0.860 >40:0.900 >60:1.000 >80:1.000 >100:1.000 >120:1.000 >140:0.980 >160:1.000\n",
            "TEST RECALL CURVE: 0:0.63 >20:0.92 >40:0.94 >60:1.00 >80:1.00 >100:1.00 >120:1.00 >140:0.99 >160:1.00\n",
            "iter = 160\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 10 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 161\n",
            "\n",
            "Training grid AUGMENT size: (33,18) from (9,16)\n",
            "iter = 161\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 11 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 162\n",
            "\n",
            "Training grid AUGMENT size: (15,38) from (10,25)\n",
            "iter = 162\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 12 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 163\n",
            "\n",
            "Training grid AUGMENT size: (26,62) from (6,36)\n",
            "iter = 163\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 13 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 164\n",
            "\n",
            "Training grid AUGMENT size: (11,38) from (6,20)\n",
            "iter = 164\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 14 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 165\n",
            "\n",
            "Training grid AUGMENT size: (25,25) from (8,23)\n",
            "iter = 165\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 15 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 166\n",
            "\n",
            "Training grid AUGMENT size: (17,40) from (10,28)\n",
            "iter = 166\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 16 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 167\n",
            "\n",
            "Training grid AUGMENT size: (23,66) from (7,56)\n",
            "iter = 167\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 17 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 168\n",
            "\n",
            "Training grid AUGMENT size: (30,31) from (6,18)\n",
            "iter = 168\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 18 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 169\n",
            "\n",
            "Training grid AUGMENT size: (19,28) from (8,25)\n",
            "iter = 169\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 19 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 170\n",
            "\n",
            "Training grid AUGMENT size: (27,33) from (10,28)\n",
            "\t >>time per step: 11.29s <<\n",
            "len(data_input_flat): 891\n",
            "indexes: [ 12  44  55  79  83  87 110 115 119 122 144 151 170 177 202 206 212 215\n",
            " 260 269 270 271 275 280 288 302 305 306 311 312 327 332 336 345 347 373\n",
            " 396 407 411 420 426 432 436 439 445 453 459 462 465 471 493 500 525 533\n",
            " 539 546 553 601 621 631 639 646 662 667 674 683 689 733 737 739 741 743\n",
            " 746 749 754 755 762 771 783 787 789 790 795 803 807 810 815 820 829 841\n",
            " 854 873]\n",
            "len(data_input_flat): 891\n",
            "indexes: [ 32  93 140 141 144 175 176 179 180 205 206 207 209 221 239 242 244 245\n",
            " 246 253 286 306 307 308 309 310 311 334 339 358 367 368 370 374 389 393\n",
            " 401 402 406 420 433 434 435 438 439 471 472 477 478 486 504 505 510 511\n",
            " 512 537 539 540 551 552 582 594 599 603 605 606 622 631 635 636 639 655\n",
            " 670 671 690 702 715 735 749 768 828 834 866]\n",
            "len(data_input_flat): 891\n",
            "indexes: [ 17 318 559 626 661 689 793]\n",
            "len(data_input_flat): 891\n",
            "indexes: [ 21  85 109 137 150 177 179 180 184 214 243 247 249 257 268 269 271 272\n",
            " 274 275 276 277 280 281 284 289 292 302 304 306 311 316 319 322 333 338\n",
            " 399 404 409 425 431 436 459 507 525 539 541 558 564 571 574 575 577 580\n",
            " 582 584 585 590 597 621 669 690 693 730 743 761 776 860 863 866 869 871\n",
            " 875 880]\n",
            "\n",
            "Iter: 170/200, total loss: 1.6049, model loss: 1.3911, regularization loss: 0.2139\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607 >140:1.610 >150:1.605 >160:1.606 >170:1.605\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000 >140:1.000 >150:1.000 >160:1.000 >170:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00 >140:0.88 >150:1.00 >160:1.00 >170:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00 >140:1.00 >150:1.00 >160:1.00 >170:1.00\n",
            "VALIDATION Statistic 170(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "iter = 170\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 20 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 171\n",
            "\n",
            "Training grid AUGMENT size: (6,43) from (6,23)\n",
            "iter = 171\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 21 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 172\n",
            "\n",
            "Training grid AUGMENT size: (10,29) from (8,29)\n",
            "iter = 172\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 22 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 173\n",
            "\n",
            "Training grid AUGMENT size: (25,32) from (8,17)\n",
            "iter = 173\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 23 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 174\n",
            "\n",
            "Training grid AUGMENT size: (27,80) from (7,56)\n",
            "iter = 174\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 24 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 175\n",
            "\n",
            "Training grid AUGMENT size: (11,27) from (8,24)\n",
            "iter = 175\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 25 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 176\n",
            "\n",
            "Training grid AUGMENT size: (9,32) from (7,20)\n",
            "iter = 176\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 26 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 177\n",
            "\n",
            "Training grid AUGMENT size: (16,35) from (7,17)\n",
            "iter = 177\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 27 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 178\n",
            "\n",
            "Training grid AUGMENT size: (22,24) from (7,23)\n",
            "iter = 178\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 28 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 179\n",
            "\n",
            "Training grid AUGMENT size: (22,34) from (7,32)\n",
            "iter = 179\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 29 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 180\n",
            "\n",
            "Training grid AUGMENT size: (47,26) from (7,26)\n",
            "\t >>time per step: 13.16s <<\n",
            "len(data_input_flat): 1222\n",
            "indexes: [   9   12   15   59   63   67   70   94  113  117  145  167  246  262\n",
            "  296  298  301  305  308  313  340  343  349  443  455  466  486  493\n",
            "  499  544  547  550  559  565  596  611  617  653  666  703  707  710\n",
            "  808  858  860  864  963 1067 1120 1198]\n",
            "len(data_input_flat): 1222\n",
            "indexes: [  50  161  166  171  187  189  216  219  222  241  245  247  262  265\n",
            "  269  274  304  316  320  326  369  382  452  456  460  480  499  504\n",
            "  590  616  642  676  703  756  782  889  893  912  916  944  968 1020\n",
            " 1045 1047 1078 1082 1202 1206]\n",
            "len(data_input_flat): 1222\n",
            "indexes: [  33  165  168  230  270  297  300  301  312  325  326  328  344  345\n",
            "  346  349  372  377  395  397  398  400  402  403  405  406  426  447\n",
            "  449  451  453  455  457  472  474  476  478  480  482  484  504  560\n",
            "  563  585  587  589  611  614  616  635  641  653  667  681  684  686\n",
            "  688  690  705  711  714  732  745  771  783  785  793  796  810  816\n",
            "  835  849  927  928  939  942  944 1075 1078 1095 1105 1110 1117 1147\n",
            " 1149 1151 1157 1162 1177 1183 1188 1209 1214]\n",
            "len(data_input_flat): 1222\n",
            "indexes: [  15   34   37   49   75  125  132  136  177  180  184  186  231  236\n",
            "  239  309  318  320  339  341  361  364  367  369  370  372  375  413\n",
            "  417  420  424  428  453  465  468  469  472  474  475  476  478  570\n",
            "  574  599  622  704  708  710  712  726  734  807  830  914  921  932\n",
            " 1021 1024 1026 1028 1031 1070 1077 1081 1083 1098 1112 1129 1132 1136\n",
            " 1152 1177 1184]\n",
            "\n",
            "Iter: 180/200, total loss: 1.6053, model loss: 1.3917, regularization loss: 0.2136\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607 >140:1.610 >150:1.605 >160:1.606 >170:1.605 >180:1.605\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000 >180:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000 >180:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000 >140:1.000 >150:1.000 >160:1.000 >170:1.000 >180:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00 >140:0.88 >150:1.00 >160:1.00 >170:1.00 >180:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00 >140:1.00 >150:1.00 >160:1.00 >170:1.00 >180:1.00\n",
            "VALIDATION Statistic 180(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 101  357  368  413  426  501  527  723  820  871  925  933  939 1009\n",
            " 1042 1110 1116 1249 1362 1438 1554 1562 1572 1619 1628 1659 1700 1712\n",
            " 1742 1754 1764 1851 1881 1892 1946 1952 2026 2066 2078 2087 2128 2138\n",
            " 2215 2234 2258 2265 2275 2313 2318 2347 2372 2388 2405 2423 2439 2521\n",
            " 2534 2543 2585 2615 2667 2833 2892 2935 3063 3086 3098 3183 3267 3275\n",
            " 3348 3356 3367 3399 3445 3475 3483 3489 3522 3568 3602 3616 3619 3621\n",
            " 3653 3683 3693 3707 3727 3739 3810 3821 3987 4071]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   31   43  149  158  167  544  553  681  693  709  724  820  837\n",
            "  874  918 1028 1309 1427 1438 1465 1483 1592 1612 1617 1628 1810 1818\n",
            " 1894 1913 1932 2060 2105 2213 2233 2251 2258 2267 2379 2389 2400 2425\n",
            " 2553 2571 2579 2592 2699 2706 2721 2745 2890 2903 2938 3018 3090 3131\n",
            " 3210 3217 3225 3232 3240 3258 3608 3637 3741 3830 3997 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   88   98  150  221  228  275  284  290  296  344  353  436  465\n",
            "  493  519  630  646  652  658  688  710  752  781  899  903  909  916\n",
            "  948 1033 1041 1091 1099 1108 1141 1154 1163 1205 1282 1288 1333 1346\n",
            " 1352 1357 1397 1426 1434 1474 1480 1487 1525 1609 1617 1671 1782 1794\n",
            " 1796 1801 1811 1860 1865 1874 1975 1985 1992 2001 2049 2054 2060 2066\n",
            " 2103 2180 2190 2237 2370 2378 2422 2498 2503 2551 2626 2630 2680 2886\n",
            " 2892 2935 3145 3678 3926 3939 3956 4055 4065]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   32   86  147  160  212  285  339  347  403  407  412  537  711\n",
            "  716  721  759  841  900  918  937  942  987 1030 1036 1060 1066 1094\n",
            " 1214 1221 1224 1229 1234 1321 1348 1352 1356 1361 1365 1449 1476 1481\n",
            " 1487 1577 1604 1610 1619 1705 1732 1736 1742 1748 1834 1859 1864 1961\n",
            " 2115 2123 2131 2214 3534 3541 3561 3587 3610 3616 3732 3739 3745 3753\n",
            " 3980 4057]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  75   92  138  145  194  267  274  322  330  476  583  598  612  651\n",
            "  662  796  842 1111 1117 1175 1182 1265 1312 1329 1376 1634 1713 1824\n",
            " 1841 1888 1907 2016 2032 2080 2208 2301 2783 2975 3057 3431 3606 3679\n",
            " 3726 3734 3751 3868 3872 3919 3926 3943 3990 3999 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  15  142  151  157  171  277  285  294  344  353  360  411  484  601\n",
            "  658  672  681  856  870  920  931  937 1006 1049 1060 1067 1209 1219\n",
            " 1398 1403 1412 1419 1431 1589 1593 1606 1737 1745 1753 1782 1787 1849\n",
            " 1852 1860 1867 1873 1878 2040 2051 2242 2251 2314 2324 2335 2341 2372\n",
            " 2525 2536 2544 2576 2627 2637 2646 2650 2658 2669 2677 2683 2722 2777\n",
            " 2830 2842 2850 2856 2868 2877 2884 2929 2935 2941 2947 2957 2967 2974\n",
            " 2983 3034 3045 3053 3061 3067 3076 3084 3090 3145 3156 3165 3176 3187\n",
            " 3194 3279 3288 3293 3302 3312 3355 3361 3366 3375 3384 3390 3394 3399\n",
            " 3410 3474 3484 3496 3590 3603 3617 3627 3638 3702 3715 3724 3730 3737\n",
            " 3750 3774 3910 3919 3923 3932 3943 3952 3963 4063]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  95  323  364  375  380  571  580  827 1033 1074 1083 1227 1266 1276\n",
            " 1299 1482 1487 1492 1521 1531 1739 1748 1752 1778 1787 1930 2003 2033\n",
            " 2044 2181 2300 2436 2454 2509 2556 2690 2806 2811 3098 3111 3396 3586\n",
            " 3592 3599 3605 3780 3793 3976]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  56  650  734  948 1358 1617 1927 2044 2340 2349 2438 2651 3158 3993\n",
            " 4008]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  35  139  150  164  175  180  295  345  464  470  479  489  598  606\n",
            "  615  726  734  743  846  852  859  873  931  978 1046 1097 1193 1199\n",
            " 1208 1219 1232 1317 1348 1357 1366 1456 1467 1479 1491 1605 1636 1657\n",
            " 1779 1786 1796 1810 1913 2051 2059 2070 2090 2098 2107 2233 2360 2377\n",
            " 2489 2502 2512 2522 2618 2628 2637 2647 2745 2758 2932 2954 3078 3088\n",
            " 3097 3192 3215 3238 3270 3398 3409 3514 3588 3728 3905 3909 3917 3923\n",
            " 4039]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24  408  418 2755 2763 2776 2792 2804 2812 3192 3199 3528 3705 4060]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76  540  553  592  672  681  724  786  796  804  811  919  931 1149\n",
            " 1157 1169 1204 1246 1253 1260 1274 1303 1348 1354 1360 1398 1404 1476\n",
            " 1483 1525 1533 1594 1603 1609 1616 1624 1630 1641 1650 1723 1729 1736\n",
            " 1793 1800 1812 1822 1851 1921 1927 1937 1978 2052 2058 2069 2182 2188\n",
            " 2194 2370 2381 2427 2499 2511 2555 2627 2632 2685 2885 2938 3219 3225\n",
            " 3235 3239 3244 3351 3365 3374 3612 3620 3920]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22  476  480  797  804 1019 1104 1112 1162 1168 1174 1275 1281 1293\n",
            " 1339 1457 1507 1531 1697 1705 1724 1824 1834 1907 1915 2142 2218 2527\n",
            " 2607 2835 2842 2849 2858 2963 3034 3041 3050 3154 3161 3167 3178 3281\n",
            " 3289 3359 3369 3790 3800 3807 3882 3919 3994 4001 4011 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 221  481  490  528  550  604  659  679  720  728  907  912  954  966\n",
            " 1003 1020 1027 1038 1048 1052 1056 1077 1113 1158 1217 1246 1481 1496\n",
            " 1525 1612 1653 1667 1732 1738 1782 1820 1860 1863 1869 1875 1910 1988\n",
            " 1995 2039 2116 2122 2167 2197 2244 2249 2255 2296 2323 2372 2379 2424\n",
            " 2456 2462 2500 2504 2511 2552 2628 2636 2708 2744 2756 2762 2844 2857\n",
            " 2869 2884 2894 2938 3013 3021 3130 3141 3149 3385 3406 3530 3579 3655\n",
            " 3666 3701 4041 4047 4053 4069 4086 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  102  278  287  296  407  480  487  596  612  718  800  815 1031\n",
            " 1044 1052 1058 1222 1230 1237 1248 1253 1263 1354 1372 1672 1679 1692\n",
            " 1701 1705 1729 1810 1821 1862 1867 1986 1992 2113 2120 2305 2311 2320\n",
            " 2497 2504 2625 2631 2978 2987 3006 3169 3196 3318 3332 3342 3356 3655\n",
            " 3670 3848 3855 3870 3976 3998 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  351  407  541  667  720  788  790  794  853  860  920  978  982\n",
            "  989 1114 1176 1183 1294 1297 1302 1369 1376 1422 1493 1496 1622 1676\n",
            " 1740 1750 1869 1873 1878 1941 2007 2010 2255 2271 2765 3023 3281 3291\n",
            " 3294 3327 3340 3422 3453 3468 3472 3475 3478 3483 3519 3599 3709 3727\n",
            " 3917 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29   33   95  108  415  425  574  702 1113 1123 1152 1314 1321 1693\n",
            " 1699 1705 1880 1883 1903 2006 2073 2095 2266 2591 2651 2721 3097 3102\n",
            " 3117 3737 3743 3746 3752 3991 3998 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 146  166  205  250  346  364  788  800  810  819  918  926  934  941\n",
            " 1052 1060 1065 1181 1190 1421 1432 1440 1448 1454 1547 1554 1564 1571\n",
            " 1576 1586 1677 1688 1927 1932 1939 1949 1958 1978 2055 2059 2067 2078\n",
            " 2106 2183 2187 2235 2491 2533 2540 2673 2684 2789 2796 2812 2952 2961\n",
            " 2974 2983 3000 3074 3275 3289 3297 3304 3316 3346 3465 3471 3481 3648\n",
            " 3666 3679 3794 3840 3968 3986]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  347 1040 1822 2038 2184 2193 2441 2680 2755 2756 2762 2770 2873\n",
            " 3128 3143 3386 3401 3461 3961 3971]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  102  219  234  344  353  362  474  483  490  609  711  755  765\n",
            "  899  910  934  947  954 1157 1162 1167 1211 1285 1290 1297 1304 1311\n",
            " 1339 1488 1496 1529 1616 1622 1657 1745 1756 1785 1871 1878 1914 2004\n",
            " 2011 2042 2063 2335 2364 2459 2492 2716 2748 2907 2940 3037 3068 3358\n",
            " 3363 3474 3489 3546 3564 3741 3751 3926 4004 4015]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 158  479  733  862  934 1159 1284 1524 2371 2381 2626 2632 2941 2953\n",
            " 3033 3066 3069 3324 3635 3645 3679 3934 3945 4064 4077]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25   28   31  157  216  221  224  228  349  528  533  552  596  616\n",
            "  619  636  655  658  746  787  823  828  857  879  900  968 1013 1018\n",
            " 1093 1195 1205 1213 1231 1234 1299 1303 1323 1388 1410 1413 1430 1432\n",
            " 1435 1438 1470 1478 1492 1495 1559 1561 1564 1578 1678 1683 1687 1706\n",
            " 1935 1937 1940 1943 1945 1950 1958 2062 2065 2069 2074 2127 2136 2143\n",
            " 2147 2254 2256 2261 2265 2269 2273 2276 2298 2320 2324 2330 2333 2353\n",
            " 2390 2400 2402 2446 2451 2459 2468 2488 2574 2589 2638 2643 2648 2652\n",
            " 2659 2695 2696 2752 2779 2820 2830 2833 2836 2839 2843 2947 2949 2952\n",
            " 3059 3067 3090 3105 3184 3252 3320 3379 3407 3435 3563 3569 3724 3755\n",
            " 3762 3850 3889 3989 3994 4008 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  175  333  343  351  357  365  377  475  482  489  600  604  608\n",
            "  618  708  721  732  776  849  904  974  987  994 1000 1011 1018 1104\n",
            " 1115 1127 1133 1139 1144 1226 1239 1353 1365 1478 1511 1613 1683 1691\n",
            " 1721 1854 1934 1941 1951 1977 2061 2067 2075 2106 2187 2197 2209 2235\n",
            " 2381 2386 2399 2429 3345 3842 3849 3853 3854 3858 4039 4051 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29  104  217  230  234  348  491  716  812  818  830  887  995 1034\n",
            " 1048 1080 1150 1161 1270 1278 1415 1423 1432 1531 1543 1548 1619 1659\n",
            " 1671 1680 1787 1915 2043 2183 2190 2299 2311 2427 2631 2810 3091 3098\n",
            " 3107 3177 3285 3297 3370 3482 3490 3852]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84   90  104  277  279  407  599  723  731  759  836  905  916  926\n",
            "  954 1028 1034 1043 1082 1166 1179 1210 1281 1506 1514 1526 1633 1640\n",
            " 1657 1763 1770 1785 1892 1977 2025 2089 2142 2224 2244 2252 2321 2370\n",
            " 2385 2658 2679 3168 3214 3222 3241 3336 3343 3355 3367 3377 3534 3540\n",
            " 3547 3555 3564 3651 3662 3670 3674 3690 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  348  356  364  599  605  611  618  638  728  737  745  860  868\n",
            " 1041 1099 1109 1119 1125 1130 1165 1227 1232 1233 1236 1292 1409 1493\n",
            " 1674 1684 1803 2128 2135 2217 2271 2275 2278 2282 2385 2409 2511 2534\n",
            " 2538 2665 2728 2837 2889 2894 3150 3158 3273 3280 3286 3358 3363 3371\n",
            " 3676 3681 3939 3992 4009]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39   82   90  206  212  220  228  236  343  355  363  644  696  773\n",
            "  899  952  958 1083 1092 1100 1221 1229 1233 1532 1541 1797 1803 1852\n",
            " 1922 1980 2243 2300 2562 2620 2696 2745 2959 2973 2992 3031 3047 3154\n",
            " 3159 3166 3177 3277 3285 3292 3300 3311 3406 3412 3419 3433 3601 3610\n",
            " 3618 3626 3726 3737 3754 4050 4058 4068 4076]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 106  153  204  209  300  355  393  407  604  887  995 1034 1041 1052\n",
            " 1192 1330 1337 1859 2617 2743 3066 3573 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   33   85  104  152  156  225  231  434  467  524  567  626  651\n",
            "  657  780  785  946 1271 1290 1297 1464 1480 1485 1591 1674 1682 1802\n",
            " 1812 1848 2233 2250 2426 2438 2875 2886 3320 3340 3357 3791 3942]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  227  275  283  418  424  473  595  606  613  623  973  979 1030\n",
            " 1046 1321 1331 1338 1348 1377 1407 1419 1604 1784 1797 1806 1976 1989\n",
            " 1994 2002 2169 2181 2190 2553 2584 2637 2809 2825 2994 3001 3078 3090\n",
            " 3496 3511 3537 3545 3686 3696 3703 3730 3741 3893 3930 3941 3985]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  60  189  224  282  286  293  344  409  413  417  421  509  540  546\n",
            "  671  674  747  751  766  786  791  794  808  858  876  879  894  915\n",
            "  919 1004 1008 1022 1056 1214 1234 1407 1425 1430 1454 1553 1557 1562\n",
            " 1681 1685 1743 1747 1874 1878 1881 2000 2004 2011 2031 2080 2128 2132\n",
            " 2159 2257 2262 2267 2287 2384 2392 2416 2510 2513 2518 2638 2640 2643\n",
            " 2768 2774 2896 2901 2927 3022 3026 3032 3152 3184 3278 3281 3285 3290\n",
            " 3537 3632 3727 3731 3735 3740 3760 3855 3951 4056]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   34   60  104  317  457  463  473  546  553  675  701  856  909\n",
            " 1165 1264 1392 1420 1448 1710 1769 1949 1954 2141 2154 2157 2214 2220\n",
            " 2424 2443 2448 2465 2470 2474 2533 2552 2590 2617 2658 2663 2692 2700\n",
            " 2744 2755 2762 2767 2785 2980 3001 3066 3105 3194 3207 3214 3234 3241\n",
            " 3266 3450 3490 3496 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  73   76  143  498  563  673  682  730  737  755  794  871  929  947\n",
            "  986 1053 1059 1113 1147 1282 1339 1410 1412 2432 2470 2600 2665 2792\n",
            " 2907 2920 2971 2978 2984 3099 3106 3115 3170 3177 3291 3306 3355 3363\n",
            " 3483 3491 3497 3547 3626 3636 3674 3676 3683 3740 3751 3764 3811 3819\n",
            " 3892 3931 4003 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 117  124  132  142  153  167  285  295  328  340  451  460  470  613\n",
            "  644  654  667  771  786 1095 1110 1116 1221 1229 1236 1242 1415 1423\n",
            " 1431 1532 1555 1572 1578 1588 1988 1994 2004 2040 2127 2137 2469 2488\n",
            " 2723 2744 2845 2857 2916 2939 3036 3044 3049 3065 3174 3192 3289 3299\n",
            " 3304 3321 3410 3418 3423 3430 3437 3445 3534 3541 3548 3553 3562 3571\n",
            " 3984 4024]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 220  296  303  398  492  548  599  648  690  735  840  889  921  935\n",
            "  944 1030 1040 1268 1286 1467 1657 1670 1689 1709 1798 1850 1926 1978\n",
            " 1996 2008 2063 2130 2161 2310 2362 2490 2500 2508 2632 2643 2682 2869\n",
            " 2876 3002 3012 3336 3350 3383 3542 3566 3591 3729 3740 3747 3753 3761\n",
            " 3852 3858 3865 3890 3936 3942 3948 4052 4062 4068 4075 4083]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   4   10   16   61   90   98  103  108  117  199  275  283  292  299\n",
            "  390  462  471  479  486  497  649  668  836  848  865 1218 1233 1272\n",
            " 1410 1418 1433 1464 1602 1611 1656 1794 1800 1810 1820 1848 1986 1995\n",
            " 2005 2040 2190 2230 2234 2242 2246 2390 2425 2435 2442 2618 2627 2635\n",
            " 2776 2787 2810 2819 2827 3008 3173 3181 3192 3361 3385 3557 3565 3577\n",
            " 3735 3747 3764 3782 3968 4062 4064 4067 4070 4074 4081 4086 4088 4089\n",
            " 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89   97  229  276  284  471  482  821  837  848 1012 1019 1027 1209\n",
            " 1221 1228 1555 1594 1604 1612 1750 1786 1794 1801 1807 1943 1978 1992\n",
            " 2130 2170 2182 2492 2502 2552 2690 2747 3131 3139 3481 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  845  929 1212 1335 1341 1549 1558 1686 2437 3972]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  57   65  125  189  344  350  354  359  409  412  416  486  538  544\n",
            "  548  746  753  834  874  877  882  937  944  948 1024 1076 1136 1170\n",
            " 1184 1191 1204 1290 1297 1354 1395 1425 1470 1482 1523 1612 1651 1661\n",
            " 1688 1715 1739 1746 1842 1846 1870 1880 1970 1974 1982 1994 2005 2046\n",
            " 2100 2118 2120 2122 2124 2127 2129 2131 2133 2137 2142 2146 2149 2151\n",
            " 2154 2250 2292 2296 2339 2375 2383 2423 2500 2503 2506 2508 2511 2515\n",
            " 2518 2522 2526 2528 2530 2533 2535 2537 2539 2544 2550 2695 2741 2746\n",
            " 3142 3152 3161 3193 3550 3564 3688 3946 3951]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  151  294  301  351  395  409  466  482  541  604 1051 1057 1206\n",
            " 1251 1283 1295 1306 1549 1699 1849 1862 1868 1873 1883 2071 2082 2090\n",
            " 2105 2125 2361 2374 2381 2387 2393 2403 2682 2694 2703 2715 2724 3049\n",
            " 3065 3240 3259 3496 3641 3753 3872 3920 3946 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  103  283  294  306  478  490  664  674  680  687 1031 1042 1050\n",
            " 1413 1422 1438 1447 1627 1708 2067 2083 2247 2255 2361 2503 2510 2522\n",
            " 2553 2695 2702 2715 2745 3160 3198 3340 3588 3596 3608 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  16   32   75  151  202  219  225  280  285  328  412  476  540  787\n",
            "  796  806  855  860  865  915  921  927  933  989 1042 1049 1052 1055\n",
            " 1060 1063 1070 1092 1099 1173 1186 1194 1307 1314 1368 1417 1481 1493\n",
            " 1554 1590 1602 1610 1653 1666 1673 1683 1717 1866 1874 1882 1910 1974\n",
            " 2103 2122 2132 2230 2361 2377 2385 2441 2451 2488 2616 2630 2746 2754\n",
            " 2883 2936 3013 3023 3032 3065 3146 3160 3324 3330 3444 3514 3523 3932\n",
            " 3974 4055 4063 4072]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 149  164  534  542  548  553  733  856  866  872 1046 1059 1276 1332\n",
            " 1351 1496 1540 1547 1666 1671 1978 1988 1996 2003 2006 2012 2123 2130\n",
            " 2139 2170 2184 2298 2316 2491 2810 3195 3323 4004]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [2080]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  32 2948 2957 3279 3314 3598 3616 3626 3640 3938 3975 3990 4009 4013\n",
            " 4029]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81   90  204  211  220  337  349  457  466  540  547  663  913 1168\n",
            " 1176 1196 1224 1350 1355 1362 1387 1516 1542 1549 1644 1676 1684 1734\n",
            " 1837 1862 1867 1873 1966 1998 2044 2054 2172 2284 2685 3044 3054 3150\n",
            " 3240 3338 3495 3592 3932]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24   29   83   88  155  159  163  216  220  224  280  285  462  467\n",
            "  487  525  529  615  618  653  657  681  844  909  914  918  921  924\n",
            "  928  999 1102 1110 1130 1194 1227 1232 1258 1366 1386 1433 1451 1560\n",
            " 1578 1802 1805 1869 1876 1885 1894 1898 1900 1994 2027 2122 2126 2134\n",
            " 2155 2252 2283 2390 2410 2475 2520 2584 2602 2817 2956 2986 3051 3082\n",
            " 3255 3306 3338 3408 3448 3449 3518 3545 3560 3577 3594 3765 3804 3810\n",
            " 3814 3864 3893 3938 3992 3998 4002 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  40   82   92  167  218  297  341  352  451  461  564  680  700  712\n",
            "  984 1002 1097 1248 1306 1316 1331 1337 1357 1438 1489 1545 1553 1588\n",
            " 1677 1690 1700 1715 1721 1809 1823 1886 1893 1907 1913 1931 1941 1952\n",
            " 2035 2041 2064 2079 2131 2163 2169 2258 2291 2297 2382 2395 2419 2425\n",
            " 2513 2546 2553 2612 2617 2639 2650 2659 2722 2740 2746 2767 2778 2842\n",
            " 2868 2873 2895 2958 2968 2984 2999 3337 3346 3364 3471 3493 3595 3605\n",
            " 3618 3725 3739 3756 3853 3866 3884 3980 3994 4012]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  265  324  338  345  416  451  462  472  579  592  786  836  901\n",
            " 1155 1414 1458 3979 3995 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 160  172  463  471  716  784  793  911  918  924  932 1165 1236 1260\n",
            " 1267 1295 1311 1432 1487 1739 1746 1754 1825 1833 2180 2427 2520 2531\n",
            " 2638 2683 2777 2853 2904 2914 2953 2959 3003 3009 3278 3330 3387 3394\n",
            " 3581 3595 3604 3613 3906 3922 3978]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 165  204  295  303  347  352  400  487  530  540  629  674  682  686\n",
            "  723  775  809  857  859  913 1017 1286 1291 1338 1346 1432 1480 1604\n",
            " 1611 1722 1736 1850 1927 1942 2043 2117 2235 2249 2427 2434 2442 2448\n",
            " 2683 2693 2701 2951 3275 3286 3294 3330 3480 3529 3615 3623 3656 3671\n",
            " 3990 3997 4007]\n",
            "\n",
            " TEST ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.630 / 0.000 (0.500) \n",
            "\n",
            "TEST ACC (STRICT) CURVE: 0:0.000 >20:0.000 >40:0.000 >60:0.000 >80:0.000 >100:0.000 >120:0.000 >140:0.000 >160:0.000 >180:0.000\n",
            "TEST ACC (SOFT) CURVE: 0:0.500 >20:0.860 >40:0.900 >60:1.000 >80:1.000 >100:1.000 >120:1.000 >140:0.980 >160:1.000 >180:1.000\n",
            "TEST RECALL CURVE: 0:0.63 >20:0.92 >40:0.94 >60:1.00 >80:1.00 >100:1.00 >120:1.00 >140:0.99 >160:1.00 >180:1.00\n",
            "iter = 180\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 30 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 181\n",
            "\n",
            "Training grid AUGMENT size: (13,31) from (8,23)\n",
            "iter = 181\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 31 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 182\n",
            "\n",
            "Training grid AUGMENT size: (17,21) from (8,20)\n",
            "iter = 182\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 32 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 183\n",
            "\n",
            "Training grid AUGMENT size: (18,38) from (8,28)\n",
            "iter = 183\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 33 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 184\n",
            "\n",
            "Training grid AUGMENT size: (23,28) from (9,25)\n",
            "iter = 184\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 34 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 185\n",
            "\n",
            "Training grid AUGMENT size: (11,53) from (9,21)\n",
            "iter = 185\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 35 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 186\n",
            "\n",
            "Training grid AUGMENT size: (16,31) from (11,23)\n",
            "iter = 186\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 36 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 187\n",
            "\n",
            "Training grid AUGMENT size: (19,31) from (7,26)\n",
            "iter = 187\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 37 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 188\n",
            "\n",
            "Training grid AUGMENT size: (9,17) from (6,17)\n",
            "iter = 188\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 38 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 189\n",
            "\n",
            "Training grid AUGMENT size: (11,32) from (7,32)\n",
            "iter = 189\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 39 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 190\n",
            "\n",
            "Training grid AUGMENT size: (23,26) from (11,21)\n",
            "\t >>time per step: 10.76s <<\n",
            "len(data_input_flat): 598\n",
            "indexes: [  7  12  17  34  38  42  60  62  64  66  69 107 132 134 136 139 142 144\n",
            " 147 151 161 167 185 191 194 200 201 205 206 209 212 215 224 237 242 262\n",
            " 263 265 268 271 276 287 296 300 309 310 314 339 342 345 361 367 387 391\n",
            " 395 414 417 431 435 440 441 444 445 463 470 495 499 509 515 519 521 525\n",
            " 529 534 581 588 591]\n",
            "len(data_input_flat): 598\n",
            "indexes: [  3   6   8  14  36  39  57  58  63  66  85  89 105 109 121 124 130 134\n",
            " 142 148 157 158 159 160 161 173 174 175 176 183 184 185 186 188 190 193\n",
            " 196 199 200 201 208 210 212 216 226 234 236 237 240 241 252 263 288 289\n",
            " 316 320 330 339 341 363 365 368 376 383 391 395 408 409 418 460 461 469\n",
            " 486 525 527 532 533 537 541 553 556 560 580 583 586 588 591]\n",
            "len(data_input_flat): 598\n",
            "indexes: [ 24 164 165 166 190 194 195 216 218 237 242 250 264 265 289 290 292 294\n",
            " 296 319 320 323 325 341 345 346 365 392 400 402 405 409 419 443 469 495\n",
            " 497 499 504 505 521 522 527 530 532 578 584]\n",
            "len(data_input_flat): 598\n",
            "indexes: [  7  10  11  13  14  16  19  35  37  40  43  62  64  67  87  91 122 127\n",
            " 128 129 148 149 169 173 177 179 180 184 190 196 199 201 210 230 231 232\n",
            " 233 236 237 241 257 258 264 269 271 273 275 280 289 308 310 311 313 317\n",
            " 319 335 339 360 363 391 399 415 421 438 504 509 521 522 524 527 536 537\n",
            " 544 545 547 550 553 562 570 573 576 579 588 596]\n",
            "\n",
            "Iter: 190/200, total loss: 1.6051, model loss: 1.3917, regularization loss: 0.2134\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607 >140:1.610 >150:1.605 >160:1.606 >170:1.605 >180:1.605 >190:1.605\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000 >180:0.000 >190:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000 >180:0.000 >190:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000 >140:1.000 >150:1.000 >160:1.000 >170:1.000 >180:1.000 >190:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00 >140:0.88 >150:1.00 >160:1.00 >170:1.00 >180:1.00 >190:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00 >140:1.00 >150:1.00 >160:1.00 >170:1.00 >180:1.00 >190:1.00\n",
            "VALIDATION Statistic 190(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "iter = 190\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 40 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "ITER: 191\n",
            "\n",
            "Training grid AUGMENT size: (18,19) from (11,19)\n",
            "iter = 191\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 41 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 1 \n",
            "ITER: 192\n",
            "\n",
            "Training grid AUGMENT size: (29,32) from (9,23)\n",
            "iter = 192\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 42 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 2 \n",
            "ITER: 193\n",
            "\n",
            "Training grid AUGMENT size: (11,58) from (9,24)\n",
            "iter = 193\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 43 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 3 \n",
            "ITER: 194\n",
            "\n",
            "Training grid AUGMENT size: (19,27) from (10,19)\n",
            "iter = 194\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 44 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 4 \n",
            "ITER: 195\n",
            "\n",
            "Training grid AUGMENT size: (9,21) from (5,16)\n",
            "iter = 195\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 45 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 5 \n",
            "ITER: 196\n",
            "\n",
            "Training grid AUGMENT size: (16,25) from (7,20)\n",
            "iter = 196\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 46 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 6 \n",
            "ITER: 197\n",
            "\n",
            "Training grid AUGMENT size: (15,45) from (7,19)\n",
            "iter = 197\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 47 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 7 \n",
            "ITER: 198\n",
            "\n",
            "Training grid AUGMENT size: (32,16) from (11,16)\n",
            "iter = 198\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 48 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 8 \n",
            "ITER: 199\n",
            "\n",
            "Training grid AUGMENT size: (26,41) from (9,22)\n",
            "iter = 199\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 49 \n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 9 \n",
            "ITER: 200\n",
            "\n",
            "Training grid AUGMENT size: (26,50) from (10,28)\n",
            "\t >>time per step: 10.26s <<\n",
            "len(data_input_flat): 1300\n",
            "indexes: [   2    6   11   15   19   24   28   33   41   47   54   57   62   63\n",
            "   69   72   80  104  113  120  127  152  203  205  217  221  253  259\n",
            "  260  265  266  270  303  310  354  364  371  375  380  386  395  450\n",
            "  451  454  457  460  468  493  496  497  500  507  519  543  546  550\n",
            "  556  566  593  596  600  603  606  613  622  646  678  696  742  776\n",
            "  792  796  829  843  846  866  878  890  897 1070 1078 1116 1124 1132\n",
            " 1162 1170 1176 1185 1214 1225 1235 1266 1282]\n",
            "len(data_input_flat): 1300\n",
            "indexes: [  22   37   76   78   82  122  123  131  135  169  177  186  311  318\n",
            "  330  336  362  369  380  386  412  424  427  432  509  515  522  528\n",
            "  546  571  645  673  696  717  723  745  769  770  774  792  824  829\n",
            "  834  838  846  868  881  918  985 1027 1208]\n",
            "len(data_input_flat): 1300\n",
            "indexes: [   4   62  102  120  158  165  221  260  334  342  343  348  349  391\n",
            "  398  399  441  574  575  618  621  622  623  627  670  674  675  676\n",
            "  678  679  720  721  722  724  728  771  772  774  819  820  834  835\n",
            "  870  874  885  886  924  933  935  984  985  990 1001 1004 1006 1010\n",
            " 1034 1051 1052 1053 1059 1084 1105 1141 1143 1151 1152 1153 1154 1198\n",
            " 1200 1201 1202 1203 1251 1252]\n",
            "len(data_input_flat): 1300\n",
            "indexes: [  49  141  213  214  219  260  263  265  267  271  273  312  314  317\n",
            "  320  335  367  371  372  384  385  413  416  419  463  465  466  470\n",
            "  492  506  507  510  540  556  558  560  567  587  595  606  608  610\n",
            "  613  615  616  665  673  686  713  714  723  724  725  763  767  769\n",
            "  773  785  786  814  864  866  867  882  900  906  908  912  914  919\n",
            "  942  943  965  966  996 1015 1033 1064 1085 1114 1206 1213 1263]\n",
            "\n",
            "Iter: 200/200, total loss: 1.6037, model loss: 1.3906, regularization loss: 0.2131\n",
            "LOSS CURVE: 0:4.676 >10:2.915 >20:2.217 >30:1.892 >40:1.790 >50:1.787 >60:1.676 >70:1.641 >80:1.627 >90:1.634 >100:1.619 >110:1.614 >120:1.614 >130:1.607 >140:1.610 >150:1.605 >160:1.606 >170:1.605 >180:1.605 >190:1.605 >200:1.604\n",
            "TRAINING ACC CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000 >180:0.000 >190:0.000 >200:0.000\n",
            "TRAINING ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 1.000 / 0.000 (1.000)\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  93  291  344  550  584  599  606  754  791  799  804  835  839 1035\n",
            " 1048 1058 1064 1404 1464 1533 1537 1543 1550 1656 1729 1735 1743 1751\n",
            " 1784 1789 2210 2217 2233 2475 2681 2722 2729 2871 2948 2958 2972 3463\n",
            " 3478 3678 3727 3934 3983]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  28  207  213  219  227  234  349  360  399  403  717  725  966  982\n",
            " 1095 1123 1160 1179 1208 1368 1382 1555 1562 1571 1579 1800 1806 1813\n",
            " 1822 1830 1837 1843 1846 1852 1995 2008 2270 2275 2455 2489 2497 2505\n",
            " 2689 2695 2707 2718 2729 2736 2744 2952 2964 3200 3207 3258 3463 3513\n",
            " 3718 3770 3975 4015 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39  340  348  373  381 1039 1048 1059 1156 1231 1235 1240 1246 1258\n",
            " 1261 1265 1361 1367 1371 1390 1394 1521 1603 1696 1797 1807 1813 1923\n",
            " 1948 2002 2080 2128 2135 2227 2237 2265 2284 2308 2382 2388 2498 2509\n",
            " 2513 2516 2519 2522 2525 2528 2530 2654 2672 2784 2838 2848 2995 3022\n",
            " 3029 3038 3149 3155 3164 3169 3174 3180 3443 3471 3598 3602 3636 3745\n",
            " 3917 3955 4044 4050 4054 4059 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  47   76   96  204  213  223  233  355  409  541  621  644  669  772\n",
            "  782  794  900  929  944  971 1052 1095 1105 1221 1232 1246 1349 1359\n",
            " 1380 1486 1500 1540 1627 1670 1798 1819 1926 1946 1957 2217 2228 2248\n",
            " 2500 2822 3274 3481 3495 3590 3597 3702 3707 3835 3849 3859 3870 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  17   25  102  418  425  431  469  541  550  669  675  791  848  860\n",
            " 1108 1112 1117 1122 1126 1142 1165 1172 1255 1263 1270 1419 1424 1429\n",
            " 1463 1483 1491 1496 1508 1527 1547 1557 1566 1676 1741 1784 1801 1875\n",
            " 1883 1931 1937 1943 1977 2004 2059 2184 2297 2308 2362 2501 2618 2756\n",
            " 2774 3011 3019 3025 3035 3042 3047 3052 3059 3066 3139 3145 3408 3430\n",
            " 3553 3937]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23  200  263  984  993 1051 1238 1250 1447 1472 1492 1502 1678 1683\n",
            " 1694 1708 1744 1761 1775 1869 1886 1898 1901 1918 2187 2191 2197 2221\n",
            " 2324 2329 2455 2519 2635 2638 2645 2670 2772 2776 2902 2990 3019 3022\n",
            " 3029 3094 3310 3352 3357 3419 3438 3544 3550 3567 3673 3678 3695 3758\n",
            " 3800 3806 4052]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25  147  151  157  230  262  270  301  366  371  387  393  502  628\n",
            "  842 1016 1029 1038 1158 1167 1208 1284 1290 1360 1367 1481 1492 1603\n",
            " 1610 1616 1657 1927 1977 2106 2115 2427 2435 2447 2463 2812 2819 2837\n",
            " 2959 3014 3546 3554 3748 3803 3991 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  80  100  178  282  291  405  414  485  492  608  838  849 1034 1048\n",
            " 1361 1372 1401 1410 1417 1546 1552 1560 1569 1594 1602 1737 1745 1786\n",
            " 1794 2119 2170 2363 2371 2683 2756 3123 3141 3160 3663 3675 3716 3984]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  87  101  223  416  533  564  574  646  657  668  677  685  759  854\n",
            "  867  974  993 1008 1120 1220 1296 1333 1412 1419 1460 1469 1668 1860\n",
            " 2116 2124 2132 2141 2163 2172 2247 2260 2291 2299 2382 2437 2491 2563\n",
            " 2572 2611 2619 2693 2702 2709 2716 2749 2821 2830 2837 2844 2874 2878\n",
            " 3337 3371 3383 3615 3790 3797 3803 3812 3820 3827 3917 3923 3931 3938\n",
            " 3949 4048 4082]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22   25   30   35   40   66  255  344  787  796  805  912  917  923\n",
            "  930  935 1042 1049 1055 1060 1149 1234 1256 1264 1269 1277 1286 1294\n",
            " 1341 1349 1355 1392 1397 1606 1615 1651 1733 1741 1747 1779 1853 1858\n",
            " 1860 1867 1876 1883 1907 1938 1971 1991 2053 2057 2065 2180 2185 2192\n",
            " 2311 2322 2355 2436 2442 2564 2571 2578 2586 2612 2691 2699 2740 2814\n",
            " 2820 2868 3070 3076 3123 3202 3252 3331 3380 3603 3608 3612 3682 4074\n",
            " 4079 4083 4088]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  88   98  105  282  289  295  468  478  485  492  672  982  996 1006\n",
            " 1223 1230 1418 1433 1602 1613 1622 1628 1658 1794 1806 1816 1822 1850\n",
            " 1985 1995 2002 2007 2013 2021 2042 2374 2426 2563 2619 3012 3067 3396\n",
            " 3413 3716 3722 3736 3743 3791 3817 3827 3833 3906 3977 3984 3988 3992\n",
            " 3999 4005 4011]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  41   46   95  539  668  922  929  981 1041 1435 1447 1487 1492 1557\n",
            " 1564 1571 1631 1635 1684 1689 1759 1869 1950 1973 1977 1984 2028 2034\n",
            " 2041 2102 2295 2421 2426 2552 2809 3002 3129 3942 4093 4095]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  112  145  239  245  252  276  286  295  332  423  478  844  884\n",
            "  901  981  987  992 1094 1098 1103 1109 1113 1173 1193 1221 1272 1399\n",
            " 1411 1421 1431 1441 1580 1592 1606 1615 1691 1802 1812 1818 1944 1976\n",
            " 2136 2168 2389 2419 2763 2819 2947 2953 2960 3041 3139 3145 3152 3157\n",
            " 3233 3331 3337 3343 3349 3425 3853]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  160  167  228  275  285  358  365  403  415  486  493  601  613\n",
            "  838  850  947  964 1022 1047 1130 1156 1163 1172 1266 1275 1287 1298\n",
            " 1413 1466 1485 1878 1887 1959 2048 2055 2069 2129 2171 2247 2259 2364\n",
            " 2683 2877 3259 3644 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94   99  238  282  289  340  424  478  531  537  613  620  729  807\n",
            "  921  929 1053 1111 1265 1275 1286 1447 1460 1468 1476 1489 1684 1688\n",
            " 1732 1741 1914 1924 1928 1937 2106 2116 2120 2129 2363 2372 2377 2386\n",
            " 2575 2583 3037 3067 3290 3324 3516 3549 3857 3897]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   9  350  913  924  938 1042 1055 1067 1112 1124 1130 1247 1365 1382\n",
            " 1558 1569 1579 1670 1679 1685 1801 1817 2007 2040 2059 2123 2135 2168\n",
            " 2232 2249 2255 2263 2361 2379 2391 2454 2489 2506 2682 2694 2811 2819\n",
            " 3067 3076 3216 3230 3269 3276 3287 3480 3508 3523 3532 3681 3985 3998\n",
            " 4035]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  13  206  780  938 1161 1320 1326 1333 1366 1378 1567 1574 1580 1612\n",
            " 1621 1650 1655 1665 1688 1696 1702 1708 1729 1747 1849 1887 1922 1936\n",
            " 1940 1946 1977 1997 2014 2064 2069 2106 2128 2134 2142 2184 2247 2248\n",
            " 2249 2298 2311 2315 2347 2353 2425 2457 2466 2478 2492 2610 2617 2668\n",
            " 2685 2788 2795 2873 2909 2915 2920 2964 3021 3126 3164 3172 3178 3219\n",
            " 3244 3302 3370 3376 3426 3431 3498 3505 3547 3555 3630 3679 3684 3756\n",
            " 3806 3811 3815 3879 3883 3949 3952 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   8  988 1175 1189 1360 1365 1374 1384 1437 1565 1813 1832 1908 1914\n",
            " 1925 1934 2013 2017 2024 2069 2309 2321 2569 2581 2584 2696 2707 2824\n",
            " 2829 2831 2834 2839 2895 2958 2966 3016 3025 3083 3154 3453 3581 3588\n",
            " 3594 3597 3896 3911 3918 4027]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  52  237  299  365  721  742  926 1036 1045 1162 1173 1188 1192 1195\n",
            " 1198 1202 1225 1230 1242 1247 1265 1268 1374 1420 1494 1511 1520 1566\n",
            " 1673 1678 1686 1695 1708 1737 1743 1750 1771 1865 1870 1878 1884 1899\n",
            " 1929 1999 2008 2015 2022 2027 2057 2060 2066 2076 2080 2092 2185 2190\n",
            " 2200 2210 2220 2248 2254 2264 2284 2376 2380 2386 2394 2399 2412 2575\n",
            " 2603 2700 2732 2828 2840 2859 2867 3030 3043 3051 3153 3161 3169 3174\n",
            " 3179 3286 3297 3305 3407 3425 3440 3542 3554 3860 3870 3879 4047 4052\n",
            " 4054 4063 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   7   25   51  145  150  156  164  268  276  284  291  297  405  415\n",
            "  645  654  686  693  795  964  970  976 1007 1013 1092 1099 1134 1141\n",
            " 1227 1240 1254 1267 1345 1351 1360 1396 1472 1477 1486 1524 1794 1801\n",
            " 1843 1922 1926 1972 2094 2101 2158 2165 2246 2254 2287 2500 2515 2528\n",
            " 2537 2546 2707 2720 2728 3031 3039 4089]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  207  216  335  342  348  464  473  706  778  857 1000 1029 1047\n",
            " 1169 1176 1182 1299 1319 1353 1478 1563 1575 1609 1618 1703 1744 1753\n",
            " 1895 2005 2023 2069 2151 2279 2407 2727 3041 3048 3454 3517 3586 3589\n",
            " 3592 3596 3599 3601 3717 3726 3739 3854 3870 3976 3987 4001]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   3   86  143  201  541  555  588  606  611  679  690  730  732  737\n",
            "  749  803  925  979  986 1125 1377 1433 1528 1539 1555 1579 1586 1816\n",
            " 1824 1844 1973 2006 2154 2231 2264 2284 2518 2708 2711 2717 2796 2801\n",
            " 2810 2901 2909 3143 3156 3161 3348 3358 3540 3544 3547 3550 3554 3668\n",
            " 3673 3907 4067 4074]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  337  340  344  348  367  371  495  499  532  538  543  657  664\n",
            "  690  855  905 1058 1235 1239 1289 1440 1508 1684 1689 1691 1713 1725\n",
            " 1792 1817 1853 1875 2002 2006 2134 2138 2194 2389 2419 2641 2659 2667\n",
            " 2674 2711 2837 2930 2963 3123 3153 3158 3210 3269 3402 3473 3506 3874\n",
            " 3994 3999 4005 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  12   16  671 1430 1817 1873 2108 2124 2244 2254 2299 2437 2446 2711\n",
            " 2720 2726 2877 2883 3193 3204 3219 3525 3534 3579 3859 3865 3871 3880\n",
            " 3984 3991 3995 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  65  730  741  746  841  868 1149 1193 1199 1200 1288 1445 1450 1458\n",
            " 1489 1493 1501 1532 1564 1575 1582 1617 1622 1691 1701 1705 1711 1746\n",
            " 1750 1789 1878 1899 1957 1971 2001 2006 2076 2085 2092 2100 2131 2181\n",
            " 2201 2206 2213 2220 2229 2258 2301 2328 2341 2348 2356 2385 2483 2549\n",
            " 2675 2748 3203 3210 3317 3329 3333 3337 3410 3620 4000 4005 4011 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  38   85   92  220  226  227  231  274  279  413  556  592  606 1091\n",
            " 1097 1104 1244 1251 1262 1283 1295 1389 1413 1424 1605 1612 1924 1931\n",
            " 2053 2059 2245 2251 2373 2377 2390 2501 2569 2576 2583 2693 2699 2705\n",
            " 3223 3230 3424 3495 3508 3566 3616 3624 3928 3966 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  27   37  163  168  176  182  216  223  230  235  431  437  456  462\n",
            "  531  542  550  559  563  565  571  719  723  822  846  851  856  860\n",
            "  950  977  985 1102 1109 1115 1119 1142 1366 1491 1494 1502 1622 1742\n",
            " 1752 1781 1879 1935 1952 2007 2037 2128 2135 2261 2384 2396 2421 2509\n",
            " 2551 2636 2668 2764 2807 2892 2998 3207 3221 3245 3463 3473 3544 3552\n",
            " 3560 3572 3631 3668 3675 3684 3695 3876 3887 4003 4015 4079]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  34  219  225  232  343  350  356  362  472  478  486  712  720  732\n",
            " 1033 1041 1287 1297 1480 1493 1501 1737 1744 1850 1926 1932 1940 1978\n",
            " 2054 2059 2069 2106 2390 2489 2584 2588 2617 2704 2776 2809 2901 3002\n",
            " 3124 3154 3160 3164 3322 3329 3332 3401 3513 4059 4069]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  94  105  179  538  548  618  689  732  739  799  805  812  904  917\n",
            "  987  993 1029 1041 1080 1085 1129 1139 1208 1214 1222 1228 1360 1371\n",
            " 1403 1424 1499 1555 1566 1628 1682 1733 1808 1819 1849 1936 1946 1999\n",
            " 2009 2128 2137 2181 2187 2196 2257 2267 2276 2295 2309 2315 2322 2326\n",
            " 2423 2437 2443 2450 2458 2468 2565 2571 2577 2582 2615 2679 2781 2871\n",
            " 2907 2935 2936 3034 3063 3146 3417 3442 3545 3594 3631 3673 3884 3989\n",
            " 4012 4053]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 130  143  187  217  231  257  339  344  352  361  476  598  605  673\n",
            "  678  965  972 1093 1111 1203 1225 1283 1309 1314 1332 1338 1432 1442\n",
            " 1618 1715 1795 1802 1844 1929 1937 1987 2036 2115 2122 2135 2164 2307\n",
            " 2317 2356 2499 2506 2514 2548 2691 2698 2708 2740 2853 2859 2868 2883\n",
            " 2887 2896 2909 3252 3276 3445 3464 3589 3633 3664 3982 3990 3997 4006]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 212  545  858  870  981  989  998 1124 1130 1172 1181 1301 1315 1543\n",
            " 1550 1561 1573 1579 1603 1669 1672 1677 1683 1753 1763 1776 1797 1808\n",
            " 2242 2251 2265 2294 2369 2374 2381 2487 2561 2567 2577 2588 2614 2688\n",
            " 2693 2703 2714 2807 2880 2894 2935 2948 3315 3424 3447 3621 3640 3822\n",
            " 3827 3830 3832 3985 4004 4018 4028]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89  103  213  227  234  243  645  654  662  670  774  783  789  795\n",
            "  899  911 1029 1036 1046 1059 1067 1222 1230 1349 1359 1374 1381 1390\n",
            " 1655 1669 1682 1704 1797 1832 1848 1951 2104 2120 2297 2307 2466 2485\n",
            " 2517 2808 2822 2838 3115 3141 3152 3162 3495 3652 3659 3665 3673 3974\n",
            " 3982 3987]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  98  149  183  235  276  289  294  325  421  428  467  478  923  935\n",
            "  979 1066 1117 1169 1309 1315 1620 1651 1674 1678 1802 1807 1817 1828\n",
            " 1843 1994 2001 2012 2020 2035 2185 2193 2206 2313 2321 2331 2465 2702\n",
            " 2897 3213 3342 3807 3856 3928]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  23   28   33   39  144  149  159  164  169  279  290  294  413  541\n",
            "  795  801  976  985  992 1010 1032 1160 1169 1354 1361 1478 1482 1492\n",
            " 1550 1608 1738 1755 1780 1810 1937 1943 2121 2131 2144 2164 2316 2321\n",
            " 2508 2512 2760 2767 2776 2783 2791 2805 2896 3237 3244 3253 3426 3446\n",
            " 3625 3639 3683 3691 3699 3786 3823 3834 3928 3984 3999 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 291  393 1753 2675 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   97  112  337  349  367  475  488  496  611  730  748  900  909\n",
            " 1155 1163 1184 1191 1308 1322 1436 1449 1679 1752 1760 1765 1836 1867\n",
            " 1915 1997 2006 2128 2136 2256 2264 2384 2391 2509 2517 2573 2578 2587\n",
            " 2692 2698 2706 2714 2747 2828 2834 2957 2964 3085 3094 3202 3219 3229\n",
            " 3236 3258 3292 3305 3534 3578 3657 3707 3766 3781 3794 3802 4027 4041\n",
            " 4050]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 141  160  179  401  413  423  432  665  675  683  864 1245 1257 1478\n",
            " 1489 1737 1753 2113 2132 2186 2298 2369 2377 2385 2491 2822 2938 3074\n",
            " 3130 3523 3577 3971 3979 3989 4064]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76   83   93  108  345  366  837  848  894  947 1170 1219 1226 1271\n",
            " 1407 1412 1420 1461 1685 1729 1734 1740 1758 1921 1926 1935 2118 2305\n",
            " 2311 2323 2336 2497 2500 2508 2689 2694 2703 2747 2881 2887 2900 2912\n",
            " 3203 3211 3257 3321 3395 3400 3515 3843 3961 3966]\n",
            "VALIDATION ACC (STRICT) CURVE: 0:0.000 >10:0.000 >20:0.000 >30:0.000 >40:0.000 >50:0.000 >60:0.000 >70:0.000 >80:0.000 >90:0.000 >100:0.000 >110:0.000 >120:0.000 >130:0.000 >140:0.000 >150:0.000 >160:0.000 >170:0.000 >180:0.000 >190:0.000 >200:0.000\n",
            "VALIDATION ACC (SOFT) CURVE: 0:0.368 >10:0.737 >20:0.842 >30:0.579 >40:0.974 >50:0.868 >60:1.000 >70:1.000 >80:1.000 >90:1.000 >100:1.000 >110:1.000 >120:1.000 >130:1.000 >140:1.000 >150:1.000 >160:1.000 >170:1.000 >180:1.000 >190:1.000 >200:1.000\n",
            "TRAINING RECALL CURVE: 0:0.62 >10:1.00 >20:1.00 >30:1.00 >40:1.00 >50:1.00 >60:0.88 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:0.88 >130:1.00 >140:0.88 >150:1.00 >160:1.00 >170:1.00 >180:1.00 >190:1.00 >200:1.00\n",
            "VALIDATION RECALL CURVE: 0:0.50 >10:0.83 >20:0.89 >30:0.70 >40:0.99 >50:0.91 >60:1.00 >70:1.00 >80:1.00 >90:1.00 >100:1.00 >110:1.00 >120:1.00 >130:1.00 >140:1.00 >150:1.00 >160:1.00 >170:1.00 >180:1.00 >190:1.00 >200:1.00\n",
            "VALIDATION Statistic 200(0) (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.500 / 0.000 (0.368) \n",
            "\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 101  357  368  413  426  501  527  723  820  871  925  933  939 1009\n",
            " 1042 1110 1116 1249 1362 1438 1554 1562 1572 1619 1628 1659 1700 1712\n",
            " 1742 1754 1764 1851 1881 1892 1946 1952 2026 2066 2078 2087 2128 2138\n",
            " 2215 2234 2258 2265 2275 2313 2318 2347 2372 2388 2405 2423 2439 2521\n",
            " 2534 2543 2585 2615 2667 2833 2892 2935 3063 3086 3098 3183 3267 3275\n",
            " 3348 3356 3367 3399 3445 3475 3483 3489 3522 3568 3602 3616 3619 3621\n",
            " 3653 3683 3693 3707 3727 3739 3810 3821 3987 4071]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   31   43  149  158  167  544  553  681  693  709  724  820  837\n",
            "  874  918 1028 1309 1427 1438 1465 1483 1592 1612 1617 1628 1810 1818\n",
            " 1894 1913 1932 2060 2105 2213 2233 2251 2258 2267 2379 2389 2400 2425\n",
            " 2553 2571 2579 2592 2699 2706 2721 2745 2890 2903 2938 3018 3090 3131\n",
            " 3210 3217 3225 3232 3240 3258 3608 3637 3741 3830 3997 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  83   88   98  150  221  228  275  284  290  296  344  353  436  465\n",
            "  493  519  630  646  652  658  688  710  752  781  899  903  909  916\n",
            "  948 1033 1041 1091 1099 1108 1141 1154 1163 1205 1282 1288 1333 1346\n",
            " 1352 1357 1397 1426 1434 1474 1480 1487 1525 1609 1617 1671 1782 1794\n",
            " 1796 1801 1811 1860 1865 1874 1975 1985 1992 2001 2049 2054 2060 2066\n",
            " 2103 2180 2190 2237 2370 2378 2422 2498 2503 2551 2626 2630 2680 2886\n",
            " 2892 2935 3145 3678 3926 3939 3956 4055 4065]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  19   32   86  147  160  212  285  339  347  403  407  412  537  711\n",
            "  716  721  759  841  900  918  937  942  987 1030 1036 1060 1066 1094\n",
            " 1214 1221 1224 1229 1234 1321 1348 1352 1356 1361 1365 1449 1476 1481\n",
            " 1487 1577 1604 1610 1619 1705 1732 1736 1742 1748 1834 1859 1864 1961\n",
            " 2115 2123 2131 2214 3534 3541 3561 3587 3610 3616 3732 3739 3745 3753\n",
            " 3980 4057]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  75   92  138  145  194  267  274  322  330  476  583  598  612  651\n",
            "  662  796  842 1111 1117 1175 1182 1265 1312 1329 1376 1634 1713 1824\n",
            " 1841 1888 1907 2016 2032 2080 2208 2301 2783 2975 3057 3431 3606 3679\n",
            " 3726 3734 3751 3868 3872 3919 3926 3943 3990 3999 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  15  142  151  157  171  277  285  294  344  353  360  411  484  601\n",
            "  658  672  681  856  870  920  931  937 1006 1049 1060 1067 1209 1219\n",
            " 1398 1403 1412 1419 1431 1589 1593 1606 1737 1745 1753 1782 1787 1849\n",
            " 1852 1860 1867 1873 1878 2040 2051 2242 2251 2314 2324 2335 2341 2372\n",
            " 2525 2536 2544 2576 2627 2637 2646 2650 2658 2669 2677 2683 2722 2777\n",
            " 2830 2842 2850 2856 2868 2877 2884 2929 2935 2941 2947 2957 2967 2974\n",
            " 2983 3034 3045 3053 3061 3067 3076 3084 3090 3145 3156 3165 3176 3187\n",
            " 3194 3279 3288 3293 3302 3312 3355 3361 3366 3375 3384 3390 3394 3399\n",
            " 3410 3474 3484 3496 3590 3603 3617 3627 3638 3702 3715 3724 3730 3737\n",
            " 3750 3774 3910 3919 3923 3932 3943 3952 3963 4063]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  95  323  364  375  380  571  580  827 1033 1074 1083 1227 1266 1276\n",
            " 1299 1482 1487 1492 1521 1531 1739 1748 1752 1778 1787 1930 2003 2033\n",
            " 2044 2181 2300 2436 2454 2509 2556 2690 2806 2811 3098 3111 3396 3586\n",
            " 3592 3599 3605 3780 3793 3976]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  56  650  734  948 1358 1617 1927 2044 2340 2349 2438 2651 3158 3993\n",
            " 4008]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  35  139  150  164  175  180  295  345  464  470  479  489  598  606\n",
            "  615  726  734  743  846  852  859  873  931  978 1046 1097 1193 1199\n",
            " 1208 1219 1232 1317 1348 1357 1366 1456 1467 1479 1491 1605 1636 1657\n",
            " 1779 1786 1796 1810 1913 2051 2059 2070 2090 2098 2107 2233 2360 2377\n",
            " 2489 2502 2512 2522 2618 2628 2637 2647 2745 2758 2932 2954 3078 3088\n",
            " 3097 3192 3215 3238 3270 3398 3409 3514 3588 3728 3905 3909 3917 3923\n",
            " 4039]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24  408  418 2755 2763 2776 2792 2804 2812 3192 3199 3528 3705 4060]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  76  540  553  592  672  681  724  786  796  804  811  919  931 1149\n",
            " 1157 1169 1204 1246 1253 1260 1274 1303 1348 1354 1360 1398 1404 1476\n",
            " 1483 1525 1533 1594 1603 1609 1616 1624 1630 1641 1650 1723 1729 1736\n",
            " 1793 1800 1812 1822 1851 1921 1927 1937 1978 2052 2058 2069 2182 2188\n",
            " 2194 2370 2381 2427 2499 2511 2555 2627 2632 2685 2885 2938 3219 3225\n",
            " 3235 3239 3244 3351 3365 3374 3612 3620 3920]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  22  476  480  797  804 1019 1104 1112 1162 1168 1174 1275 1281 1293\n",
            " 1339 1457 1507 1531 1697 1705 1724 1824 1834 1907 1915 2142 2218 2527\n",
            " 2607 2835 2842 2849 2858 2963 3034 3041 3050 3154 3161 3167 3178 3281\n",
            " 3289 3359 3369 3790 3800 3807 3882 3919 3994 4001 4011 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 221  481  490  528  550  604  659  679  720  728  907  912  954  966\n",
            " 1003 1020 1027 1038 1048 1052 1056 1077 1113 1158 1217 1246 1481 1496\n",
            " 1525 1612 1653 1667 1732 1738 1782 1820 1860 1863 1869 1875 1910 1988\n",
            " 1995 2039 2116 2122 2167 2197 2244 2249 2255 2296 2323 2372 2379 2424\n",
            " 2456 2462 2500 2504 2511 2552 2628 2636 2708 2744 2756 2762 2844 2857\n",
            " 2869 2884 2894 2938 3013 3021 3130 3141 3149 3385 3406 3530 3579 3655\n",
            " 3666 3701 4041 4047 4053 4069 4086 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  102  278  287  296  407  480  487  596  612  718  800  815 1031\n",
            " 1044 1052 1058 1222 1230 1237 1248 1253 1263 1354 1372 1672 1679 1692\n",
            " 1701 1705 1729 1810 1821 1862 1867 1986 1992 2113 2120 2305 2311 2320\n",
            " 2497 2504 2625 2631 2978 2987 3006 3169 3196 3318 3332 3342 3356 3655\n",
            " 3670 3848 3855 3870 3976 3998 4047]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  351  407  541  667  720  788  790  794  853  860  920  978  982\n",
            "  989 1114 1176 1183 1294 1297 1302 1369 1376 1422 1493 1496 1622 1676\n",
            " 1740 1750 1869 1873 1878 1941 2007 2010 2255 2271 2765 3023 3281 3291\n",
            " 3294 3327 3340 3422 3453 3468 3472 3475 3478 3483 3519 3599 3709 3727\n",
            " 3917 4033]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29   33   95  108  415  425  574  702 1113 1123 1152 1314 1321 1693\n",
            " 1699 1705 1880 1883 1903 2006 2073 2095 2266 2591 2651 2721 3097 3102\n",
            " 3117 3737 3743 3746 3752 3991 3998 4007]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 146  166  205  250  346  364  788  800  810  819  918  926  934  941\n",
            " 1052 1060 1065 1181 1190 1421 1432 1440 1448 1454 1547 1554 1564 1571\n",
            " 1576 1586 1677 1688 1927 1932 1939 1949 1958 1978 2055 2059 2067 2078\n",
            " 2106 2183 2187 2235 2491 2533 2540 2673 2684 2789 2796 2812 2952 2961\n",
            " 2974 2983 3000 3074 3275 3289 3297 3304 3316 3346 3465 3471 3481 3648\n",
            " 3666 3679 3794 3840 3968 3986]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  92  347 1040 1822 2038 2184 2193 2441 2680 2755 2756 2762 2770 2873\n",
            " 3128 3143 3386 3401 3461 3961 3971]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  102  219  234  344  353  362  474  483  490  609  711  755  765\n",
            "  899  910  934  947  954 1157 1162 1167 1211 1285 1290 1297 1304 1311\n",
            " 1339 1488 1496 1529 1616 1622 1657 1745 1756 1785 1871 1878 1914 2004\n",
            " 2011 2042 2063 2335 2364 2459 2492 2716 2748 2907 2940 3037 3068 3358\n",
            " 3363 3474 3489 3546 3564 3741 3751 3926 4004 4015]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 158  479  733  862  934 1159 1284 1524 2371 2381 2626 2632 2941 2953\n",
            " 3033 3066 3069 3324 3635 3645 3679 3934 3945 4064 4077]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  25   28   31  157  216  221  224  228  349  528  533  552  596  616\n",
            "  619  636  655  658  746  787  823  828  857  879  900  968 1013 1018\n",
            " 1093 1195 1205 1213 1231 1234 1299 1303 1323 1388 1410 1413 1430 1432\n",
            " 1435 1438 1470 1478 1492 1495 1559 1561 1564 1578 1678 1683 1687 1706\n",
            " 1935 1937 1940 1943 1945 1950 1958 2062 2065 2069 2074 2127 2136 2143\n",
            " 2147 2254 2256 2261 2265 2269 2273 2276 2298 2320 2324 2330 2333 2353\n",
            " 2390 2400 2402 2446 2451 2459 2468 2488 2574 2589 2638 2643 2648 2652\n",
            " 2659 2695 2696 2752 2779 2820 2830 2833 2836 2839 2843 2947 2949 2952\n",
            " 3059 3067 3090 3105 3184 3252 3320 3379 3407 3435 3563 3569 3724 3755\n",
            " 3762 3850 3889 3989 3994 4008 4081]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  31  175  333  343  351  357  365  377  475  482  489  600  604  608\n",
            "  618  708  721  732  776  849  904  974  987  994 1000 1011 1018 1104\n",
            " 1115 1127 1133 1139 1144 1226 1239 1353 1365 1478 1511 1613 1683 1691\n",
            " 1721 1854 1934 1941 1951 1977 2061 2067 2075 2106 2187 2197 2209 2235\n",
            " 2381 2386 2399 2429 3345 3842 3849 3853 3854 3858 4039 4051 4084]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  29  104  217  230  234  348  491  716  812  818  830  887  995 1034\n",
            " 1048 1080 1150 1161 1270 1278 1415 1423 1432 1531 1543 1548 1619 1659\n",
            " 1671 1680 1787 1915 2043 2183 2190 2299 2311 2427 2631 2810 3091 3098\n",
            " 3107 3177 3285 3297 3370 3482 3490 3852]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84   90  104  277  279  407  599  723  731  759  836  905  916  926\n",
            "  954 1028 1034 1043 1082 1166 1179 1210 1281 1506 1514 1526 1633 1640\n",
            " 1657 1763 1770 1785 1892 1977 2025 2089 2142 2224 2244 2252 2321 2370\n",
            " 2385 2658 2679 3168 3214 3222 3241 3336 3343 3355 3367 3377 3534 3540\n",
            " 3547 3555 3564 3651 3662 3670 3674 3690 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  348  356  364  599  605  611  618  638  728  737  745  860  868\n",
            " 1041 1099 1109 1119 1125 1130 1165 1227 1232 1233 1236 1292 1409 1493\n",
            " 1674 1684 1803 2128 2135 2217 2271 2275 2278 2282 2385 2409 2511 2534\n",
            " 2538 2665 2728 2837 2889 2894 3150 3158 3273 3280 3286 3358 3363 3371\n",
            " 3676 3681 3939 3992 4009]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  39   82   90  206  212  220  228  236  343  355  363  644  696  773\n",
            "  899  952  958 1083 1092 1100 1221 1229 1233 1532 1541 1797 1803 1852\n",
            " 1922 1980 2243 2300 2562 2620 2696 2745 2959 2973 2992 3031 3047 3154\n",
            " 3159 3166 3177 3277 3285 3292 3300 3311 3406 3412 3419 3433 3601 3610\n",
            " 3618 3626 3726 3737 3754 4050 4058 4068 4076]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 106  153  204  209  300  355  393  407  604  887  995 1034 1041 1052\n",
            " 1192 1330 1337 1859 2617 2743 3066 3573 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   33   85  104  152  156  225  231  434  467  524  567  626  651\n",
            "  657  780  785  946 1271 1290 1297 1464 1480 1485 1591 1674 1682 1802\n",
            " 1812 1848 2233 2250 2426 2438 2875 2886 3320 3340 3357 3791 3942]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  86  227  275  283  418  424  473  595  606  613  623  973  979 1030\n",
            " 1046 1321 1331 1338 1348 1377 1407 1419 1604 1784 1797 1806 1976 1989\n",
            " 1994 2002 2169 2181 2190 2553 2584 2637 2809 2825 2994 3001 3078 3090\n",
            " 3496 3511 3537 3545 3686 3696 3703 3730 3741 3893 3930 3941 3985]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  60  189  224  282  286  293  344  409  413  417  421  509  540  546\n",
            "  671  674  747  751  766  786  791  794  808  858  876  879  894  915\n",
            "  919 1004 1008 1022 1056 1214 1234 1407 1425 1430 1454 1553 1557 1562\n",
            " 1681 1685 1743 1747 1874 1878 1881 2000 2004 2011 2031 2080 2128 2132\n",
            " 2159 2257 2262 2267 2287 2384 2392 2416 2510 2513 2518 2638 2640 2643\n",
            " 2768 2774 2896 2901 2927 3022 3026 3032 3152 3184 3278 3281 3285 3290\n",
            " 3537 3632 3727 3731 3735 3740 3760 3855 3951 4056]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  26   34   60  104  317  457  463  473  546  553  675  701  856  909\n",
            " 1165 1264 1392 1420 1448 1710 1769 1949 1954 2141 2154 2157 2214 2220\n",
            " 2424 2443 2448 2465 2470 2474 2533 2552 2590 2617 2658 2663 2692 2700\n",
            " 2744 2755 2762 2767 2785 2980 3001 3066 3105 3194 3207 3214 3234 3241\n",
            " 3266 3450 3490 3496 3977]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  73   76  143  498  563  673  682  730  737  755  794  871  929  947\n",
            "  986 1053 1059 1113 1147 1282 1339 1410 1412 2432 2470 2600 2665 2792\n",
            " 2907 2920 2971 2978 2984 3099 3106 3115 3170 3177 3291 3306 3355 3363\n",
            " 3483 3491 3497 3547 3626 3636 3674 3676 3683 3740 3751 3764 3811 3819\n",
            " 3892 3931 4003 4010]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 117  124  132  142  153  167  285  295  328  340  451  460  470  613\n",
            "  644  654  667  771  786 1095 1110 1116 1221 1229 1236 1242 1415 1423\n",
            " 1431 1532 1555 1572 1578 1588 1988 1994 2004 2040 2127 2137 2469 2488\n",
            " 2723 2744 2845 2857 2916 2939 3036 3044 3049 3065 3174 3192 3289 3299\n",
            " 3304 3321 3410 3418 3423 3430 3437 3445 3534 3541 3548 3553 3562 3571\n",
            " 3984 4024]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 220  296  303  398  492  548  599  648  690  735  840  889  921  935\n",
            "  944 1030 1040 1268 1286 1467 1657 1670 1689 1709 1798 1850 1926 1978\n",
            " 1996 2008 2063 2130 2161 2310 2362 2490 2500 2508 2632 2643 2682 2869\n",
            " 2876 3002 3012 3336 3350 3383 3542 3566 3591 3729 3740 3747 3753 3761\n",
            " 3852 3858 3865 3890 3936 3942 3948 4052 4062 4068 4075 4083]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [   4   10   16   61   90   98  103  108  117  199  275  283  292  299\n",
            "  390  462  471  479  486  497  649  668  836  848  865 1218 1233 1272\n",
            " 1410 1418 1433 1464 1602 1611 1656 1794 1800 1810 1820 1848 1986 1995\n",
            " 2005 2040 2190 2230 2234 2242 2246 2390 2425 2435 2442 2618 2627 2635\n",
            " 2776 2787 2810 2819 2827 3008 3173 3181 3192 3361 3385 3557 3565 3577\n",
            " 3735 3747 3764 3782 3968 4062 4064 4067 4070 4074 4081 4086 4088 4089\n",
            " 4092]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  89   97  229  276  284  471  482  821  837  848 1012 1019 1027 1209\n",
            " 1221 1228 1555 1594 1604 1612 1750 1786 1794 1801 1807 1943 1978 1992\n",
            " 2130 2170 2182 2492 2502 2552 2690 2747 3131 3139 3481 4094]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 345  845  929 1212 1335 1341 1549 1558 1686 2437 3972]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  57   65  125  189  344  350  354  359  409  412  416  486  538  544\n",
            "  548  746  753  834  874  877  882  937  944  948 1024 1076 1136 1170\n",
            " 1184 1191 1204 1290 1297 1354 1395 1425 1470 1482 1523 1612 1651 1661\n",
            " 1688 1715 1739 1746 1842 1846 1870 1880 1970 1974 1982 1994 2005 2046\n",
            " 2100 2118 2120 2122 2124 2127 2129 2131 2133 2137 2142 2146 2149 2151\n",
            " 2154 2250 2292 2296 2339 2375 2383 2423 2500 2503 2506 2508 2511 2515\n",
            " 2518 2522 2526 2528 2530 2533 2535 2537 2539 2544 2550 2695 2741 2746\n",
            " 3142 3152 3161 3193 3550 3564 3688 3946 3951]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  97  151  294  301  351  395  409  466  482  541  604 1051 1057 1206\n",
            " 1251 1283 1295 1306 1549 1699 1849 1862 1868 1873 1883 2071 2082 2090\n",
            " 2105 2125 2361 2374 2381 2387 2393 2403 2682 2694 2703 2715 2724 3049\n",
            " 3065 3240 3259 3496 3641 3753 3872 3920 3946 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81  103  283  294  306  478  490  664  674  680  687 1031 1042 1050\n",
            " 1413 1422 1438 1447 1627 1708 2067 2083 2247 2255 2361 2503 2510 2522\n",
            " 2553 2695 2702 2715 2745 3160 3198 3340 3588 3596 3608 3993]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  16   32   75  151  202  219  225  280  285  328  412  476  540  787\n",
            "  796  806  855  860  865  915  921  927  933  989 1042 1049 1052 1055\n",
            " 1060 1063 1070 1092 1099 1173 1186 1194 1307 1314 1368 1417 1481 1493\n",
            " 1554 1590 1602 1610 1653 1666 1673 1683 1717 1866 1874 1882 1910 1974\n",
            " 2103 2122 2132 2230 2361 2377 2385 2441 2451 2488 2616 2630 2746 2754\n",
            " 2883 2936 3013 3023 3032 3065 3146 3160 3324 3330 3444 3514 3523 3932\n",
            " 3974 4055 4063 4072]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 149  164  534  542  548  553  733  856  866  872 1046 1059 1276 1332\n",
            " 1351 1496 1540 1547 1666 1671 1978 1988 1996 2003 2006 2012 2123 2130\n",
            " 2139 2170 2184 2298 2316 2491 2810 3195 3323 4004]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [2080]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  32 2948 2957 3279 3314 3598 3616 3626 3640 3938 3975 3990 4009 4013\n",
            " 4029]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  81   90  204  211  220  337  349  457  466  540  547  663  913 1168\n",
            " 1176 1196 1224 1350 1355 1362 1387 1516 1542 1549 1644 1676 1684 1734\n",
            " 1837 1862 1867 1873 1966 1998 2044 2054 2172 2284 2685 3044 3054 3150\n",
            " 3240 3338 3495 3592 3932]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  24   29   83   88  155  159  163  216  220  224  280  285  462  467\n",
            "  487  525  529  615  618  653  657  681  844  909  914  918  921  924\n",
            "  928  999 1102 1110 1130 1194 1227 1232 1258 1366 1386 1433 1451 1560\n",
            " 1578 1802 1805 1869 1876 1885 1894 1898 1900 1994 2027 2122 2126 2134\n",
            " 2155 2252 2283 2390 2410 2475 2520 2584 2602 2817 2956 2986 3051 3082\n",
            " 3255 3306 3338 3408 3448 3449 3518 3545 3560 3577 3594 3765 3804 3810\n",
            " 3814 3864 3893 3938 3992 3998 4002 4058]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  40   82   92  167  218  297  341  352  451  461  564  680  700  712\n",
            "  984 1002 1097 1248 1306 1316 1331 1337 1357 1438 1489 1545 1553 1588\n",
            " 1677 1690 1700 1715 1721 1809 1823 1886 1893 1907 1913 1931 1941 1952\n",
            " 2035 2041 2064 2079 2131 2163 2169 2258 2291 2297 2382 2395 2419 2425\n",
            " 2513 2546 2553 2612 2617 2639 2650 2659 2722 2740 2746 2767 2778 2842\n",
            " 2868 2873 2895 2958 2968 2984 2999 3337 3346 3364 3471 3493 3595 3605\n",
            " 3618 3725 3739 3756 3853 3866 3884 3980 3994 4012]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [  84  265  324  338  345  416  451  462  472  579  592  786  836  901\n",
            " 1155 1414 1458 3979 3995 4021]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 160  172  463  471  716  784  793  911  918  924  932 1165 1236 1260\n",
            " 1267 1295 1311 1432 1487 1739 1746 1754 1825 1833 2180 2427 2520 2531\n",
            " 2638 2683 2777 2853 2904 2914 2953 2959 3003 3009 3278 3330 3387 3394\n",
            " 3581 3595 3604 3613 3906 3922 3978]\n",
            "len(data_input_flat): 4096\n",
            "indexes: [ 165  204  295  303  347  352  400  487  530  540  629  674  682  686\n",
            "  723  775  809  857  859  913 1017 1286 1291 1338 1346 1432 1480 1604\n",
            " 1611 1722 1736 1850 1927 1942 2043 2117 2235 2249 2427 2434 2442 2448\n",
            " 2683 2693 2701 2951 3275 3286 3294 3330 3480 3529 3615 3623 3656 3671\n",
            " 3990 3997 4007]\n",
            "\n",
            " TEST ACC (Recall/Acc): 1.000 / 0.000 (1.000) | highest 0.630 / 0.000 (0.500) \n",
            "\n",
            "TEST ACC (STRICT) CURVE: 0:0.000 >20:0.000 >40:0.000 >60:0.000 >80:0.000 >100:0.000 >120:0.000 >140:0.000 >160:0.000 >180:0.000 >200:0.000\n",
            "TEST ACC (SOFT) CURVE: 0:0.500 >20:0.860 >40:0.900 >60:1.000 >80:1.000 >100:1.000 >120:1.000 >140:0.980 >160:1.000 >180:1.000 >200:1.000\n",
            "TEST RECALL CURVE: 0:0.63 >20:0.92 >40:0.94 >60:1.00 >80:1.00 >100:1.00 >120:1.00 >140:0.99 >160:1.00 >180:1.00 >200:1.00\n",
            "iter = 200\n",
            "params.log_save_step = 10\n",
            "params.ckpt_save_step = 50\n",
            "iter>=params.log_save_step = True\n",
            "iter%params.ckpt_save_step = 0 \n",
            "\n",
            "Checkpoint saved to: checkpoint/ExpressExpense/CUTIE_atrousSPP_d20000c2(r80c80)_iter_200.ckpt\n",
            "\n",
            "params.og_save_step = 10\n",
            "iter%params.log_save_step = 0 \n",
            "Namespace(augment_strategy=1, batch_size=4, c_threshold=0.5, ckpt_path='checkpoint/', ckpt_save_step=50, cols_segment=72, cols_target=64, cols_ulimit=80, data_augmentation_dropout=0.9, data_augmentation_extra=True, data_augmentation_extra_cols=16, data_augmentation_extra_rows=16, dict_path='dict/ExpressExpense', doc_path='ExpressExpenseJson', embedding_size=128, eps=1e-06, f='/root/.local/share/jupyter/runtime/kernel-e40204f8-7f70-44ee-bb7e-e765a7aa5614.json', ghm_bins=30, ghm_momentum=0, hard_negative_ratio=3, iterations=200, learning_rate=0.0001, load_dict=True, load_dict_from_path='dict/', log_disp_step=10, log_path='log/', log_save_step=10, lr_decay_factor=0.1, lr_decay_step=13000, rows_segment=72, rows_target=64, rows_ulimit=80, save_prefix='ExpressExpense', segment_grid=False, test_path='ExpressExpenseJsonTest', test_step=20, update_dict=False, use_cutie2=False, use_ghm=0, validation_step=10, weight_decay=0.0005)\n",
            "'Data rows/cols:11,56'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CWL7T9R5Pjy"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}