{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUTIEPredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWOPk0xhyspYftYPSR4DnV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywsyws/CUTIE/blob/main/CUTIEPredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8FCvV-mKM-J",
        "outputId": "6e48afb9-f565-4636-876b-90da40a75c69"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip3 install tensorflow==1.14"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.19.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.36.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14) (1.12.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4so9rA1cKpoA"
      },
      "source": [
        "# # Mount Google Drive to Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC0XapWpKszj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade9a416-0315-4a5b-c798-d1bdceced93b"
      },
      "source": [
        "# Import libraries\n",
        "from IPython.display import Image, display\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_v2_behavior() \n",
        "\n",
        "import json, re, random, argparse, timeit, cv2\n",
        "from os import chdir, walk, makedirs\n",
        "from os.path import basename, split, join, exists\n",
        "from collections import defaultdict\n",
        "import unicodedata\n",
        "from pprint import pprint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWua9ZGiKv1t"
      },
      "source": [
        "# Set root path for this project\n",
        "root_path = r'/content/gdrive/MyDrive/Colab Notebooks/CUTIE/'\n",
        "# Change working directory\n",
        "chdir(root_path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE_fx8eKfOb5"
      },
      "source": [
        "# Helper functions\n",
        "\n",
        "def vis_bbox(data_loader, file_prefix, grid_table, gt_classes, model_output_val, file_name, bboxes, shape):\n",
        "    data_input_flat = grid_table.reshape([-1])\n",
        "    labels = gt_classes.reshape([-1])\n",
        "    logits = model_output_val.reshape([-1, data_loader.num_classes])\n",
        "    bboxes = bboxes.reshape([-1])\n",
        "    \n",
        "    max_len = 768*2 # upper boundary of image display size \n",
        "    img = cv2.imread(join(file_prefix, file_name))\n",
        "    if img is not None:    \n",
        "        shape = list(img.shape)\n",
        "        \n",
        "        bbox_pad = 1\n",
        "        gt_color = [[255, 250, 240], [152, 245, 255], [119,204,119], [100, 149, 237], \n",
        "                    [192, 255, 62], [119,119,204], [114,124,114], [240, 128, 128], [255, 105, 180]]\n",
        "        inf_color = [[255, 222, 173], [0, 255, 255], [50,219,50], [72, 61, 139], \n",
        "                     [154, 205, 50], [50,50,219], [64,76,64], [255, 0, 0], [255, 20, 147]]\n",
        "        \n",
        "        font_size = 0.5\n",
        "        font = cv2.FONT_HERSHEY_COMPLEX\n",
        "        ft_color = [50, 50, 250]\n",
        "        \n",
        "        factor = max_len / max(shape)\n",
        "        shape[0], shape[1] = [int(s*factor) for s in shape[:2]]\n",
        "        \n",
        "        img = cv2.resize(img, (shape[1], shape[0]))        \n",
        "        overlay_box = np.zeros(shape, dtype=img.dtype)\n",
        "        overlay_line = np.zeros(shape, dtype=img.dtype)\n",
        "        for i in range(len(data_input_flat)):\n",
        "            if len(bboxes[i]) > 0:\n",
        "                x,y,w,h = [int(p*factor) for p in bboxes[i]]\n",
        "            else:\n",
        "                row = i // data_loader.rows\n",
        "                col = i % data_loader.cols\n",
        "                x = shape[1] // data_loader.cols * col\n",
        "                y = shape[0] // data_loader.rows * row\n",
        "                w = shape[1] // data_loader.cols * 2\n",
        "                h = shape[0] // data_loader.cols * 2\n",
        "                \n",
        "            if data_input_flat[i] and labels[i]:\n",
        "                gt_id = labels[i]                \n",
        "                cv2.rectangle(overlay_box, (x,y), (x+w,y+h), gt_color[gt_id], -1)\n",
        "                    \n",
        "            if max(logits[i]) > c_threshold:\n",
        "                inf_id = np.argmax(logits[i])\n",
        "                if inf_id:                \n",
        "                    cv2.rectangle(overlay_line, (x+bbox_pad,y+bbox_pad), \\\n",
        "                                  (x+bbox_pad+w,y+bbox_pad+h), inf_color[inf_id], max_len//768*2)\n",
        "                \n",
        "            #text = data_loader.classes[gt_id] + '|' + data_loader.classes[inf_id]\n",
        "            #cv2.putText(img, text, (x,y), font, font_size, ft_color)  \n",
        "        \n",
        "        # legends\n",
        "        w = shape[1] // data_loader.cols * 4\n",
        "        h = shape[0] // data_loader.cols * 2\n",
        "        for i in range(1, len(data_loader.classes)):\n",
        "            row = i * 3\n",
        "            col = 0\n",
        "            x = shape[1] // data_loader.cols * col\n",
        "            y = shape[0] // data_loader.rows * row \n",
        "            cv2.rectangle(img, (x,y), (x+w,y+h), gt_color[i], -1)\n",
        "            cv2.putText(img, data_loader.classes[i], (x+w,y+h), font, 0.8, ft_color)  \n",
        "            \n",
        "            row = i * 3 + 1\n",
        "            col = 0\n",
        "            x = shape[1] // data_loader.cols * col\n",
        "            y = shape[0] // data_loader.rows * row \n",
        "            cv2.rectangle(img, (x+bbox_pad,y+bbox_pad), \\\n",
        "                          (x+bbox_pad+w,y+bbox_pad+h), inf_color[i], max_len//384)        \n",
        "        \n",
        "        alpha = 0.4\n",
        "        cv2.addWeighted(overlay_box, alpha, img, 1-alpha, 0, img)\n",
        "        cv2.addWeighted(overlay_line, 1-alpha, img, 1, 0, img)\n",
        "        cv2.imwrite('results/' + file_name[:-4]+'.png', img)        \n",
        "        cv2.imshow(\"test\", img)\n",
        "        cv2.waitKey(0)\n",
        "\n",
        "\n",
        "def cal_accuracy(c_threshold, data_loader, grid_table, gt_classes, model_output_val, label_mapids, bbox_mapids):\n",
        "    #num_tp = 0\n",
        "    #num_fn = 0\n",
        "    res = ''\n",
        "    num_correct = 0\n",
        "    num_correct_strict = 0\n",
        "    num_correct_soft = 0\n",
        "    num_all = grid_table.shape[0] * (model_output_val.shape[-1]-1)\n",
        "    for b in range(grid_table.shape[0]):\n",
        "        data_input_flat = grid_table[b,:,:,0].reshape([-1])\n",
        "        labels = gt_classes[b,:,:].reshape([-1])\n",
        "        logits = model_output_val[b,:,:,:].reshape(([-1, data_loader.num_classes]))\n",
        "        label_mapid = label_mapids[b]\n",
        "        bbox_mapid = bbox_mapids[b]\n",
        "        rows, cols = grid_table.shape[1:3]\n",
        "        bbox_id = np.array([row*cols+col for row in range(rows) for col in range(cols)])\n",
        "        \n",
        "        # ignore inputs that are not word\n",
        "        indexes = np.where(data_input_flat != 0)[0]\n",
        "        print(f'len(data_input_flat): {len(data_input_flat)}')\n",
        "        print(f'indexes: {indexes}')\n",
        "        data_selected = data_input_flat[indexes]\n",
        "        labels_selected = labels[indexes]\n",
        "        logits_array_selected = logits[indexes]\n",
        "        bbox_id_selected = bbox_id[indexes]\n",
        "        \n",
        "        # calculate accuracy\n",
        "        #test_classes = [1,2,3,4,5]\n",
        "        #for c in test_classes:\n",
        "        for c in range(1, data_loader.num_classes):\n",
        "            labels_indexes = np.where(labels_selected == c)[0]\n",
        "            logits_indexes = np.where(logits_array_selected[:,c] > c_threshold)[0]\n",
        "            \n",
        "            labels_words = list(data_loader.index_to_word[i] for i in data_selected[labels_indexes])\n",
        "            logits_words = list(data_loader.index_to_word[i] for i in data_selected[logits_indexes])\n",
        "            \n",
        "            label_bbox_ids = label_mapid[c] # GT bbox_ids related to the type of class\n",
        "            logit_bbox_ids = [bbox_mapid[bbox] for bbox in bbox_id_selected[logits_indexes] if bbox in bbox_mapid]            \n",
        "            \n",
        "            #if np.array_equal(labels_indexes, logits_indexes):\n",
        "            if set(label_bbox_ids) == set(logit_bbox_ids): # decide as correct when all ids match\n",
        "                num_correct_strict += 1  \n",
        "                num_correct_soft += 1\n",
        "            elif set(label_bbox_ids).issubset(set(logit_bbox_ids)): # correct when gt is subset of gt\n",
        "                num_correct_soft += 1\n",
        "            try: # calculate prevalence with decimal precision\n",
        "                num_correct += np.shape(np.intersect1d(labels_indexes, logits_indexes))[0] / np.shape(labels_indexes)[0]\n",
        "            except ZeroDivisionError:\n",
        "                if np.shape(labels_indexes)[0] == 0:\n",
        "                    num_correct += 1\n",
        "                else:\n",
        "                    num_correct += 0        \n",
        "            \n",
        "            # show results without the <DontCare> class                    \n",
        "            if b==0:\n",
        "                res += '\\n{}(GT/Inf):\\t\"'.format(data_loader.classes[c])\n",
        "                \n",
        "                # ground truth label\n",
        "                res += ' '.join(data_loader.index_to_word[i] for i in data_selected[labels_indexes])\n",
        "                res += '\" | \"'\n",
        "                res += ' '.join(data_loader.index_to_word[i] for i in data_selected[logits_indexes])\n",
        "                res += '\"'\n",
        "                \n",
        "                # wrong inferences results\n",
        "                if not np.array_equal(labels_indexes, logits_indexes): \n",
        "                    res += '\\n \\t FALSES =>>'\n",
        "                    logits_flat = logits_array_selected[:,c]\n",
        "                    fault_logits_indexes = np.setdiff1d(logits_indexes, labels_indexes)\n",
        "                    for i in range(len(data_selected)):\n",
        "                        if i not in fault_logits_indexes: # only show fault_logits_indexes\n",
        "                            continue\n",
        "                        w = data_loader.index_to_word[data_selected[i]]\n",
        "                        l = data_loader.classes[labels_selected[i]]\n",
        "                        res += ' \"%s\"/%s, '%(w, l)\n",
        "                        #res += ' \"%s\"/%.2f%s, '%(w, logits_flat[i], l)\n",
        "                        \n",
        "                #print(res)\n",
        "    prevalence = num_correct / num_all\n",
        "    accuracy_strict = num_correct_strict / num_all\n",
        "    accuracy_soft = num_correct_soft / num_all\n",
        "    return prevalence, accuracy_strict, accuracy_soft, res.encode(\"utf-8\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMiXK63nMUF3"
      },
      "source": [
        "DEBUG = True # True to show grid as image \n",
        "\n",
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        pass\n",
        " \n",
        "    try:\n",
        "        unicodedata.numeric(s)\n",
        "        return True\n",
        "    except (TypeError, ValueError):\n",
        "        pass \n",
        "    return False\n",
        "\n",
        "class DataLoader():\n",
        "    \"\"\"\n",
        "    grid tables producer\n",
        "    \"\"\"\n",
        "    def __init__(self, params, update_dict=True, load_dictionary=False, data_split=0.75):\n",
        "        self.random = False\n",
        "        # self.data_laundry = False\n",
        "        self.encoding_factor = 1 # ensures the size (rows/cols) of grid table compat with the network\n",
        "        self.classes = ['O', 'TTL']\n",
        "        #self.classes = ['DontCare', 'Table'] # for table\n",
        "        #self.classes = ['DontCare', 'Column0', 'Column1', 'Column2', 'Column3', 'Column4', 'Column5'] # for column\n",
        "        #self.classes = ['DontCare', 'Column']\n",
        "        #self.classes = ['DontCare', 'VendorName', 'VendorTaxID', 'InvoiceDate', 'InvoiceNumber', 'ExpenseAmount', 'BaseAmount', 'TaxAmount', 'TaxRate'] # for Spanish project\n",
        "        \n",
        "        self.doc_path = params.doc_path\n",
        "        # self.doc_test_path = params.test_path\n",
        "        self.use_cutie2 = params.use_cutie2 \n",
        "        # self.text_case = params.text_case \n",
        "        # self.tokenize = params.tokenize\n",
        "        # if self.tokenize:\n",
        "        #     self.tokenizer = tokenization.FullTokenizer('dict/vocab.txt', do_lower_case=not self.text_case)\n",
        "        \n",
        "        # self.rows = self.encoding_factor # to be updated \n",
        "        # self.cols = self.encoding_factor # to be updated \n",
        "        self.segment_grid = params.segment_grid if hasattr(params, 'segment_grid') else False # segment grid into two parts if grid is larger than cols_target\n",
        "        # self.augment_strategy = params.augment_strategy if hasattr(params, 'augment_strategy') else 1 \n",
        "        # self.pm_strategy = params.positional_mapping_strategy if hasattr(params, 'positional_mapping_strategy') else 2 \n",
        "        # self.rows_segment = params.rows_segment if hasattr(params, 'rows_segment') else 72 \n",
        "        # self.cols_segment = params.cols_segment if hasattr(params, 'cols_segment') else 72\n",
        "        self.rows_target = params.rows_target if hasattr(params, 'rows_target') else 64 \n",
        "        self.cols_target = params.cols_target if hasattr(params, 'cols_target') else 64 \n",
        "        # self.rows_ulimit = params.rows_ulimit if hasattr(params, 'rows_ulimit') else 80 # handle OOM, must be multiple of self.encoding_factor\n",
        "        # self.cols_ulimit = params.cols_ulimit if hasattr(params, 'cols_ulimit') else 80 # handle OOM, must be multiple of self.encoding_factor\n",
        "                \n",
        "        # self.fill_bbox = params.fill_bbox if hasattr(params, 'fill_bbox') else False # fill bbox with labels or use one single lable for the entire bbox\n",
        "        \n",
        "        # self.data_augmentation_dropout = params.data_augmentation_dropout if hasattr(params, 'data_augmentation_dropout') else False # TBD: randomly dropout rows/cols\n",
        "        # self.data_augmentation_extra = params.data_augmentation_extra if hasattr(params, 'data_augmentation_extra') else False # randomly expand rows/cols\n",
        "        # self.da_extra_rows = params.data_augmentation_extra_rows if hasattr(params, 'data_augmentation_extra_rows') else 0 # randomly expand rows/cols\n",
        "        # self.da_extra_cols = params.data_augmentation_extra_cols if hasattr(params, 'data_augmentation_extra_cols') else 0 # randomly expand rows/cols\n",
        "        \n",
        "        ## 0> parameters to be tuned\n",
        "        self.load_dictionary = load_dictionary # load dictionary from file rather than start from empty \n",
        "        self.dict_path = params.load_dict_from_path if load_dictionary else params.dict_path\n",
        "        if self.load_dictionary:\n",
        "            self.dictionary = np.load(self.dict_path + 'dictionary.npy', allow_pickle=True).item()\n",
        "            self.word_to_index = np.load(self.dict_path + 'word_to_index.npy', allow_pickle=True).item()\n",
        "            self.index_to_word = np.load(self.dict_path + 'index_to_word.npy', allow_pickle=True).item()\n",
        "            # self.dictionary = np.load(self.dict_path + '_dictionary.npy').item()\n",
        "            # self.word_to_index = np.load(self.dict_path + '_word_to_index.npy').item()\n",
        "            # self.index_to_word = np.load(self.dict_path + '_index_to_word.npy').item()\n",
        "        else:\n",
        "            self.dictionary = {'[PAD]':0, '[UNK]':0} # word/counts. to be updated in self.load_data() and self.update_docs_dictionary()\n",
        "            self.word_to_index = {}\n",
        "            self.index_to_word = {}\n",
        "\n",
        "        self.data_split = data_split # split data to training/validation, 0 for all for validation\n",
        "        self.data_mode = 2 # 0 to consider key and value as two different class, 1 the same class, 2 only value considered\n",
        "        # self.remove_lowfreq_words = False # remove low frequency words when set as True\n",
        "        \n",
        "        self.num_classes = len(self.classes)\n",
        "        # self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 1        \n",
        "        \n",
        "        # TBD: build a special cared dictionary\n",
        "        self.special_dict = {'*', '='} # map texts to specific tokens        \n",
        "        \n",
        "        ## 1.1> load words and their location/class as training/validation docs and labels \n",
        "        self.training_doc_files = self.get_filenames(self.doc_path)\n",
        "        self.training_docs, self.training_labels = self.load_data(self.training_doc_files, update_dict=update_dict) # TBD: optimize the update dict flag\n",
        "        \n",
        "        # polish and load dictionary/word_to_index/index_to_word as file\n",
        "        self.num_words = len(self.dictionary)              \n",
        "        # self.update_word_to_index()\n",
        "        # self.update_docs_dictionary(self.training_docs, 3, self.remove_lowfreq_words) # remove low frequency words and add it under the <unknown> key\n",
        "        \n",
        "        # # save dictionary/word_to_index/index_to_word as file\n",
        "        # np.save(self.dict_path + 'dictionary.npy', self.dictionary)\n",
        "        # np.save(self.dict_path + 'word_to_index.npy', self.word_to_index)\n",
        "        # np.save(self.dict_path + 'index_to_word.npy', self.index_to_word)\n",
        "        # np.save(self.dict_path + 'classes.npy', self.classes)\n",
        "        # sorted(self.dictionary.items(), key=lambda x:x[1], reverse=True)\n",
        "        \n",
        "        # split training / validation docs and show statistics\n",
        "        num_training = int(len(self.training_docs)*self.data_split)\n",
        "        data_to_be_fetched = [i for i in range(len(self.training_docs))]\n",
        "        selected_training_index = data_to_be_fetched[:num_training] \n",
        "        if self.random:\n",
        "            selected_training_index = random.sample(data_to_be_fetched, num_training)\n",
        "        selected_validation_index = list(set(data_to_be_fetched).difference(set(selected_training_index)))\n",
        "        self.validation_docs = [self.training_docs[x] for x in selected_validation_index]\n",
        "        # self.training_docs = [self.training_docs[x] for x in selected_training_index]\n",
        "        self.validation_labels = self.training_labels\n",
        "        # print('\\n\\nDATASET: %d vocabularies, %d target classes'%(len(self.dictionary), len(self.classes)))\n",
        "        # print('DATASET: %d for training, %d for validation'%(len(self.training_docs), len(self.validation_docs)))\n",
        "        \n",
        "        # ## 1.2> load test files\n",
        "        # self.test_doc_files = self.get_filenames(params.test_path) if hasattr(params, 'test_path') else []\n",
        "        # self.test_docs, self.test_labels = self.load_data(self.test_doc_files, update_dict=update_dict) # TBD: optimize the update dict flag\n",
        "        # print('DATASET: %d for test from %s \\n'%(len(self.test_docs), params.test_path if hasattr(params, 'test_path') else '_'))\n",
        "        \n",
        "        # self.data_shape_statistic() # show data shape static\n",
        "        # if len(self.training_docs) > 0:# adapt grid table size to all training dataset docs \n",
        "        #     self.rows, self.cols, _, _ = self.cal_rows_cols(self.training_docs)  \n",
        "        #     print('\\nDATASHAPE: data set with maximum grid table of ({},{}), updated.\\n'.format(self.rows, self.cols))    \n",
        "        # else:\n",
        "        #     self.rows, self.cols = self.rows_ulimit, self.cols_ulimit\n",
        "                \n",
        "        ## 2> call self.next_batch() outside to generate a batch of grid tables data and labels\n",
        "        # self.training_data_tobe_fetched = [i for i in range(len(self.training_docs))]\n",
        "        self.validation_data_tobe_fetched = [i for i in range(len(self.validation_docs))]        \n",
        "        # self.test_data_tobe_fetched = [i for i in range(len(self.test_docs))]\n",
        "        \n",
        "    \n",
        "    # def update_word_to_index(self):\n",
        "    #     if self.load_dictionary:\n",
        "    #         max_index = len(self.word_to_index.keys())\n",
        "    #         for word in self.dictionary:\n",
        "    #             if word not in self.word_to_index:\n",
        "    #                 max_index += 1\n",
        "    #                 self.word_to_index[word] = max_index\n",
        "    #                 self.index_to_word[max_index] = word            \n",
        "    #     else:   \n",
        "    #         self.word_to_index = dict(list(zip(self.dictionary.keys(), list(range(self.num_words))))) \n",
        "    #         self.index_to_word = dict(list(zip(list(range(self.num_words)), self.dictionary.keys())))\n",
        "    \n",
        "    # def update_docs_dictionary(self, docs, lower_limit, remove_lowfreq_words):\n",
        "    #     # assign docs words that appear less than @lower_limit times to word [UNK]\n",
        "    #     if remove_lowfreq_words: \n",
        "    #         for doc in docs:\n",
        "    #             for line in doc:\n",
        "    #                 [file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "    #                     [image_w, image_h], max_row_words, max_col_words] = line \n",
        "    #                 if self.dictionary[dressed_text] < lower_limit:\n",
        "    #                     line = [file_name, '[UNK]', self.word_to_index['[UNK]'], [x_left, y_top, x_right, y_bottom], \\\n",
        "    #                             [image_w, image_h], max_row_words, max_col_words]\n",
        "    #                     self.dictionary[dressed_text] -= 1\n",
        "    #                     self.dictionary['[UNK]'] += 1\n",
        "    \n",
        "    # def next_batch(self):\n",
        "    #     batch_size = self.batch_size\n",
        "        \n",
        "    #     while True:\n",
        "    #         if len(self.training_data_tobe_fetched) < batch_size:\n",
        "    #             self.training_data_tobe_fetched = [i for i in range(len(self.training_docs))]            \n",
        "    #         selected_index = random.sample(self.training_data_tobe_fetched, batch_size)\n",
        "    #         self.training_data_tobe_fetched = list(set(self.training_data_tobe_fetched).difference(set(selected_index)))\n",
        "    \n",
        "    #         training_docs = [self.training_docs[x] for x in selected_index]\n",
        "            \n",
        "    #         ## data augmentation in each batch if self.data_augmentation==True\n",
        "    #         rows, cols, pre_rows, pre_cols = self.cal_rows_cols(training_docs, extra_augmentation=self.data_augmentation_extra, dropout=self.data_augmentation_dropout)\n",
        "    #         if self.data_augmentation_extra:\n",
        "    #             print('Training grid AUGMENT size: ({},{}) from ({},{})'\\\n",
        "    #                   .format(rows, cols, pre_rows, pre_cols))\n",
        "                \n",
        "                \n",
        "            \n",
        "    #         grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "    #             self.positional_mapping(training_docs, self.training_labels, rows, cols)   \n",
        "    #         if updated_cols > cols:\n",
        "    #             print('Training grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "    #                   .format(rows, updated_cols, rows, cols))\n",
        "    #             grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "    #                 self.positional_mapping(training_docs, self.training_labels, rows, updated_cols, update_col=False)  \n",
        "            \n",
        "    #         ## load image and generate corresponding @ps_1dindices\n",
        "    #         images, ps_1d_indices = [], []\n",
        "    #         if self.use_cutie2:\n",
        "    #             images, ps_1d_indices = self.positional_sampling(self.doc_path, file_names, ps_indices_x, ps_indices_y, updated_cols)   \n",
        "    #             #print(\"image fetched {}\".format(len(images)))          \n",
        "    #             if len(images) == batch_size:\n",
        "    #                 break\n",
        "    #         else:\n",
        "    #             break\n",
        "        \n",
        "    #     batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "    #              'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "    #              'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "    #              'file_name': file_names, 'shape': [rows,cols]}\n",
        "    #     return batch\n",
        "    \n",
        "    def fetch_validation_data(self):\n",
        "        batch_size = 1\n",
        "        \n",
        "        while True:\n",
        "            if len(self.validation_data_tobe_fetched) == 0:\n",
        "                self.validation_data_tobe_fetched = [i for i in range(len(self.validation_docs))]            \n",
        "            selected_index = random.sample(self.validation_data_tobe_fetched, 1)\n",
        "            self.validation_data_tobe_fetched = list(set(self.validation_data_tobe_fetched).difference(set(selected_index)))\n",
        "    \n",
        "            validation_docs = [self.validation_docs[x] for x in selected_index]\n",
        "            \n",
        "            ## fixed validation shape leads to better result (to be verified)\n",
        "            real_rows, real_cols, _, _ = self.cal_rows_cols(validation_docs, extra_augmentation=False)\n",
        "            rows = max(self.rows_target, real_rows)\n",
        "            cols = max(self.rows_target, real_cols)\n",
        "            \n",
        "            grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "                self.positional_mapping(validation_docs, self.validation_labels, rows, cols)   \n",
        "            if updated_cols > cols:\n",
        "                print('Validation grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "                      .format(rows, updated_cols, rows, cols))\n",
        "                grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "                    self.positional_mapping(validation_docs, self.validation_labels, rows, updated_cols, update_col=False)     \n",
        "            \n",
        "            ## load image and generate corresponding @ps_1dindices\n",
        "            images, ps_1d_indices = [], []\n",
        "            if self.use_cutie2:\n",
        "                images, ps_1d_indices = self.positional_sampling(self.doc_path, file_names, ps_indices_x, ps_indices_y, updated_cols)  \n",
        "                if len(images) == batch_size:\n",
        "                    break        \n",
        "            else:\n",
        "                break\n",
        "            \n",
        "        # def build_gt_pyramid(self, gt_classes):\n",
        "        #     gt_classes = np.array(gt_classes)\n",
        "            \n",
        "        #     rate = 4 # self.pooling_factor\n",
        "        #     b, h, w = np.shape(gt_classes)\n",
        "        #     same_padding_left = (rate-w%rate)//2 if w%rate else 0\n",
        "        #     same_padding_right = rate-(rate-w%rate)//2 if w%rate else 0\n",
        "        #     same_padding_top = (rate-h%rate)//2 if h%rate else 0\n",
        "        #     same_padding_bottom = rate-(rate-h%rate)//2 if h%rate else 0\n",
        "        #     for gt_class in gt_classes:\n",
        "        #         pad_v =  np.pad(gt_class, ((same_padding_top, same_padding_bottom), (0,0)), 'constant', constant_values=((0,0),(0,0)))\n",
        "        #         pad_h =  np.pad(gt_class, ((0,0), (same_padding_left, same_padding_right)), 'constant', constant_values=((0,0),(0,0)))\n",
        "                \n",
        "        #         ## find mask range for each single entity\n",
        "        #         num_entities = np.max(gt_classes) / self.num_classes\n",
        "        #         entity_ranges = [[] for _ in range(0,num_entities)]\n",
        "        #         for i in range(1, num_entities):\n",
        "        #             if i % self.num_classes: # only consider non <DontCare> classes\n",
        "        #                 range_y, range_x = np.where(gt_classes==i)\n",
        "        #                 # entity_ranges[i] = [top, left, bottom, right, height, width]\n",
        "        #                 entity_ranges[i] = [min(range_y), min(range_x), max(range_y), max(range_x), \n",
        "        #                                     max(range_y) - min(range_y), max(range_x) - min(range_x)] \n",
        "                           \n",
        "                \n",
        "        \n",
        "        batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "                 'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "                 'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "                 'file_name': file_names, 'shape': [rows,cols]}\n",
        "        return batch\n",
        "    \n",
        "    # def fetch_test_data(self): \n",
        "    #     batch_size = 1\n",
        "        \n",
        "    #     while True:\n",
        "    #         if len(self.test_data_tobe_fetched) == 0:\n",
        "    #             self.test_data_tobe_fetched = [i for i in range(len(self.test_docs))]\n",
        "    #             return None\n",
        "                        \n",
        "    #         selected_index = self.test_data_tobe_fetched[0]\n",
        "    #         self.test_data_tobe_fetched = list(set(self.test_data_tobe_fetched).difference(set([selected_index])))\n",
        "    \n",
        "    #         test_docs = [self.test_docs[selected_index]]\n",
        "            \n",
        "    #         real_rows, real_cols, _, _ = self.cal_rows_cols(test_docs, extra_augmentation=False)\n",
        "    #         rows = max(self.rows_target, real_rows) # small shaped documents have better performance with shape 64\n",
        "    #         cols = max(self.cols_target, real_cols) # large shaped docuemnts have better performance with shape 80\n",
        "                \n",
        "    #         grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, updated_cols, ps_indices_x, ps_indices_y = \\\n",
        "    #             self.positional_mapping(test_docs, self.test_labels, rows, cols)   \n",
        "    #         if updated_cols > cols:\n",
        "    #             print('Test grid EXPAND size: ({},{}) from ({},{})'\\\n",
        "    #                   .format(rows, updated_cols, rows, cols))\n",
        "    #             grid_table, gt_classes, bboxes, label_mapids, bbox_mapids, file_names, _, ps_indices_x, ps_indices_y = \\\n",
        "    #                 self.positional_mapping(test_docs, self.test_labels, rows, updated_cols, update_col=False)    \n",
        "                    \n",
        "    #         ## load image and generate corresponding @ps_1dindices\n",
        "    #         images, ps_1d_indices = [], []\n",
        "    #         if self.use_cutie2:\n",
        "    #             images, ps_1d_indices = self.positional_sampling(self.doc_test_path, file_names, ps_indices_x, ps_indices_y, updated_cols)          \n",
        "    #             if len(images) == batch_size:\n",
        "    #                 break          \n",
        "    #         else:\n",
        "    #             break\n",
        "        \n",
        "    #     batch = {'grid_table': np.array(grid_table), 'gt_classes': np.array(gt_classes), \n",
        "    #              'data_image': np.array(images), 'ps_1d_indices': np.array(ps_1d_indices), # @images and @ps_1d_indices are only used for CUTIEv2\n",
        "    #              'bboxes': bboxes, 'label_mapids': label_mapids, 'bbox_mapids': bbox_mapids,\n",
        "    #              'file_name': file_names, 'shape': [rows,cols]}\n",
        "    #     return batch\n",
        "    \n",
        "    # def form_label_matrix(self, gt_classes, target_h, target_w):\n",
        "    #     \"\"\"\n",
        "    #     build gt_classes and gt_masks with given target featuremap shape (height, width)\n",
        "    #     by inspecting bboxes regions (x,y,w,h)\n",
        "    #     for table / row / column identity segmentation\n",
        "    #     \"\"\"\n",
        "    #     def has_entity_with_augmentation(entity_ranges, roi, use_jittering=False):                    \n",
        "    #         ## find mask with maximum overlap\n",
        "    #         max_iou = 0\n",
        "    #         max_idx = None\n",
        "    #         roi_t, roi_l, roi_b, roi_r = roi\n",
        "    #         roi_h = roi_b - roi_t\n",
        "    #         roi_w = roi_r - roi_l\n",
        "    #         roi_cy = roi_t + roi_h/2\n",
        "    #         roi_cx = roi_l + roi_w/2\n",
        "    #         for idx, entity in enumerate(entity_ranges):\n",
        "    #             if len(entity):\n",
        "    #                 t, l, b, r, h, w = entity\n",
        "    #                 if l>roi_l and r<roi_r and t>roi_t and b<roi_b: # overlap 1\n",
        "    #                     iou = h*w / (roi_h*roi_w)\n",
        "    #                 elif l<roi_l and r>roi_r and t<roi_t and b>roi_b: # overlap 2\n",
        "    #                     iou = roi_h*roi_w / (h*w)\n",
        "    #                 elif l>roi_r or t>roi_b or b<roi_t or r<roi_l: # no intersection\n",
        "    #                     continue\n",
        "    #                 else:\n",
        "    #                     iou = min(h*w, roi_h*roi_w) / max(h*w, roi_h*roi_w)\n",
        "                        \n",
        "    #                 # TBD: add jittering augmentation method  \n",
        "    #                 if use_jittering:\n",
        "    #                     pass                          \n",
        "    #                 if iou > max_iou:\n",
        "    #                     max_idx = idx\n",
        "    #                     max_iou = iou\n",
        "                        \n",
        "    #         ## check centrality / containment / uniqueness\n",
        "    #         t, l, b, r, h, w = entity[idx]\n",
        "    #         cy = t + h/2\n",
        "    #         cx = l + w/2\n",
        "    #         if roi_t+h/3 < cy and cy < toi_b-h/3 and roi_l+w/3 < cx and cx < roi_r-w/3: # centrality\n",
        "    #             if (w > h and roi_w > w*0.9) or (w < h and roi_h > h*0.9): # containment\n",
        "    #                 if True: # uniqueness is already checked with maixmum IOU\n",
        "    #                     return True\n",
        "    #         return False                 \n",
        "    \n",
        "    #     shape = gt_classes.shape\n",
        "    #     rate_v = shape[0] / target_h\n",
        "    #     rate_h = shape[1] / target_w\n",
        "    #     dst_classes = [[[] for i in range(target_h)] for j in range(target_w)]\n",
        "    #     dst_masks = [[[] for i in range(target_h)] for j in range(target_w)]\n",
        "    #     for i in range(target_h):\n",
        "    #         for j in range(target_w):\n",
        "    #             roi = [rate_h*j, rate_v*i, rate_h*(j+1), rate_v*(i+1)] # [top, left, bottom, right]\n",
        "                \n",
        "    #             dst_classes[i][j] = has_entity_with_augmentation(entity_ranges, roi, False)\n",
        "                \n",
        "    #             mask = gt_classes[roi[1]:roi[3], roi[0]:roi[2]]\n",
        "    #             dst_masks[i][j] = mask if dst_classes[i][j] else np.zeros(np.shape(mask))\n",
        "        \n",
        "    #     return np.array(dst_classes), np.array(dst_masks)\n",
        "        \n",
        "    def data_shape_statistic(self):        \n",
        "        def shape_statistic(docs):\n",
        "            res_all = defaultdict(int)\n",
        "            res_row = defaultdict(int)\n",
        "            res_col = defaultdict(int)\n",
        "            for doc in docs:\n",
        "                rows, cols, _, _ = self.cal_rows_cols([doc])\n",
        "                res_all[rows] += 1\n",
        "                res_all[cols] += 1\n",
        "                res_row[rows] += 1\n",
        "                res_col[cols] += 1\n",
        "            res_all = sorted(res_all.items(), key=lambda x:x[0], reverse=True)\n",
        "            res_row = sorted(res_row.items(), key=lambda x:x[0], reverse=True)\n",
        "            res_col = sorted(res_col.items(), key=lambda x:x[0], reverse=True)\n",
        "            return res_all, res_row, res_col\n",
        "    \n",
        "        tss, tss_r, tss_c = shape_statistic(self.training_docs) # training shape static\n",
        "        vss, vss_r, vss_c = shape_statistic(self.validation_docs)\n",
        "        tess, tess_r, tess_c = shape_statistic(self.test_docs)\n",
        "        print(\"Training statistic: \", tss)\n",
        "        print(\"\\t num: \", len(self.training_docs))\n",
        "        print(\"\\t rows statistic: \", tss_r)\n",
        "        print(\"\\t cols statistic: \", tss_c)\n",
        "        print(\"\\nValidation statistic: \", vss)\n",
        "        print(\"\\t num: \", len(self.validation_docs))\n",
        "        print(\"\\t rows statistic: \", vss_r)\n",
        "        print(\"\\t cols statistic: \", vss_c)\n",
        "        print(\"\\nTest statistic: \", tess)\n",
        "        print(\"\\t num: \", len(self.test_docs))\n",
        "        print(\"\\t rows statistic: \", tess_r)\n",
        "        print(\"\\t cols statistic: \", tess_c)\n",
        "        \n",
        "        ## remove data samples not matching the training principle\n",
        "        def data_laundry(docs):\n",
        "            idx = 0\n",
        "            while idx < len(docs):\n",
        "                rows, cols, _, _ = self.cal_rows_cols([docs[idx]])\n",
        "                if rows > self.rows_ulimit or cols > self.cols_ulimit:\n",
        "                    del docs[idx]\n",
        "                else:\n",
        "                    idx += 1\n",
        "        if self.data_laundry:\n",
        "            print(\"\\nRemoving grids with shape larger than ({},{}).\".format(self.rows_ulimit, self.cols_ulimit))\n",
        "            data_laundry(self.training_docs)\n",
        "            data_laundry(self.validation_docs)\n",
        "            data_laundry(self.training_docs)\n",
        "        \n",
        "            tss, tss_r, tss_c = shape_statistic(self.training_docs) # training shape static\n",
        "            vss, vss_r, vss_c = shape_statistic(self.validation_docs)\n",
        "            tess, tess_r, tess_c = shape_statistic(self.test_docs)\n",
        "            print(\"Training statistic after laundary: \", tss)\n",
        "            print(\"\\t num: \", len(self.training_docs))\n",
        "            print(\"\\t rows statistic: \", tss_r)\n",
        "            print(\"\\t cols statistic: \", tss_c)\n",
        "            print(\"Validation statistic after laundary: \", vss)\n",
        "            print(\"\\t num: \", len(self.validation_docs))\n",
        "            print(\"\\t rows statistic: \", vss_r)\n",
        "            print(\"\\t cols statistic: \", vss_c)\n",
        "            print(\"Test statistic after laundary: \", tess)\n",
        "            print(\"\\t num: \", len(self.test_docs))\n",
        "            print(\"\\t rows statistic: \", tess_r)\n",
        "            print(\"\\t cols statistic: \", tess_c)\n",
        "    \n",
        "    def positional_mapping(self, docs, labels, rows, cols):\n",
        "        \"\"\"\n",
        "        docs in format:\n",
        "        [[file_name, text, word_id, [x_left, y_top, x_right, y_bottom], [left, top, right, bottom], max_row_words, max_col_words] ]\n",
        "        return grid_tables, gird_labels, dict bboxes {file_name:[]}, file_names\n",
        "        \"\"\"\n",
        "        grid_tables = []\n",
        "        gird_labels = []\n",
        "        ps_indices_x = [] # positional sampling indices\n",
        "        ps_indices_y = [] # positional sampling indices\n",
        "        bboxes = {}\n",
        "        label_mapids = []\n",
        "        bbox_mapids = [] # [{}, ] bbox identifier, each id with one or multiple bbox/bboxes\n",
        "        file_names = []\n",
        "        for doc in docs:\n",
        "            items = []\n",
        "            cols_e = 2 * cols # use @cols_e larger than required @cols as buffer\n",
        "            grid_table = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            grid_label = np.zeros([rows, cols_e], dtype=np.int8)\n",
        "            ps_x = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            ps_y = np.zeros([rows, cols_e], dtype=np.int32)\n",
        "            bbox = [[] for c in range(cols_e) for r in range(rows)]\n",
        "            bbox_id, bbox_mapid = 0, {} # one word in one or many positions in a bbox is mapped in bbox_mapid\n",
        "            label_mapid = [[] for _ in range(self.num_classes)] # each class is connected to several bboxes (words)\n",
        "            drawing_board = np.zeros([rows, cols_e], dtype=str)\n",
        "            for item in doc:\n",
        "                file_name = item[0]\n",
        "                text = item[1]\n",
        "                word_id = item[2]\n",
        "                x_left, y_top, x_right, y_bottom = item[3][:]\n",
        "                left, top, right, bottom = item[4][:]\n",
        "                \n",
        "                dict_id = self.word_to_index[text]                \n",
        "                entity_id, class_id = self.dress_class(file_name, word_id, labels)\n",
        "                \n",
        "                bbox_id += 1\n",
        "#                 if self.fill_bbox: # TBD: overlap avoidance\n",
        "#                     top = int(rows * y_top / image_h)\n",
        "#                     bottom = int(rows * y_bottom / image_h)\n",
        "#                     left = int(cols * x_left / image_w)\n",
        "#                     right = int(cols * x_right / image_w)\n",
        "#                     grid_table[top:bottom, left:right] = dict_id  \n",
        "#                     grid_label[top:bottom, left:right] = class_id  \n",
        "#                      \n",
        "#                     label_mapid[class_id].append(bbox_id)\n",
        "#                     for row in range(top, bottom):\n",
        "#                         for col in range(left, right):\n",
        "#                             bbox_mapid[row*cols+col] = bbox_id\n",
        "#                      \n",
        "#                     for y in range(top, bottom):\n",
        "#                         for x in range(left, right):\n",
        "#                             bbox[y][x] = [x_left, y_top, x_right-x_left, y_bottom-y_top]\n",
        "                label_mapid[class_id].append(bbox_id)    \n",
        "                \n",
        "                #v_c = (y_top - top + (y_bottom-y_top)/2) / (bottom-top)\n",
        "                #h_c = (x_left - left + (x_right-x_left)/2) / (right-left)\n",
        "                #v_c = (y_top + (y_bottom-y_top)/2) / bottom\n",
        "                #h_c = (x_left + (x_right-x_left)/2) / right \n",
        "                #v_c = (y_top-top) / (bottom-top)\n",
        "                #h_c = (x_left-left) / (right-left)\n",
        "                #v_c = (y_top) / (bottom)\n",
        "                #h_c = (x_left) / (right)\n",
        "                box_y = y_top + (y_bottom-y_top)/2\n",
        "                box_x = x_left # h_l is used for image feature map positional sampling\n",
        "                v_c = (y_top - top + (y_bottom-y_top)/2) / (bottom-top)\n",
        "                h_c = (x_left - left + (x_right-x_left)/2) / (right-left) # h_c is used for sorting items\n",
        "                row = int(rows * v_c) \n",
        "                col = int(cols * h_c) \n",
        "                items.append([row, col, [box_y, box_x], [v_c, h_c], file_name, dict_id, class_id, entity_id, bbox_id, [x_left, y_top, x_right-x_left, y_bottom-y_top]])                       \n",
        "            \n",
        "            items.sort(key=lambda x: (x[0], x[3], x[5])) # sort according to row > h_c > bbox_id\n",
        "            for item in items:\n",
        "                row, col, [box_y, box_x], [v_c, h_c], file_name, dict_id, class_id, entity_id, bbox_id, box = item\n",
        "                entity_class_id = entity_id*self.num_classes + class_id\n",
        "                \n",
        "                while col < cols and grid_table[row, col] != 0:\n",
        "                    col += 1            \n",
        "                \n",
        "                ptr = 0\n",
        "                if col == cols: # shift to find slot to drop the current item\n",
        "                    col -= 1\n",
        "                    while ptr<cols and grid_table[row, ptr] != 0:\n",
        "                        ptr += 1\n",
        "                    if ptr == cols:\n",
        "                        grid_table[row, :-1] = grid_table[row, 1:]\n",
        "                    else:\n",
        "                        grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                \n",
        "                grid_table[row, col] = dict_id\n",
        "                grid_label[row, col] = entity_class_id\n",
        "                ps_x[row, col] = box_x\n",
        "                ps_y[row, col] = box_y\n",
        "                bbox_mapid[row*cols+col] = bbox_id     \n",
        "                bbox[row*cols+col] = box\n",
        "                \n",
        "                # # self.pm_strategy 0: skip if overlap\n",
        "                # # self.pm_strategy 1: shift to find slot if overlap\n",
        "                # # self.pm_strategy 2: expand grid table if overlap\n",
        "                # if self.pm_strategy == 0:\n",
        "                #     if col == cols:                     \n",
        "                #         print('overlap in {} row {} r{}c{}!'.\n",
        "                #               format(file_name, row, rows, cols))\n",
        "                #         #print(grid_table[row,:])\n",
        "                #         #print('overlap in {} <{}> row {} r{}c{}!'.\n",
        "                #         #      format(file_name, self.index_to_word[dict_id], row, rows, cols))\n",
        "                #     else:\n",
        "                #         grid_table[row, col] = dict_id\n",
        "                #         grid_label[row, col] = entity_class_id                       \n",
        "                #         bbox_mapid[row*cols+col] = bbox_id                       \n",
        "                #         bbox[row*cols+col] = box   \n",
        "                # elif self.pm_strategy==1 or self.pm_strategy==2:\n",
        "                #     ptr = 0\n",
        "                #     if col == cols: # shift to find slot to drop the current item\n",
        "                #         col -= 1\n",
        "                #         while ptr<cols and grid_table[row, ptr] != 0:\n",
        "                #             ptr += 1\n",
        "                #         if ptr == cols:\n",
        "                #             grid_table[row, :-1] = grid_table[row, 1:]\n",
        "                #         else:\n",
        "                #             grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                        \n",
        "                #     if self.pm_strategy == 2:\n",
        "                #         while col < cols_e and grid_table[row, col] != 0:\n",
        "                #             col += 1\n",
        "                #         if col > cols: # update maximum cols in current grid\n",
        "                #             print(grid_table[row,:col])\n",
        "                #             print('overlap in {} <{}> row {} r{}c{}!'.\n",
        "                #                   format(file_name, self.index_to_word[dict_id], row, rows, cols))\n",
        "                #             cols = col\n",
        "                #         if col == cols_e:      \n",
        "                #             print('overlap!')\n",
        "                    \n",
        "                #     grid_table[row, col] = dict_id\n",
        "                #     grid_label[row, col] = entity_class_id\n",
        "                #     ps_x[row, col] = box_x\n",
        "                #     ps_y[row, col] = box_y\n",
        "                #     bbox_mapid[row*cols+col] = bbox_id     \n",
        "                #     bbox[row*cols+col] = box\n",
        "                \n",
        "            cols = self.fit_shape(cols)\n",
        "            grid_table = grid_table[..., :cols]\n",
        "            grid_label = grid_label[..., :cols]\n",
        "            ps_x = np.array(ps_x[..., :cols])\n",
        "            ps_y = np.array(ps_y[..., :cols])\n",
        "            \n",
        "            if DEBUG:\n",
        "                self.grid_visualization(file_name, grid_table, grid_label)\n",
        "            \n",
        "            grid_tables.append(np.expand_dims(grid_table, -1)) \n",
        "            gird_labels.append(grid_label) \n",
        "            ps_indices_x.append(ps_x)\n",
        "            ps_indices_y.append(ps_y)\n",
        "            bboxes[file_name] = bbox\n",
        "            label_mapids.append(label_mapid)\n",
        "            bbox_mapids.append(bbox_mapid)\n",
        "            file_names.append(file_name)\n",
        "            \n",
        "        return grid_tables, gird_labels, bboxes, label_mapids, bbox_mapids, file_names, cols, ps_indices_x, ps_indices_y\n",
        "    \n",
        "    def positional_sampling(self, path, file_names, ps_indices_x, ps_indices_y, updated_cols):\n",
        "        images, ps_1d_indices = [], []\n",
        "        \n",
        "        ## load image and generate corresponding @ps_1dindices\n",
        "        max_h, max_w = 0, updated_cols\n",
        "        for i in range(len(file_names)):\n",
        "            file_name = file_names[i]\n",
        "            file_path = join(path, file_name) # TBD: ensure image is upright\n",
        "            ps_1d_x = np.array(ps_indices_x[i], dtype=np.float32).reshape([-1])\n",
        "            ps_1d_y = np.array(ps_indices_y[i], dtype=np.float32).reshape([-1])\n",
        "            \n",
        "            image = cv2.imread(file_path)\n",
        "            if image is not None:\n",
        "                h, w, _ = image.shape # [h,w,c]\n",
        "                factor = max_w / w\n",
        "                \n",
        "                h = int(h*factor)\n",
        "                ps_1d_x *= factor # TBD: implement more accurate mapping method rather than nearest neighbor, since the .4 or .6 leads to two different sampling results\n",
        "                ps_1d_y *= factor                \n",
        "                \n",
        "                ps_1d = np.int32(np.floor(ps_1d_x) + np.floor(ps_1d_y) * max_w)\n",
        "                max_items = max_w * h - 1\n",
        "                for i in range(len(ps_1d)):\n",
        "                    if ps_1d[i] > max_items - 1:\n",
        "                        ps_1d[i] = max_items - 1\n",
        "                    \n",
        "                \n",
        "                image = cv2.resize(image, (max_w, h))\n",
        "                image = (image-127.5) / 255\n",
        "            else:\n",
        "                #print('Warning: {} image not found!'.format(file_path))\n",
        "                print('{} ignored due to image file not found.'.format(file_path))\n",
        "                image, ps_1d = None, None\n",
        "                break\n",
        "                \n",
        "            if image is not None and ps_1d is not None: # ignore data with no images                 \n",
        "                ps_1d_indices.append(ps_1d)\n",
        "                images.append(image)\n",
        "                h,w,c = image.shape\n",
        "                if h > max_h:\n",
        "                    max_h = h\n",
        "            else:\n",
        "                pass\n",
        "                #print('{} ignored due to image file not found.'.format(file_path))\n",
        "                \n",
        "        ## pad image to the same shape\n",
        "        for i,image in enumerate(images): \n",
        "            pad_img = np.zeros([max_h, max_w, 3], dtype=image.dtype)\n",
        "            pad_img[:image.shape[0], :, :] = image\n",
        "            images[i] = pad_img\n",
        "        \n",
        "        return images, ps_1d_indices\n",
        "    \n",
        "    def load_data(self, data_files, update_dict=False):\n",
        "        \"\"\"\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        load doc words with location and class returned in format: \n",
        "        [[file_name, text, word_id, [x_left, y_top, x_right, y_bottom], [left, top, right, bottom], max_row_words, max_col_words] ]\n",
        "        \"\"\"\n",
        "        label_dressed = {}\n",
        "        doc_dressed = []\n",
        "        if not data_files:\n",
        "            print(\"no data file found.\")        \n",
        "        for file in data_files:\n",
        "            with open(file, encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "                file_id = data['global_attributes']['file_id']\n",
        "                \n",
        "                label = self.collect_label(file_id, data['fileds'])\n",
        "                # ignore corrupted data\n",
        "                if not label:\n",
        "                    continue                \n",
        "                label_dressed.update(label) \n",
        "                \n",
        "                data = self.collect_data(file_id, data['text_boxes'], update_dict)\n",
        "                for i in data:\n",
        "                    doc_dressed.append(i)\n",
        "                    \n",
        "        return doc_dressed, label_dressed       \n",
        "    \n",
        "    def cal_rows_cols(self, docs, extra_augmentation=False, dropout=False):                  \n",
        "        max_row = self.encoding_factor\n",
        "        max_col = self.encoding_factor\n",
        "        for doc in docs:\n",
        "            for line in doc: \n",
        "                _, _, _, _, _, max_row_words, max_col_words = line\n",
        "                if max_row_words > max_row:\n",
        "                    max_row = max_row_words\n",
        "                if max_col_words > max_col:\n",
        "                    max_col = max_col_words\n",
        "        \n",
        "        pre_rows = self.fit_shape(max_row) #(max_row//self.encoding_factor+1) * self.encoding_factor\n",
        "        pre_cols = self.fit_shape(max_col) #(max_col//self.encoding_factor+1) * self.encoding_factor\n",
        "        \n",
        "        rows, cols = 0, 0\n",
        "        if extra_augmentation:\n",
        "            pad_row = int(random.gauss(0, self.da_extra_rows*self.encoding_factor)) #abs(random.gauss(0, u))\n",
        "            pad_col = int(random.gauss(0, self.da_extra_cols*self.encoding_factor)) #random.randint(0, u)\n",
        "            \n",
        "            if self.augment_strategy == 1: # strategy 1: augment data by increasing grid shape sizes\n",
        "                pad_row = abs(pad_row)\n",
        "                pad_col = abs(pad_col)\n",
        "                rows = self.fit_shape(max_row+pad_row) # apply upper boundary to avoid OOM\n",
        "                cols = self.fit_shape(max_col+pad_col) # apply upper boundary to avoid OOM\n",
        "            elif self.augment_strategy == 2 or self.augment_strategy == 3: # strategy 2: augment by increasing or decreasing the target gird shape size\n",
        "                rows = self.fit_shape(max(self.rows_target+pad_row, max_row)) # protect grid shape\n",
        "                cols = self.fit_shape(max(self.cols_target+pad_col, max_col)) # protect grid shape\n",
        "            else:\n",
        "                raise Exception('unknown augment strategy')\n",
        "            rows = min(rows, self.rows_ulimit) # apply upper boundary to avoid OOM\n",
        "            cols = min(cols, self.cols_ulimit) # apply upper boundary to avoid OOM                                \n",
        "        else:\n",
        "            rows = pre_rows\n",
        "            cols = pre_cols\n",
        "        return rows, cols, pre_rows, pre_cols \n",
        "    \n",
        "    def fit_shape(self, shape): # modify shape size to fit the encoding factor\n",
        "        while shape % self.encoding_factor:\n",
        "            shape += 1\n",
        "        return shape\n",
        "    \n",
        "    def expand_shape(self, shape): # expand shape size with step 2\n",
        "        return self.fit_shape(shape+1)\n",
        "        \n",
        "    def collect_data(self, file_name, content, update_dict):\n",
        "        \"\"\"\n",
        "        dress and preserve only interested data.\n",
        "        \"\"\"          \n",
        "        content_dressed = []\n",
        "        left, top, right, bottom, buffer = 9999, 9999, 0, 0, 2\n",
        "        for line in content:\n",
        "            bbox = line['bbox'] # handle data corrupt\n",
        "            if len(bbox) == 0:\n",
        "                continue\n",
        "            if line['text'] in self.special_dict: # ignore potential overlap causing characters\n",
        "                continue\n",
        "            \n",
        "            x_left, y_top, x_right, y_bottom = self.dress_bbox(bbox)        \n",
        "            # TBD: the real image size is better for calculating the relative x/y/w/h\n",
        "            if x_left < left: left = x_left - buffer\n",
        "            if y_top < top: top = y_top - buffer\n",
        "            if x_right > right: right = x_right + buffer\n",
        "            if y_bottom > bottom: bottom = y_bottom + buffer\n",
        "            \n",
        "            word_id = line['word_id']\n",
        "            dressed_texts = self.dress_text(line['text'], update_dict)\n",
        "            \n",
        "            num_block = len(dressed_texts)\n",
        "            for i, dressed_text in enumerate(dressed_texts): # handling tokenized text, separate bbox\n",
        "                new_left = int(x_left + (x_right-x_left) / num_block * (i))\n",
        "                new_right = int(x_left + (x_right-x_left) / num_block * (i+1))\n",
        "                content_dressed.append([file_name, dressed_text, word_id, [new_left, y_top, new_right, y_bottom]])\n",
        "            \n",
        "        # initial calculation of maximum number of words in rows/cols in terms of image size\n",
        "        num_words_row = [0 for _ in range(bottom)] # number of words in each row\n",
        "        num_words_col = [0 for _ in range(right)] # number of words in each column\n",
        "        for line in content_dressed:\n",
        "            _, _, _, [x_left, y_top, x_right, y_bottom] = line\n",
        "            for y in range(y_top, y_bottom):\n",
        "                num_words_row[y] += 1\n",
        "            for x in range(x_left, x_right):\n",
        "                num_words_col[x] += 1\n",
        "        max_row_words = self.fit_shape(max(num_words_row))\n",
        "        max_col_words = 0#self.fit_shape(max(num_words_col))\n",
        "        \n",
        "        # further expansion of maximum number of words in rows/cols in terms of grid shape\n",
        "        max_rows = max(self.encoding_factor, max_row_words)\n",
        "        max_cols = max(self.encoding_factor, max_col_words)\n",
        "        DONE = False\n",
        "        while not DONE:\n",
        "            DONE = True\n",
        "            grid_table = np.zeros([max_rows, max_cols], dtype=np.int32)\n",
        "            for line in content_dressed:\n",
        "                _, _, _, [x_left, y_top, x_right, y_bottom] = line\n",
        "                row = int(max_rows * (y_top - top + (y_bottom-y_top)/2) / (bottom-top))\n",
        "                col = int(max_cols * (x_left - left + (x_right-x_left)/2) / (right-left))\n",
        "                #row = int(max_rows * (y_top + (y_bottom-y_top)/2) / (bottom))\n",
        "                #col = int(max_cols * (x_left + (x_right-x_left)/2) / (right))\n",
        "                #row = int(max_rows * (y_top-top) / (bottom-top))\n",
        "                #col = int(max_cols * (x_left-left) / (right-left))\n",
        "                #row = int(max_rows * (y_top) / (bottom))\n",
        "                #col = int(max_cols * (x_left) / (right))\n",
        "                #row = int(max_rows * (y_top + (y_bottom-y_top)/2) / bottom)  \n",
        "                #col = int(max_cols * (x_left + (x_right-x_left)/2) / right) \n",
        "                \n",
        "                while col < max_cols and grid_table[row, col] != 0: # shift to find slot to drop the current item\n",
        "                    col += 1\n",
        "                if col == max_cols: # shift to find slot to drop the current item\n",
        "                    col -= 1\n",
        "                    ptr = 0\n",
        "                    while ptr<max_cols and grid_table[row, ptr] != 0:\n",
        "                        ptr += 1\n",
        "                    if ptr == max_cols: # overlap cannot be solved in current row, then expand the grid\n",
        "                        max_cols = self.expand_shape(max_cols)\n",
        "                        DONE = False\n",
        "                        break\n",
        "                    \n",
        "                    grid_table[row, ptr:-1] = grid_table[row, ptr+1:]\n",
        "                \n",
        "                if DONE:\n",
        "                    if row > max_rows or col>max_cols:\n",
        "                        print('wrong')\n",
        "                    grid_table[row, col] = 1\n",
        "        \n",
        "        max_rows = self.fit_shape(max_rows)\n",
        "        max_cols = self.fit_shape(max_cols)\n",
        "        \n",
        "        #print('{} collected in shape: {},{}'.format(file_name, max_rows, max_cols))\n",
        "        \n",
        "        # segment grid into two parts if number of cols is larger than self.cols_target\n",
        "        data = []\n",
        "        if self.segment_grid and max_cols > self.cols_segment:\n",
        "            content_dressed_left = []\n",
        "            content_dressed_right = []\n",
        "            cnt = defaultdict(int) # counter for number of words in a specific row\n",
        "            cnt_l, cnt_r = defaultdict(int), defaultdict(int) # update max_cols if larger than self.cols_segment\n",
        "            left_boundary = max_cols - self.cols_segment\n",
        "            right_boundary = self.cols_segment\n",
        "            for i, line in enumerate(content_dressed):\n",
        "                file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom] = line\n",
        "                \n",
        "                row = int(max_rows * (y_top + (y_bottom-y_top)/2) / bottom)\n",
        "                cnt[row] += 1                \n",
        "                if cnt[row] <= left_boundary:\n",
        "                    cnt_l[row] += 1\n",
        "                    content_dressed_left.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, self.cols_segment])\n",
        "                elif left_boundary < cnt[row] <= right_boundary:\n",
        "                    cnt_l[row] += 1\n",
        "                    cnt_r[row] += 1\n",
        "                    content_dressed_left.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, self.cols_segment])\n",
        "                    content_dressed_right.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max(max(cnt_r.values()), self.cols_segment)])\n",
        "                else:\n",
        "                    cnt_r[row] += 1\n",
        "                    content_dressed_right.append([file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max(max(cnt_r.values()), self.cols_segment)])\n",
        "            #print(sorted(cnt.items(), key=lambda x:x[1], reverse=True))\n",
        "            #print(sorted(cnt_l.items(), key=lambda x:x[1], reverse=True))\n",
        "            #print(sorted(cnt_r.items(), key=lambda x:x[1], reverse=True))\n",
        "            if max(cnt_l.values()) < 2*self.cols_segment:\n",
        "                data.append(content_dressed_left)\n",
        "            if max(cnt_r.values()) < 2*self.cols_segment: # avoid OOM, which tends to happen in the right side\n",
        "                data.append(content_dressed_right)\n",
        "        else:\n",
        "            for i, line in enumerate(content_dressed): # append height/width/numofwords to the list\n",
        "                file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom] = line\n",
        "                content_dressed[i] = [file_name, dressed_text, word_id, [x_left, y_top, x_right, y_bottom], \\\n",
        "                                      [left, top, right, bottom], max_rows, max_cols ]\n",
        "            data.append(content_dressed)\n",
        "        return data\n",
        "    \n",
        "    def collect_label(self, file_id, content):\n",
        "        \"\"\"\n",
        "        dress and preserve only interested data.\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        \"\"\"\n",
        "        label_dressed = dict()\n",
        "        label_dressed[file_id] = {cls:[] for cls in self.classes[1:]}\n",
        "        for line in content:\n",
        "            cls = line['field_name']\n",
        "            if cls in self.classes:\n",
        "                #identity = line.get('identity', 0) \n",
        "                label_dressed[file_id][cls].append( {'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''} )\n",
        "                label_dressed[file_id][cls][-1]['key_id'] = line.get('key_id', [])\n",
        "                label_dressed[file_id][cls][-1]['value_id'] = line['value_id'] # value_id\n",
        "                label_dressed[file_id][cls][-1]['key_text'] = line.get('key_text', []) \n",
        "                label_dressed[file_id][cls][-1]['value_text'] = line['value_text'] # value_text\n",
        "                \n",
        "        # handle corrupted data\n",
        "        for cls in label_dressed[file_id]: \n",
        "            for idx, label in enumerate(label_dressed[file_id][cls]):\n",
        "                if len(label) == 0: # no relevant class in sample @file_id\n",
        "                    continue\n",
        "                if (len(label['key_text'])>0 and len(label['key_id'])==0) or \\\n",
        "                   (len(label['value_text'])>0 and len(label['value_id'])==0):\n",
        "                    return None\n",
        "            \n",
        "        return label_dressed\n",
        "\n",
        "    def dress_class(self, file_name, word_id, labels):\n",
        "        \"\"\"\n",
        "        label_dressed in format:\n",
        "        {file_id: {class: [{'key_id':[], 'value_id':[], 'key_text':'', 'value_text':''}, ] } }\n",
        "        \"\"\"\n",
        "        if file_name in labels:\n",
        "            for cls, cls_labels in labels[file_name].items():\n",
        "                for idx, cls_label in enumerate(cls_labels):\n",
        "                    for key, values in cls_label.items():\n",
        "                        if (key=='key_id' or key=='value_id') and word_id in values:\n",
        "                            if key == 'key_id':\n",
        "                                if self.data_mode == 0:\n",
        "                                    return idx, self.classes.index(cls) * 2 - 1 # odd\n",
        "                                elif self.data_mode == 1:\n",
        "                                    return idx, self.classes.index(cls)\n",
        "                                else: # ignore key_id when self.data_mode is not 0 or 1\n",
        "                                    return 0, 0\n",
        "                            elif key == 'value_id':\n",
        "                                if self.data_mode == 0:\n",
        "                                    return idx, self.classes.index(cls) * 2 # even \n",
        "                                else: # when self.data_mode is 1 or 2\n",
        "                                    return idx, self.classes.index(cls) \n",
        "            return 0, 0 # 0 is of class type 'DontCare'\n",
        "        print(\"No matched labels found for {}\".format(file_name))\n",
        "    \n",
        "    def dress_text(self, text, update_dict):\n",
        "        \"\"\"\n",
        "        three cases covered: \n",
        "        alphabetic string, numeric string, special character\n",
        "        \"\"\"\n",
        "        string = text.lower()\n",
        "        for i, c in enumerate(string):\n",
        "            if is_number(c):\n",
        "                string = string[:i] + '0' + string[i+1:]\n",
        "                \n",
        "        strings = [string]\n",
        "        # if self.tokenize:\n",
        "        #     strings = self.tokenizer.tokenize(strings[0])\n",
        "        #     #print(string, '-->', strings)\n",
        "            \n",
        "        for idx, string in enumerate(strings):            \n",
        "            if string.isalpha():\n",
        "                if string in self.special_dict:\n",
        "                    string = self.special_dict[string]\n",
        "                # TBD: convert a word to its most similar word in a known vocabulary\n",
        "            elif is_number(string):\n",
        "                pass\n",
        "            elif len(string)==1: # special character\n",
        "                pass\n",
        "            else:\n",
        "                # TBD: seperate string as parts for alpha and number combinated strings\n",
        "                #string = re.findall('[a-z]+', string)\n",
        "                pass            \n",
        "            \n",
        "            if string not in self.dictionary.keys():\n",
        "                if update_dict:\n",
        "                    self.dictionary[string] = 0\n",
        "                else:\n",
        "                    #print('unknown text: ' + string)\n",
        "                    string = '[UNK]' # TBD: take special care to unmet words\\\n",
        "            self.dictionary[string] += 1\n",
        "            \n",
        "            strings[idx] = string\n",
        "        return strings\n",
        "            \n",
        "    def dress_bbox(self, bbox):\n",
        "        positions = np.array(bbox).reshape([-1])\n",
        "        x_left = max(0, min(positions[0::2]))\n",
        "        x_right = max(positions[0::2])\n",
        "        y_top = max(0, min(positions[1::2]))\n",
        "        y_bottom = max(positions[1::2])\n",
        "        w = x_right - x_left\n",
        "        h = y_bottom - y_top\n",
        "        return int(x_left), int(y_top), int(x_right), int(y_bottom)       \n",
        "    \n",
        "    def get_filenames(self, data_path):\n",
        "        files = []\n",
        "        for dirpath,dirnames,filenames in walk(data_path):\n",
        "            for filename in filenames:\n",
        "                file = join(dirpath,filename)\n",
        "                if file.endswith('csv') or file.endswith('json'):\n",
        "                    files.append(file)\n",
        "        return files       \n",
        "            \n",
        "    def grid_visualization(self, file_name, grid, label):\n",
        "        from google.colab.patches import cv2_imshow\n",
        "        height, width = np.shape(grid)\n",
        "        grid_box_h, grid_box_w = 20, 40\n",
        "        palette = np.zeros([height*grid_box_h, width*grid_box_w, 3], np.uint8)\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        gt_color = [[255, 250, 240], [152, 245, 255], [127, 255, 212], [100, 149, 237], \n",
        "                    [192, 255, 62], [175, 238, 238], [255, 130, 171], [240, 128, 128], [255, 105, 180]]\n",
        "        cv2.putText(palette, file_name+\"({},{})\".format(height,width), (grid_box_h,grid_box_w), font, 0.6, [255,0,0])  \n",
        "        for h in range(height):\n",
        "            cv2.line(palette, (0,h*grid_box_h), (width*grid_box_w, h*grid_box_h), (100,100,100))\n",
        "            for w in range(width):\n",
        "                if grid[h,w]:\n",
        "                    org = (int((w+1)*grid_box_w*0.7),int((h+1)*grid_box_h*0.9))\n",
        "                    color = gt_color[label[h,w]]\n",
        "                    cv2.putText(palette, self.index_to_word[grid[h,w]], org, font, 0.4, color)        \n",
        "        \n",
        "        img = cv2.imread(self.doc_path+'/'+file_name)\n",
        "        if img is not None:\n",
        "            shape = list(img.shape)\n",
        "            max_len = 768\n",
        "            factor = max_len / max(shape)\n",
        "            shape[0], shape[1] = [int(s*factor) for s in shape[:2]]\n",
        "            img = cv2.resize(img, (shape[1], shape[0]))  \n",
        "            cv2.imshow(\"img\", img)\n",
        "        cv2_imshow(palette)\n",
        "        cv2.waitKey(0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_vW3QhxLvMS"
      },
      "source": [
        "# 1. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C04fAhXgLuwf"
      },
      "source": [
        "def layer(op):\n",
        "    def layer_decorated(self, *args, **kwargs):\n",
        "        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))        \n",
        "        if len(self.layer_inputs) == 0:\n",
        "            raise RuntimeError('No input variables found for layers %s' % name)\n",
        "        elif len(self.layer_inputs) == 1:\n",
        "            layer_input = self.layer_inputs[0]\n",
        "        else:\n",
        "            layer_input = list(self.layer_inputs)            \n",
        "            \n",
        "        layer_output = op(self, layer_input, *args, **kwargs)\n",
        "        \n",
        "        self.layers[name] = layer_output\n",
        "        self.feed(layer_output)\n",
        "        \n",
        "        return self\n",
        "    return layer_decorated\n",
        "    \n",
        "    \n",
        "class Model(object):\n",
        "    def __init__(self, trainable=True):\n",
        "        self.layers = dict()      \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "    \n",
        "    \n",
        "    def build_loss(self):\n",
        "        raise NotImplementedError('Must be subclassed.')\n",
        "    \n",
        "    \n",
        "    def setup(self):        \n",
        "        raise NotImplementedError('Must be subclassed.')\n",
        "     \n",
        "    \n",
        "    @layer\n",
        "    def embed(self, layer_input, vocabulary_size, embedding_size, name, dropout=1, trainable=True):\n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_embedding = tf.random_uniform_initializer(-1.0, 1.0)\n",
        "            embeddings = self.make_var('weights', [vocabulary_size, embedding_size], init_embedding, None, trainable)\n",
        "            shape = tf.shape(layer_input)\n",
        "            \n",
        "            reshaped_input = tf.reshape(layer_input, [-1])\n",
        "            e = tf.nn.embedding_lookup(embeddings, reshaped_input)\n",
        "            e = tf.nn.dropout(e, dropout)\n",
        "            reshaped_e = tf.reshape(e, [shape[0], shape[1], shape[2], embedding_size])\n",
        "            return reshaped_e\n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def bert_embed(self, layer_input, vocab_size, embedding_size=768, use_one_hot_embeddings=False, \n",
        "    #                initializer_range=0.02, name=\"embeddings\", trainable=False):\n",
        "    #     with tf.compat.v1.variable_scope(\"bert\"):\n",
        "    #       with tf.compat.v1.variable_scope(\"embeddings\"):\n",
        "    #         # Perform embedding lookup on the word ids.\n",
        "    #         (embedding_output, embedding_table) = self.embedding_lookup(\n",
        "    #             input_ids=layer_input, vocab_size=vocab_size, embedding_size=embedding_size,\n",
        "    #             initializer_range=initializer_range,\n",
        "    #             word_embedding_name=\"word_embeddings\",\n",
        "    #             use_one_hot_embeddings=use_one_hot_embeddings,\n",
        "    #             trainable=trainable)\n",
        "    #         self.embedding_table = embedding_table # the inherited class need a self.embedding_table variable\n",
        "    #         return embedding_output        \n",
        "        \n",
        "    \n",
        "    # @layer\n",
        "    # def positional_sampling(self, layer_input, feature_dimension, name='positional_sampling'):\n",
        "    #     featuremap = layer_input[0]\n",
        "    #     batch_indices = layer_input[1]\n",
        "    #     grid = layer_input[2]        \n",
        "        \n",
        "    #     shape_grid = tf.shape(grid)\n",
        "        \n",
        "    #     featuremap_flat = tf.reshape(featuremap, [shape_grid[0], -1, feature_dimension])        \n",
        "    #     batch_indices_flat = tf.reshape(batch_indices, [shape_grid[0], -1])        \n",
        "    #     batch_ps_flat = tf.batch_gather(featuremap_flat, batch_indices_flat)\n",
        "        \n",
        "    #     b, h, w, c = shape_grid[0], shape_grid[1], shape_grid[2], feature_dimension\n",
        "    #     return tf.reshape(batch_ps_flat, [b,h,w,c])\n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def sepconv(self, layer_input, k_h, k_w, cardinality, compression, name, activation='relu', trainable=True):\n",
        "    #     \"\"\" customized seperable convolution\n",
        "    #     \"\"\"\n",
        "    #     convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "    #     activate = lambda z: tf.nn.relu(z, 'relu')\n",
        "    #     with tf.compat.v1.variable_scope(name) as scope:\n",
        "    #         init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "    #         init_biases = tf.constant_initializer(0.0)\n",
        "    #         regularizer = self.l2_regularizer(self.weight_decay)\n",
        "    #         c_i = layer_input.get_shape().as_list()[-1]\n",
        "            \n",
        "    #         layer_output = []\n",
        "    #         c = c_i / cardinality / compression\n",
        "    #         for _ in range(cardinality):\n",
        "    #             a = self.convolution(convolve, activate, layer_input, 1, 1, c_i, c,\n",
        "    #                                  init_weights, init_biases, regularizer, trainable, '0_{}'.format(_))                \n",
        "    #             a = self.convolution(convolve, activate, a, k_h, k_w, c, c, \n",
        "    #                                  init_weights, init_biases, regularizer, trainable, '1_{}'.format(_))\n",
        "    #             a = self.convolution(convolve, activate, a, 1, 1, c, c_i, \n",
        "    #                                  init_weights, init_biases, regularizer, trainable, '2_{}'.format(_))\n",
        "    #             layer_output.append(a)\n",
        "    #         layer_output = tf.add_n(layer_output)\n",
        "    #         return tf.add(layer_output, layer_input)\n",
        "        \n",
        "    \n",
        "    # @layer\n",
        "    # def up_sepconv(self, layer_input, k_h, k_w, cardinality, compression, name, activation='relu', trainable=True):\n",
        "    #     \"\"\" customized upscale seperable convolution\n",
        "    #     \"\"\"\n",
        "    #     convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "    #     activate = lambda z: tf.nn.relu(z, 'relu')        \n",
        "    #     with tf.compat.v1.variable_scope(name) as scope:\n",
        "    #         shape = tf.shape(layer_input)\n",
        "    #         h = shape[1]\n",
        "    #         w = shape[2]\n",
        "    #         layer_input = tf.image.resize_nearest_neighbor(layer_input, [2*h, 2*w])\n",
        "    #         init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "    #         init_biases = tf.constant_initializer(0.0)\n",
        "    #         regularizer = self.l2_regularizer(self.weight_decay)\n",
        "    #         c_i = layer_input.get_shape().as_list()[-1]\n",
        "            \n",
        "    #         layer_output = []\n",
        "    #         c = c_i / cardinality / compression\n",
        "    #         for _ in range(cardinality):\n",
        "    #             a = self.convolution(convolve, activate, layer_input, 1, 1, c_i, c,\n",
        "    #                                  init_weights, init_biases, regularizer, trainable, '0_{}'.format(_))                \n",
        "    #             a = self.convolution(convolve, activate, a, k_h, k_w, c, c, \n",
        "    #                                  init_weights, init_biases, regularizer, trainable, '1_{}'.format(_))\n",
        "    #             a = self.convolution(convolve, activate, a, 1, 1, c, c_i, \n",
        "    #                                  init_weights, init_biases, regularizer, trainable, '2_{}'.format(_))\n",
        "    #             layer_output.append(a)\n",
        "    #         layer_output = tf.add_n(layer_output)\n",
        "    #         return tf.add(layer_output, layer_input)\n",
        "        \n",
        "        \n",
        "    # @layer\n",
        "    # def dense_block(self, layer_input, k_h, k_w, c_o, depth, name, activation='relu', trainable=True):\n",
        "    #     convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "    #     activate = lambda z: tf.nn.relu(z, 'relu')\n",
        "    #     with tf.compat.v1.variable_scope(name) as scope:\n",
        "    #         init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "    #         init_biases = tf.constant_initializer(0.0)\n",
        "    #         regularizer = self.l2_regularizer(self.weight_decay)  \n",
        "            \n",
        "    #         layer_tmp = layer_input\n",
        "    #         for d in range(depth):          \n",
        "    #             c_i = layer_tmp.get_shape()[-1]\n",
        "    #             a = self.convolution(convolve, activate, layer_tmp, 1, 1, c_i, c_i//2,\n",
        "    #                                  init_weights, init_biases, regularizer, trainable)\n",
        "                \n",
        "    #             a = self.convolution(convolve, activate, a, k_h, k_w, c_i, c_o, \n",
        "    #                                  init_weights, init_biases, regularizer, trainable)\n",
        "                \n",
        "    #             layer_tmp = tf.concat([a, layer_input], 3)\n",
        "                \n",
        "    #         return layer_tmp\n",
        "            \n",
        "        \n",
        "    @layer\n",
        "    def conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,s_h,s_w,1], 'SAME')\n",
        "        #convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, 2, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "     \n",
        "     \n",
        "    @layer\n",
        "    def dilate_conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, rate, name, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, rate, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "        if activation == 'sigmoid':\n",
        "            activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def dilate_module(self, layer_input, k_h, k_w, c_o, s_h, s_w, rate, name, activation='relu', trainable=True):\n",
        "    #     convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, rate, 'SAME', 'DILATE')\n",
        "        \n",
        "    #     activate = lambda z: tf.nn.relu(z, 'relu') #if activation == 'relu':\n",
        "    #     if activation == 'sigmoid':\n",
        "    #         activate = lambda z: tf.nn.sigmoid(z, 'sigmoid')\n",
        "            \n",
        "    #     with tf.compat.v1.variable_scope(name) as scope:\n",
        "    #         init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "    #         init_biases = tf.constant_initializer(0.0)\n",
        "    #         regularizer = self.l2_regularizer(self.weight_decay)\n",
        "    #         c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "    #         a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "    #                              init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def up_conv(self, layer_input, k_h, k_w, c_o, s_h, s_w, name, factor=2, activation='relu', trainable=True):\n",
        "        convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,s_h,s_w,1], 'SAME')\n",
        "        #convolve = lambda input, filter: tf.nn.atrous_conv2d(input, filter, 2, 'SAME', 'DILATE')\n",
        "        \n",
        "        activate = lambda z: tf.nn.relu(z, 'relu')        \n",
        "        with tf.compat.v1.variable_scope(name) as scope:\n",
        "            shape = tf.shape(layer_input)\n",
        "            h = shape[1]\n",
        "            w = shape[2]\n",
        "            layer_input = tf.image.resize_nearest_neighbor(layer_input, [factor*h, factor*w])\n",
        "            init_weights = tf.compat.v1.truncated_normal_initializer(0.0, 0.01)\n",
        "            init_biases = tf.constant_initializer(0.0)\n",
        "            regularizer = self.l2_regularizer(self.weight_decay)\n",
        "            c_i = layer_input.get_shape()[-1]\n",
        "            \n",
        "            a = self.convolution(convolve, activate, layer_input, k_h, k_w, c_i, c_o, \n",
        "                                 init_weights, init_biases, regularizer, trainable)\n",
        "            return a  \n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def attention(self, layer_input, num_heads, name, att_dropout=0.0, hidden_dropout=0.1, trainable=True):\n",
        "    #     \"\"\"\n",
        "    #     implement self attention with residual addition,\n",
        "    #     layer_input[0] and layer_input[1] should have the same shape for residual addition \n",
        "    #     \"\"\"\n",
        "    #     f = layer_input[0]\n",
        "    #     x = layer_input[1]\n",
        "        \n",
        "    #     convolve = lambda input, filter: tf.nn.conv2d(input, filter, [1,1,1,1], 'SAME')\n",
        "    #     with tf.variable_scope(name) as scope:\n",
        "    #         init_weights = tf.truncated_normal_initializer(0.0, 0.02)\n",
        "    #         regularizer = self.l2_regularizer(self.weight_decay)\n",
        "    #         shape = tf.shape(f)\n",
        "    #         c_i = f.get_shape()[-1]\n",
        "    #         c_o = f.get_shape()[-1]\n",
        "    #         c_a = c_o // num_heads # attention kernel depth, size per head\n",
        "            \n",
        "    #         query = self.make_var('weights_query', [1, 1, c_i, c_a], init_weights, regularizer, trainable)\n",
        "    #         query_layer = convolve(f, query) # [B, H, W, c_a]\n",
        "    #         query_layer = tf.reshape(query_layer, [shape[0], -1, c_a]) # [B, H*W, c_a]\n",
        "            \n",
        "    #         key = self.make_var('weights_key', [1, 1, c_i, c_a], init_weights, regularizer, trainable)\n",
        "    #         key_layer = convolve(f, key) # [B, H, W, c_a]\n",
        "    #         key_layer = tf.reshape(key_layer, [shape[0], -1, c_a]) # [B, H*W, c_a]\n",
        "            \n",
        "    #         value = self.make_var('weights_value', [1, 1, c_i, c_o], init_weights, regularizer, trainable)\n",
        "    #         value_layer = convolve(f, value) \n",
        "    #         value_layer = tf.reshape(value_layer, [shape[0], -1, c_o])# [B, H*W, c_o]\n",
        "            \n",
        "    #         attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True) # [B, H*W, H*W]\n",
        "    #         attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(c_a.value)))\n",
        "            \n",
        "    #         attention_probs = tf.nn.softmax(attention_scores)\n",
        "    #         #attention_probs = dropout(attention_probs, att_dropout)\n",
        "            \n",
        "    #         context_layer = tf.matmul(attention_probs, value_layer) # [B, H*W, c_o]\n",
        "    #         context_layer = tf.reshape(context_layer, shape) # [B, H, W, c_o]\n",
        "            \n",
        "    #         kernel = self.make_var('output', [1, 1, c_o, c_o], init_weights, regularizer, trainable)\n",
        "    #         attention_output = convolve(context_layer, kernel) \n",
        "    #         #attention_output = dropout(attention_output, hidden_dropout)\n",
        "    #         attention_output = attention_output + x\n",
        "            \n",
        "    #         return tf.contrib.layers.instance_norm(attention_output, center=False, scale=False)\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def concat(self, layer_input, axis, name):\n",
        "        return tf.concat(layer_input, axis)\n",
        "    \n",
        "    \n",
        "    # @layer\n",
        "    # def add(self, layer_input, name):\n",
        "    #     return tf.math.add_n(layer_input)\n",
        "        \n",
        "    \n",
        "    @layer\n",
        "    def max_pool(self, layer_input, k_h, k_w, s_h, s_w, name, padding='SAME'):\n",
        "        return tf.nn.max_pool(layer_input, [1,k_h,k_w,1], [1,s_h,s_w,1], name=name, padding=padding)\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def global_pool(self, layer_input, name):\n",
        "        shape = tf.shape(layer_input)\n",
        "        h = shape[1]\n",
        "        w = shape[2]\n",
        "        output = tf.reduce_mean(layer_input, [1,2], keepdims=True, name=name)\n",
        "        return tf.image.resize_nearest_neighbor(output, [h, w])\n",
        "    \n",
        "    \n",
        "    @layer\n",
        "    def softmax(self, layer_input, name):\n",
        "        return tf.nn.softmax(layer_input, name=name)      \n",
        "    \n",
        "    \n",
        "    # def embedding_lookup(self, input_ids, vocab_size, embedding_size=768,\n",
        "    #                      initializer_range=0.02, word_embedding_name=\"word_embeddings\",\n",
        "    #                      use_one_hot_embeddings=False, trainable=False):\n",
        "    #     \"\"\"Looks up words embeddings for id tensor.\n",
        "        \n",
        "    #     Args:\n",
        "    #       input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
        "    #         ids.\n",
        "    #       vocab_size: int. Size of the embedding vocabulary.\n",
        "    #       embedding_size: int. Width of the word embeddings.\n",
        "    #       initializer_range: float. Embedding initialization range.\n",
        "    #       word_embedding_name: string. Name of the embedding table.\n",
        "    #       use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
        "    #         embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n",
        "    #         for TPUs.\n",
        "        \n",
        "    #     Returns:\n",
        "    #       float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "    #     \"\"\"\n",
        "    #     bert_vocab_size = 119547\n",
        "    #     # This function assumes that the input is of shape [batch_size, seq_length,\n",
        "    #     # num_inputs].\n",
        "    #     #\n",
        "    #     # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
        "    #     # reshape to [batch_size, seq_length, 1].\n",
        "    #     if input_ids.shape.ndims == 3: # originally 2\n",
        "    #         input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "        \n",
        "    #     bert_embedding_table = embedding_table = tf.get_variable(\n",
        "    #         name=word_embedding_name,\n",
        "    #         shape=[bert_vocab_size, embedding_size],\n",
        "    #         initializer=tf.truncated_normal_initializer(stddev=initializer_range),\n",
        "    #         trainable=trainable)\n",
        "    #     if vocab_size > bert_vocab_size: # handle dict augmentation\n",
        "    #         embedding_table_plus = tf.get_variable(\n",
        "    #             name=word_embedding_name + '_plus',\n",
        "    #             shape=[vocab_size-bert_vocab_size, embedding_size],\n",
        "    #             initializer=tf.truncated_normal_initializer(stddev=initializer_range),\n",
        "    #             trainable=True)\n",
        "    #         embedding_table = tf.concat([embedding_table, embedding_table_plus], 0)        \n",
        "        \n",
        "    #     if use_one_hot_embeddings:\n",
        "    #         flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "    #         one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    #         output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "    #     else:\n",
        "    #         output = tf.nn.embedding_lookup(embedding_table, input_ids)\n",
        "        \n",
        "    #     input_shape = self.get_shape_list(input_ids)\n",
        "        \n",
        "    #     output = tf.reshape(output,\n",
        "    #                         input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "    #     return (output, bert_embedding_table)\n",
        "    \n",
        "    # def get_shape_list(self, tensor, expected_rank=None, name=None):\n",
        "    #     \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "        \n",
        "    #     Args:\n",
        "    #       tensor: A tf.Tensor object to find the shape of.\n",
        "    #       expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "    #         specified and the `tensor` has a different rank, and exception will be\n",
        "    #         thrown.\n",
        "    #       name: Optional name of the tensor for the error message.\n",
        "        \n",
        "    #     Returns:\n",
        "    #       A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    #       be returned as python integers, and dynamic dimensions will be returned\n",
        "    #       as tf.Tensor scalars.\n",
        "    #     \"\"\"\n",
        "    #     if name is None:\n",
        "    #       name = tensor.name\n",
        "        \n",
        "    #     if expected_rank is not None:\n",
        "    #       assert_rank(tensor, expected_rank, name)\n",
        "        \n",
        "    #     shape = tensor.shape.as_list()\n",
        "        \n",
        "    #     non_static_indexes = []\n",
        "    #     for (index, dim) in enumerate(shape):\n",
        "    #       if dim is None:\n",
        "    #         non_static_indexes.append(index)\n",
        "        \n",
        "    #     if not non_static_indexes:\n",
        "    #       return shape\n",
        "        \n",
        "    #     dyn_shape = tf.shape(tensor)\n",
        "    #     for index in non_static_indexes:\n",
        "    #       shape[index] = dyn_shape[index]\n",
        "    #     return shape\n",
        "    \n",
        "    \n",
        "    def convolution(self, convolve, activate, input, k_h, k_w, c_i, c_o, init_weights, init_biases, \n",
        "                    regularizer, trainable, name=''):   \n",
        "        kernel = self.make_var('weights'+name, [k_h, k_w, c_i, c_o], init_weights, regularizer, trainable) \n",
        "        biases = self.make_var('biases'+name, [c_o], init_biases, None, trainable)\n",
        "        tf.summary.histogram('w', kernel)\n",
        "        tf.summary.histogram('b', biases)\n",
        "        # test with different orders: convolve/activate/normalize; normalize/convolve/activate; convolve/normalize/activate\n",
        "        wx = convolve(input, kernel)\n",
        "        a = activate(tf.nn.bias_add(wx, biases))\n",
        "        a = tf.contrib.layers.instance_norm(a, center=False, scale=False)\n",
        "        return a\n",
        "    \n",
        "    \n",
        "    def l2_regularizer(self, weight_decay=0.0005, scope=None):\n",
        "        def regularizer(tensor):\n",
        "            with tf.name_scope(scope, default_name='l2_regularizer', values=[tensor]):\n",
        "                factor = tf.convert_to_tensor(weight_decay, name='weight_decay')\n",
        "                return tf.multiply(factor, tf.nn.l2_loss(tensor), name='decayed_value')\n",
        "        return regularizer\n",
        "    \n",
        "    \n",
        "    def make_var(self, name, shape, initializer=None, regularizer=None, trainable=True):\n",
        "        return tf.compat.v1.get_variable(name, shape, initializer=initializer, regularizer=regularizer, trainable=trainable)      \n",
        "    \n",
        "    \n",
        "    def feed(self, *args):\n",
        "        assert len(args) != 0\n",
        "        \n",
        "        self.layer_inputs = []\n",
        "        for layer in args:\n",
        "            if isinstance(layer, str):\n",
        "                try:\n",
        "                    layer = self.layers[layer]\n",
        "                    print(layer)\n",
        "                except KeyError:\n",
        "                    print(list(self.layers.keys()))\n",
        "                    raise KeyError('Unknown layer name fed: %s' % layer)\n",
        "            self.layer_inputs.append(layer)\n",
        "        return self\n",
        "        \n",
        "        \n",
        "    def get_output(self, layer):\n",
        "        try:\n",
        "            layer = self.layers[layer]\n",
        "        except KeyError:\n",
        "            print(list(self.layers.keys()))\n",
        "            raise KeyError('Unknown layer name fed: %s' % layer)\n",
        "        return layer\n",
        "        \n",
        "        \n",
        "    def get_unique_name(self, prefix):\n",
        "        id = sum(t.startswith(prefix) for t,_ in list(self.layers.items())) + 1\n",
        "        return '%s_%d' % (prefix, id)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZsXtJr-KyLu"
      },
      "source": [
        "class CUTIE(Model):\n",
        "    def __init__(self, num_vocabs, num_classes, params, trainable=True):\n",
        "        self.name = \"CUTIE_benchmark\"\n",
        "        \n",
        "        self.data = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None, 1], name='grid_table')\n",
        "        self.gt_classes = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None], name='gt_classes')\n",
        "        self.use_ghm = tf.equal(1, params.use_ghm) if hasattr(params, 'use_ghm') else tf.equal(1, 0) #params.use_ghm \n",
        "        self.activation = 'sigmoid' if (hasattr(params, 'use_ghm') and params.use_ghm) else 'relu'\n",
        "        self.ghm_weights = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, num_classes], name='ghm_weights')        \n",
        "        self.layers = dict({'data': self.data, 'gt_classes': self.gt_classes, 'ghm_weights': self.ghm_weights}) \n",
        "         \n",
        "        self.num_vocabs = num_vocabs\n",
        "        self.num_classes = num_classes     \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.embedding_size = params.embedding_size\n",
        "        self.weight_decay = params.weight_decay if hasattr(params, 'weight_decay') else 0.0\n",
        "        self.hard_negative_ratio = params.hard_negative_ratio if hasattr(params, 'hard_negative_ratio') else 0.0\n",
        "        # self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 0\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "        \n",
        "    \n",
        "    def setup(self):        \n",
        "        # input\n",
        "        (self.feed('data')\n",
        "             .embed(self.num_vocabs, self.embedding_size, name='embedding'))  \n",
        "        \n",
        "        # encoder\n",
        "        (self.feed('embedding')\n",
        "             .conv(3, 5, 64, 1, 1, name='encoder1_1')\n",
        "             .conv(3, 5, 128, 1, 1, name='encoder1_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool1')\n",
        "             .conv(3, 5, 128, 1, 1, name='encoder2_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder2_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool2')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder3_1')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder3_2')\n",
        "             .max_pool(2, 2, 2, 2, name='pool3')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder4_1')\n",
        "             .conv(3, 5, 512, 1, 1, name='encoder4_2'))\n",
        "        \n",
        "        # decoder\n",
        "        (self.feed('encoder4_2')\n",
        "             .up_conv(3, 5, 512, 1, 1, name='up1')\n",
        "             .conv(3, 5, 256, 1, 1, name='decoder1_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='decoder1_2')\n",
        "             .up_conv(3, 5, 256, 1, 1, name='up2')\n",
        "             .conv(3, 5, 128, 1, 1, name='decoder2_1')\n",
        "             .conv(3, 5, 128, 1, 1, name='decoder2_2')\n",
        "             .up_conv(3, 5, 128, 1, 1, name='up3')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder3_1')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder3_2'))\n",
        "        \n",
        "        # classification\n",
        "        (self.feed('decoder3_2')\n",
        "             .conv(1, 1, self.num_classes, 1, 1, activation=self.activation, name='cls_logits')\n",
        "             .softmax(name='softmax'))  \n",
        "        \n",
        "    # def disp_results(self, data_input, data_label, model_output, threshold):\n",
        "    #     data_input_flat = data_input.reshape([-1]) # [b * h * w]\n",
        "    #     labels = [] # [b * h * w, classes]\n",
        "    #     for item in data_label.reshape([-1]):\n",
        "    #         labels.append([i==item for i in range(self.num_classes)])\n",
        "    #     logits = model_output.reshape([-1, self.num_classes]) # [b * h * w, classes] \n",
        "        \n",
        "    #     # ignore none word input\n",
        "    #     labels_flat = []\n",
        "    #     results_flat = []\n",
        "    #     for idx, item in enumerate(data_input_flat):\n",
        "    #         if item != 0: \n",
        "    #             labels_flat.extend(labels[idx])\n",
        "    #             results_flat.extend(logits[idx] > threshold)\n",
        "        \n",
        "    #     num_p = sum(labels_flat)\n",
        "    #     num_n = sum([1-label for label in labels_flat])   \n",
        "    #     num_all = len(results_flat)     \n",
        "    #     num_correct = sum([True for i in range(num_all) if labels_flat[i] == results_flat[i]])        \n",
        "        \n",
        "    #     labels_flat_p = [label!=0 for label in labels_flat]\n",
        "    #     labels_flat_n = [label==0 for label in labels_flat]\n",
        "    #     num_tp = sum([labels_flat_p[i] * results_flat[i] for i in range(num_all)])\n",
        "    #     num_tn = sum([labels_flat_n[i] * (not results_flat[i]) for i in range(num_all)])\n",
        "    #     num_fp = num_n - num_tp\n",
        "    #     num_fn = num_p - num_tp\n",
        "        \n",
        "    #     # accuracy, precision, recall\n",
        "    #     accuracy = num_correct / num_all\n",
        "    #     precision = num_tp / (num_tp + num_fp)\n",
        "    #     recall = num_tp / (num_tp + num_fn)\n",
        "        \n",
        "    #     return accuracy, precision, recall\n",
        "        \n",
        "        \n",
        "    # def inference(self):\n",
        "    #     return self.get_output('softmax') #cls_logits\n",
        "        \n",
        "    \n",
        "    def build_loss(self):\n",
        "        labels = self.get_output('gt_classes')\n",
        "        cls_logits = self.get_output('cls_logits')         \n",
        "        cls_logits = tf.cond(self.use_ghm, lambda: cls_logits*self.get_output('ghm_weights'), \n",
        "                             lambda: cls_logits, name=\"GradientHarmonizingMechanism\")      \n",
        "        \n",
        "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=cls_logits)\n",
        "            \n",
        "        with tf.compat.v1.variable_scope('HardNegativeMining'):\n",
        "            labels = tf.reshape(labels, [-1])  \n",
        "            cross_entropy = tf.reshape(cross_entropy, [-1])\n",
        "            \n",
        "            fg_idx = tf.where(tf.not_equal(labels, 0))\n",
        "            fgs = tf.gather(cross_entropy, fg_idx)\n",
        "            bg_idx = tf.where(tf.equal(labels, 0))\n",
        "            bgs = tf.gather(cross_entropy, bg_idx)\n",
        "             \n",
        "            num = self.hard_negative_ratio * tf.shape(fgs)[0]\n",
        "            num_bg = tf.cond(tf.shape(bgs)[0]<num, lambda:tf.shape(bgs)[0], lambda:num)\n",
        "            sorted_bgs, _ = tf.nn.top_k(tf.transpose(bgs), num_bg, sorted=True)\n",
        "            cross_entropy = fgs + sorted_bgs\n",
        "        \n",
        "        # total loss\n",
        "        model_loss = tf.reduce_mean(cross_entropy)\n",
        "        regularization_loss = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES), name='regularization')\n",
        "        total_loss = model_loss + regularization_loss\n",
        "        \n",
        "        tf.summary.scalar('model_loss', model_loss)\n",
        "        tf.summary.scalar('regularization_loss', regularization_loss)\n",
        "        tf.summary.scalar('total_loss', total_loss)\n",
        "        \n",
        "        logits = self.get_output('cls_logits')\n",
        "        softmax_logits = self.get_output('softmax') #cls_logits\n",
        "        return model_loss, regularization_loss, total_loss, logits, softmax_logits \n",
        "    \n",
        "    # def build_multi_loss(self):\n",
        "    #     labels = self.get_output('gt_classes')\n",
        "    #     cls_logits = self.get_output('cls_logits')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGsw6HsaL2SL"
      },
      "source": [
        "class CUTIERes(CUTIE):\n",
        "    def __init__(self, num_vocabs, num_classes, params, trainable=True):\n",
        "        self.name = \"CUTIE_atrousSPP\" # \n",
        "        \n",
        "        self.data_grid = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None, 1], name='data_grid')\n",
        "        self.gt_classes = tf.compat.v1.placeholder(tf.int32, shape=[None, None, None], name='gt_classes') \n",
        "        self.data_image = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, 3], name='data_image') # not used in CUTIEv1\n",
        "        self.ps_1d_indices = tf.compat.v1.placeholder(tf.int32, shape=[None, None], name='ps_1d_indices') # not used in CUTIEv1\n",
        "        \n",
        "        self.use_ghm = tf.equal(1, params.use_ghm) if hasattr(params, 'use_ghm') else tf.equal(1, 0) #params.use_ghm \n",
        "        self.activation = 'sigmoid' if (hasattr(params, 'use_ghm') and params.use_ghm) else 'relu'\n",
        "        self.dropout = params.data_augmentation_dropout if hasattr(params, 'data_augmentation_dropout') else 1\n",
        "        self.ghm_weights = tf.compat.v1.placeholder(tf.float32, shape=[None, None, None, num_classes], name='ghm_weights')        \n",
        "        self.layers = dict({'data_grid': self.data_grid, 'gt_classes': self.gt_classes, 'ghm_weights':self.ghm_weights})\n",
        "\n",
        "        self.num_vocabs = num_vocabs\n",
        "        self.num_classes = num_classes     \n",
        "        self.trainable = trainable\n",
        "        \n",
        "        self.embedding_size = params.embedding_size\n",
        "        self.weight_decay = params.weight_decay if hasattr(params, 'weight_decay') else 0.0\n",
        "        self.hard_negative_ratio = params.hard_negative_ratio if hasattr(params, 'hard_negative_ratio') else 0.0\n",
        "        # self.batch_size = params.batch_size if hasattr(params, 'batch_size') else 0\n",
        "        \n",
        "        self.layer_inputs = []        \n",
        "        self.setup()\n",
        "        \n",
        "    \n",
        "    def setup(self):        \n",
        "        # input\n",
        "        (self.feed('data_grid')\n",
        "             .embed(self.num_vocabs, self.embedding_size, name='embedding', dropout=self.dropout))  \n",
        "        \n",
        "        # encoder\n",
        "        (self.feed('embedding')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_1')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_2')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_3')\n",
        "             .conv(3, 5, 256, 1, 1, name='encoder1_4')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 2, name='encoder1_5')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 4, name='encoder1_6')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 8, name='encoder1_7')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 16, name='encoder1_8'))\n",
        "        \n",
        "        # Atrous Spatial Pyramid Pooling module\n",
        "        #(self.feed('encoder1_8')\n",
        "        #     .conv(1, 1, 256, 1, 1, name='aspp_0'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 4, name='aspp_1'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 8, name='aspp_2'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .dilate_conv(3, 5, 256, 1, 1, 16, name='aspp_3'))\n",
        "        (self.feed('encoder1_8')\n",
        "             .global_pool(name='aspp_4'))\n",
        "        (self.feed('aspp_1', 'aspp_2', 'aspp_3', 'aspp_4')\n",
        "             .concat(3, name='aspp_concat')\n",
        "             .conv(1, 1, 256, 1, 1, name='aspp_1x1'))\n",
        "        \n",
        "        # combine low level features\n",
        "        (self.feed('encoder1_1', 'aspp_1x1')\n",
        "             .concat(3, name='concat1')\n",
        "             .conv(3, 5, 64, 1, 1, name='decoder1_1'))\n",
        "        \n",
        "        # classification\n",
        "        (self.feed('decoder1_1') \n",
        "             .conv(1, 1, self.num_classes, 1, 1, activation=self.activation, name='cls_logits') # sigmoid for ghm\n",
        "             .softmax(name='softmax'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xfB_lstL_nh"
      },
      "source": [
        "# 2. Predict Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kke4HoK2L4F2"
      },
      "source": [
        "parser = argparse.ArgumentParser(description='CUTIE parameters')\n",
        "\n",
        "# Dummy parser arguments for notebook\n",
        "parser.add_argument('-f')\n",
        "\n",
        "parser.add_argument('--use_cutie2', type=bool, default=False) # True to read image from doc_path \n",
        "parser.add_argument('--is_table', type=bool, default=False) # True to read image from doc_path \n",
        "parser.add_argument('--doc_path', type=str, default='ExpressExpenseJsonPredict') # modify this\n",
        "parser.add_argument('--save_prefix', type=str, default='ExpressExpense', help='prefix for load ckpt model') # modify this\n",
        "# parser.add_argument('--test_path', type=str, default='ExpressExpenseJsonTest') # leave empty if no test data provided\n",
        "\n",
        "# parser.add_argument('--fill_bbox', type=bool, default=False) # augment data row/col in each batch\n",
        "\n",
        "parser.add_argument('--e_ckpt_path', type=str, default='checkpoint/')\n",
        "parser.add_argument('--ckpt_file', type=str, default='CUTIE_atrousSPP_d20000c2(r80c80)_iter_200.ckpt')\n",
        "# parser.add_argument('--positional_mapping_strategy', type=int, default=1)\n",
        "parser.add_argument('--rows_target', type=int, default=80)  # I used 64 last time\n",
        "parser.add_argument('--cols_target', type=int, default=80)  # I used 64 last time\n",
        "parser.add_argument('--rows_ulimit', type=int, default=80) \n",
        "parser.add_argument('--cols_ulimit', type=int, default=80) \n",
        "\n",
        "parser.add_argument('--load_dict', type=bool, default=True, help='True to work based on an existing dict') \n",
        "parser.add_argument('--load_dict_from_path', type=str, default='dict/') # 40000 or table or 20000TC\n",
        "# parser.add_argument('--tokenize', type=bool, default=False) # tokenize input text ### default = True\n",
        "# parser.add_argument('--text_case', type=bool, default=False) # case sensitive ### default = True == case sensitive\n",
        "# parser.add_argument('--dict_path', type=str, default='dict/ExpressExpense') # not used if load_dict is True\n",
        "\n",
        "parser.add_argument('--restore_ckpt', type=bool, default=True) \n",
        "\n",
        "parser.add_argument('--embedding_size', type=int, default=128) \n",
        "# parser.add_argument('--batch_size', type=int, default=4) # default=32 \n",
        "parser.add_argument('--c_threshold', type=float, default=0.5) \n",
        "params = parser.parse_args()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh0Vu_HxMCGK",
        "outputId": "3069d33f-f926-45e8-f811-a8446a8fd442"
      },
      "source": [
        "# data\n",
        "#data_loader = DataLoader(params, True, True) # True to use 25% training data\n",
        "data_loader = DataLoader(params, update_dict=False, load_dictionary=True, data_split=0.75) # False to provide a path with only test data\n",
        "num_words = max(20000, data_loader.num_words)\n",
        "num_classes = data_loader.num_classes\n",
        "\n",
        "# model\n",
        "network = CUTIERes(num_words, num_classes, params)\n",
        "model_output = network.get_output('softmax')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"data_grid:0\", shape=(?, ?, ?, 1), dtype=int32)\n",
            "WARNING:tensorflow:From <ipython-input-7-11283867f653>:46: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"embedding/Reshape_1:0\", shape=(?, ?, ?, 128), dtype=float32)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_8/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_2/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_3/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"ResizeNearestNeighbor:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"encoder1_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"aspp_1x1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 256), dtype=float32)\n",
            "Tensor(\"decoder1_1/InstanceNorm/instancenorm/add_1:0\", shape=(?, ?, ?, 64), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Np6dshtNMEaR",
        "outputId": "53111b9a-e84a-4d3f-892b-acbc36833951"
      },
      "source": [
        "# evaluation\n",
        "ckpt_saver = tf.train.Saver()\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "with tf.Session(config=config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    try:\n",
        "        ckpt_path = join(params.e_ckpt_path, params.save_prefix, params.ckpt_file)\n",
        "        # ckpt_path = '/content/content/CUTIE/graph/INVOICE/CUTIE_atrousSPP_best.ckpt'\n",
        "        ckpt = tf.train.get_checkpoint_state(ckpt_path)\n",
        "        print('Restoring from {}...'.format(ckpt_path))\n",
        "        ckpt_saver.restore(sess, ckpt_path)\n",
        "        print('{} restored'.format(ckpt_path))\n",
        "    except:\n",
        "        raise Exception('Check your pretrained {:s}'.format(ckpt_path))\n",
        "    \n",
        "    # calculate validation accuracy and display results   \n",
        "    recalls, accs_strict, accs_soft = [], [], []\n",
        "    num_test = len(data_loader.validation_docs)\n",
        "    for i in range(num_test):\n",
        "        data = data_loader.fetch_validation_data()\n",
        "        print('{:d} samples left to be tested'.format(num_test-i))\n",
        "        \n",
        "#             grid_table = data['grid_table']\n",
        "#             gt_classes = data['gt_classes']\n",
        "        feed_dict = {\n",
        "            network.data_grid: data['grid_table'],\n",
        "        }\n",
        "        if params.use_cutie2:\n",
        "            feed_dict = {\n",
        "                network.data_grid: data['grid_table'],\n",
        "                network.data_image: data['data_image'],\n",
        "                network.ps_1d_indices: data['ps_1d_indices']\n",
        "            }\n",
        "        fetches = [model_output]\n",
        "        \n",
        "        print(data['file_name'][0])\n",
        "        print(data['grid_table'].shape, data['data_image'].shape, data['ps_1d_indices'].shape)\n",
        "        \n",
        "        timer_start = timeit.default_timer()\n",
        "        [model_output_val] = sess.run(fetches=fetches, feed_dict=feed_dict)\n",
        "        timer_stop = timeit.default_timer()\n",
        "        print('\\t >>time per step: %.2fs <<'%(timer_stop - timer_start))\n",
        "            \n",
        "        if not params.is_table:\n",
        "            recall, acc_strict, acc_soft, res = cal_accuracy(params.c_threshold, data_loader, np.array(data['grid_table']), \n",
        "                                                    np.array(data['gt_classes']), model_output_val, \n",
        "                                                    np.array(data['label_mapids']), data['bbox_mapids'])  \n",
        "        else:\n",
        "            recall, acc_strict, acc_soft, res = cal_accuracy_table(params.c_threshold, data_loader, np.array(data['grid_table']), \n",
        "                                                    np.array(data['gt_classes']), model_output_val, \n",
        "                                                    np.array(data['label_mapids']), data['bbox_mapids'])                     \n",
        "#             recall, acc_strict, acc_soft, res = cal_save_results(data_loader, np.array(data['grid_table']), \n",
        "#                                                        np.array(data['gt_classes']), model_output_val, \n",
        "#                                                        np.array(data['label_mapids']), data['bbox_mapids'],\n",
        "#                                                        data['file_name'][0], params.save_prefix)\n",
        "        recalls += [recall]\n",
        "        accs_strict += [acc_strict] \n",
        "        accs_soft += [acc_soft]\n",
        "        if acc_strict != 1:\n",
        "            print(res.decode()) # show res for current batch\n",
        "        \n",
        "        # visualize result\n",
        "        shape = data['shape']\n",
        "        file_name = data['file_name'][0] # use one single file_name\n",
        "        bboxes = data['bboxes'][file_name]\n",
        "        if not params.is_table:\n",
        "            vis_bbox(data_loader, params.doc_path, np.array(data['grid_table'])[0], \n",
        "                      np.array(data['gt_classes'])[0], np.array(model_output_val)[0], file_name, \n",
        "                      np.array(bboxes), shape)\n",
        "        else:\n",
        "            vis_table(data_loader, params.doc_path, np.array(data['grid_table'])[0], \n",
        "                      np.array(data['gt_classes'])[0], np.array(model_output_val)[0], file_name, \n",
        "                      np.array(bboxes), shape)\n",
        "\n",
        "    recall = sum(recalls) / len(recalls)\n",
        "    acc_strict = sum(accs_strict) / len(accs_strict)\n",
        "    acc_soft = sum(accs_soft) / len(accs_soft)\n",
        "    print('EVALUATION ACC (Recall/Acc): %.3f / %.3f (%.3f) \\n'%(recall, acc_strict, acc_soft))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Restoring from checkpoint/ExpressExpense/CUTIE_atrousSPP_d20000c2(r80c80)_iter_200.ckpt...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/ExpressExpense/CUTIE_atrousSPP_d20000c2(r80c80)_iter_200.ckpt\n",
            "checkpoint/ExpressExpense/CUTIE_atrousSPP_d20000c2(r80c80)_iter_200.ckpt restored\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=3200x1600 at 0x7F346B826278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1 samples left to be tested\n",
            "1154-receipt.jpg\n",
            "(1, 80, 80, 1) (0,) (0,)\n",
            "\t >>time per step: 2.62s <<\n",
            "len(data_input_flat): 6400\n",
            "indexes: [ 104  111  122  268  277  285  424  435  442  450  510  601  706  742\n",
            "  777  809 1027 1047 1055 1063 1100 1128 1180 1217 1426 1444 1449 1456\n",
            " 1466 1611 1621 1683 1746 1773 1785 1843 1853 1906 2003 2011 2066 2163\n",
            " 2170 2177 2183 2193 2226 2323 2330 2339 2386 2491 2501 2649 2788 2802\n",
            " 2806 2811 2824 2966 2972 2983 3108 3122 3130 3141 3281 3288 3295 3303\n",
            " 3348 3445 3457 3516 3763 3773 3828 3922 3929 3989 4082 4088 4150 4495\n",
            " 4549 4567 4971 5717 6108 6123 6145 6349 6361]\n",
            "\n",
            "TTL(GT/Inf):\t\"[UNK] [UNK]\" | \"[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\"\n",
            " \t FALSES =>> \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O,  \"[UNK]\"/O, \n",
            "EVALUATION ACC (Recall/Acc): 1.000 / 0.000 (1.000) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQAQb8MUNNyq"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}